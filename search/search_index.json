{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the k0rdent docs","text":""},{"location":"#introduction","title":"Introduction","text":"<p>k0rdent has been developed to provide a way to manage distributed infrastructure at massive scale leveraging kubernetes.</p> <p>The project is based on the premise that:</p> <ul> <li>Kubernetes and its ecosystem is mature and inherently stable.</li> <li>Large scale adoption means that it can run anywhere.</li> <li>Community standards and open source projects ensure support and reduces adoption risk.</li> </ul> <p>The goal of the k0rdent project is to provide platform engineers with the means to deliver a distributed container management environment (DCME) and enable them to compose unique internal developer platforms (IDP) to support a diverse range of complex modern application workloads.</p> <p>Another way to think of k0rdent is as a \"super control plane\" designed to ensure the consistent provisioning and lifecycle management of kubernetes clusters and the services that make it useful.</p> <p>In short: Kubernetes clusters at scale, managed centrally, template driven, based on open community driven standards, enabling Golden Paths ... k0rdent aspires to do that.</p> <p>Whether you want to manage Kubernetes clusters on-premises, in the cloud, or a combination of both, k0rdent provides a consistent way to do it. With full life-cycle management, including provisioning, configuration, and maintenance, k0rdent is designed to be a repeatable and secure way to manage your Kubernetes clusters in a central location.</p>"},{"location":"#k0rdent-vs-project-2a","title":"k0rdent vs Project 2A","text":"<p>k0rdent is the official name of this open source project developed by Mirantis. It started out life as an internal project codenamed \"Project 2A\". 2A references the hexadecimal 0x2A, which encompasses our hopes for the project.</p>"},{"location":"#k0rdent-components","title":"k0rdent Components","text":"<p>The main components of k0rdent include:</p> <ul> <li> <p>k0rdent Cluster Manager (KCM)</p> <p>Deployment and life-cycle management of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>k0rdent State Manager (KSM)</p> <p>Installation and life-cycle management of beach-head services, policy, Kubernetes API configurations, and more.</p> </li> <li> <p>k0rdent Observability and FinOps (KOF)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul>"},{"location":"#structure-and-history","title":"Structure and History","text":"<p>The project has a number of components, including:</p> <ul> <li>k0rdent: the overall project<ul> <li>k0rdent Cluster Manager (KCM)</li> <li>k0rdent State Manager (KSM)<ul> <li>This is currently rolled into kcm, but may be split out in the future</li> <li>ksm leverages Project Sveltos   for an increasing amount of functionality</li> </ul> </li> <li>k0rdent Observability and FinOps (KOF)</li> </ul> </li> </ul> <p>There are a few historical names that may show up in the code and older docs.</p> <ul> <li>Project 2A: the original codename of k0rdent (may occasionally show   up in some documentation)</li> <li>HMC or hmc: the original repository name for k0rdent and KCM   development (may occasionally show up in some documentation and code)</li> <li>motel: the original repository and codename for kof (may   occasionally show up in some documentation and code)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the k0rdent Quick Start Guide.</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>k0rdent leverages the Cluster API provider ecosystem, the following providers have had <code>ProviderTemplates</code> created and validated, and more are in the works. </p> <ul> <li>AWS</li> <li>Azure</li> <li>vSphere</li> <li>OpenStack</li> </ul> <p>k0rdent also includes a way to add custom providers.</p>"},{"location":"#development-documentation","title":"Development Documentation","text":"<p>Documentation related to the development process and developer-specific notes is located in the main k0rdent repository.</p>"},{"location":"admin-adopting-clusters/","title":"Adopting an Existing Cluster","text":"<p>Creating a new cluster isn't the only way to use k0rdent. Adopting an existing Kubernetes cluster enables you to  bring it under k0rdent's management. This process is useful when you already have a running cluster but want  to centralize management and leverage k0rdent's capabilities, such as unified monitoring, configuration, and automation, but you don't want to redeploy your cluster.</p> <p>To adopt a cluster, k0rdent establishes communication between the management cluster (where kcm is installed)  and the target cluster. This requires proper credentials, network connectivity, and a standardized configuration. </p> <p>Follow these steps to adopt an existing cluster:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A kubeconfig file for the cluster you want to adopt (this file provides access credentials and configuration details    for the cluster).</li> <li>A management cluster with k0rdent installed and running. See the installation instructions    if you need to set it up.</li> <li>Network connectivity between the management cluster and the cluster to be adopted (for example, ensure firewall    rules and VPNs allow communication).</li> </ul> </li> <li> <p>Create a Credential</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure  provider. Follow the instructions in the Credential System, as well as the specific instructions  for your target infrastructure.</p> <p>Tip</p> <p>Double-check that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Configure the management cluster kubeconfig</p> <p>Set the <code>KUBECONFIG</code> environment variable to the path of your management cluster's kubeconfig file so you can  execute commands against the management cluster.</p> <p>For example:</p> <pre><code>export KUBECONFIG=/path/to/management-cluster-kubeconfig\n</code></pre> </li> <li> <p>Create the <code>ClusterDeployment</code> YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is used to define how k0rdent should manage the adopted cluster. Create a  YAML file for the <code>ClusterDeployment</code> object, as shown below:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;CLUSTER_NAME&gt;\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  template: adopted-cluster-&lt;VERSION&gt;\n  credential: &lt;CREDENTIAL_NAME&gt;\n  dryRun: &lt;BOOLEAN&gt;\n  config:\n    &lt;CONFIGURATION&gt;\n</code></pre> <p>Replace placeholders such as <code>&lt;CLUSTER_NAME&gt;</code>, <code>&lt;NAMESPACE&gt;</code>, <code>&lt;VERSION&gt;</code>, <code>&lt;CREDENTIAL_NAME&gt;</code>, and <code>&lt;CONFIGURATION&gt;</code> with actual values. The <code>dryRun</code> flag is useful for testing the configuration without making changes to the cluster. For more details, see the Dry Run section.</p> <p>You can also get a list of the available templates with:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-1-0           true\naws-eks-0-1-0                   true\naws-hosted-cp-0-1-0             true\naws-standalone-cp-0-1-0         true\nazure-aks-0-1-0                 true\nazure-hosted-cp-0-1-0           true\nazure-standalone-cp-0-1-0       true\nopenstack-standalone-cp-0-1-0   true\nvsphere-hosted-cp-0-1-0         true\nvsphere-standalone-cp-0-1-0     true\n</code></pre></p> <p>Putting it all together, your YAML would look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster\n  namespace: kcm-system\nspec:\n  template: adopted-cluster-0-1-0\n  credential: my-cluster-credential\n  dryRun: false\n  config: {}\n</code></pre> </li> <li> <p>Apply the <code>ClusterDeployment</code> configuration</p> <p>Once your configuration file is ready, apply it to the management cluster using <code>kubectl</code>:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits the <code>ClusterDeployment</code> object to k0rdent, initiating the adoption process.</p> </li> <li> <p>Check the Status of the <code>ClusterDeployment</code> Object</p> <p>To ensure the adoption process is progressing as expected, check the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output includes the current state and any conditions (for example, errors or progress updates). Review  this information to confirm that the adoption is successful.</p> </li> </ol>"},{"location":"admin-adopting-clusters/#whats-happening-behind-the-scenes","title":"What's Happening Behind the Scenes?","text":"<p>When you adopt a cluster, k0rdent performs several actions: 1. It validates the credentials and configuration provided in the <code>ClusterDeployment</code> object. 2. It ensures network connectivity between the management cluster and the adopted cluster. 3. It registers the adopted cluster within the k0rdent system, enabling it to be monitored and managed like      any k0rdent-deployed cluster.</p> <p>This process doesn't change the adopted cluster's existing workloads or configurations. Instead, it enhances your  ability to manage the cluster through k0rdent.</p>"},{"location":"admin-adopting-clusters/#additional-tips","title":"Additional Tips","text":"<ul> <li>If you encounter issues, double-check that kubeconfig file you used for the adopted cluster is valid    and matches the cluster you're trying to adopt.</li> <li>Use the <code>dryRun</code> option during the first attempt to validate the configuration without making actual changes.</li> </ul>"},{"location":"admin-backup/","title":"Backing Up and Restoring a k0rdent Management Cluster","text":"<p>Any production system needs to provide Disaster Recovery features, and the heart of these capabilities is the ability to perform backup and restore operations. In this chapter we'll look at backing up and restoring k0rdent management cluster so that in case of an emergency, you can restore your system to its previous condition or recreate it on another cluster.</p> <p>While it's possible to back up a Kubernetes cluster manually, it's better to build on the work of others. In this case we're going to leverage the <code>velero</code> project for backup management on the backend and see how it integrates with k0rdent to ensure data persistence and recovery.</p>"},{"location":"admin-backup/#motivation","title":"Motivation","text":"<p>The primary goal of this feature is to provide a reliable and efficient way to back up and restore a k0rdent deployment in the event of a disaster that impacts the management cluster. By using <code>velero</code> as the backup provider, we can create consistent backups across different cloud storage options while maintaining the integrity of critical resources.</p> <p>The main goal of the feature is to provide:</p> <ul> <li>Management Backup: The ability to backup all configuration objects created and managed by k0rdent, including   into an offsite location.</li> <li>Restore: The ability to create configuration objects from a specific Management Backup in order to create a management   cluster in the same state that existed at the time of backup without (re)provisioning of cloud resources.</li> <li>Disaster Recovery: The ability to restore k0rdent on another management cluster, plus ensuring that clusters are not   recreated or lost.</li> <li>Rollback: The possibility to manually restore after a specific event, such as a failed k0rdent upgrade</li> </ul>"},{"location":"admin-backup/#velero-as-provider-for-management-backups","title":"Velero as Provider for Management Backups","text":"<p><code>Velero</code> is an open-source tool that simplifies backing up and restoring clusters as well as individual resources. It seamlessly integrates into the k0rdent management environment to provide robust disaster recovery capabilities.</p> <p>The <code>velero</code> instance is part of the Helm chart that installs k0rdent, which means that it can be customized if necessary.</p> <p>k0rdent manages the schedule and is responsible for collecting data to be included in a backup.</p>"},{"location":"admin-backup/#scheduled-management-backups","title":"Scheduled Management Backups","text":"<p>Backups should be scheduled on a regular basis, depending on how often information changes.</p>"},{"location":"admin-backup/#preparation","title":"Preparation","text":"<p>Before you create a scheduled backup, you need to perform a few preparatory steps:</p> <ol> <li> <p>If no <code>velero</code> plugins have been installed as suggested    in the corresponding section,    install it by modifying the <code>Management</code> object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  # ... \n  core:\n    kcm:\n      config:\n        velero:\n          initContainers:\n          - name: velero-plugin-for-&lt;provider-name&gt;\n            image: velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt;\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - mountPath: /target\n              name: plugins\n  # ...\n</code></pre> </li> <li> <p>Prepare a cloud storage location, such as an Amazon S3 bucket, to which to save backups.</p> </li> <li> <p>Create a <code>BackupStorageLocation</code>    object referencing a <code>Secret</code> with credentials to access the cloud storage    (if the multiple credentials feature is supported by the plugin).</p> <p>For example, if you are using Amazon S3, your <code>BackupStorageLocation</code> and the related <code>Secret</code> might look like this:</p> <pre><code>---\napiVersion: v1\ndata:\n  # base64-encoded credentials for Amazon S3 in the following format:\n  # [default]\n  # aws_access_key_id = EXAMPLE_ACCESS_KEY_ID\n  # aws_secret_access_key = EXAMPLE_SECRET_ACCESS_KEY\n  cloud: WW2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkID0gRVhBTVBMRV9BQ0NFU1NfS0VZX0lECmF3c19zZWNyZXRfYWNjZXNzX2tleSA9IEVYQU1QTEVfU0VDUkVUX0FDQ0VTU19LRVkKICA=\nkind: Secret\nmetadata:\n  name: cloud-credentials\n  namespace: kcm-system\ntype: Opaque\n---\napiVersion: velero.io/v1\nkind: BackupStorageLocation\nmetadata:\n  name: aws-s3\n  namespace: kcm-system\nspec:\n  config:\n    region: &lt;your-region-name&gt;\n  default: true # optional, if not set, then storage location name must always be set in ManagementBackup\n  objectStorage:\n    bucket: &lt;your-bucket-name&gt;\n  provider: aws\n  backupSyncPeriod: 1m\n  credential:\n    name: cloud-credentials\n    key: cloud\n</code></pre> </li> </ol> <p>You can get more information on how to build these objects at the official Velero documentation.</p>"},{"location":"admin-backup/#create-a-management-backup","title":"Create a Management Backup","text":"<p>Periodic backups are handled by a <code>ManagementBackup</code> object, which uses a Cron expression for its <code>.spec.schedule</code> field. If the <code>.spec.schedule</code> field is not set, a backup on demand will be created instead.</p> <p>Optionally, set the name of the <code>BackupStorageLocation</code> <code>.spec.backup.storageLocation</code>. The default location is the <code>BackupStorageLocation</code> object with <code>.spec.default</code> set to <code>true</code>.</p> <p>For example, you can create a <code>ManagementBackup</code> object that backs up to the storage object created in the previous step every 6 minutes would look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ManagementBackup\nmetadata:\n  name: kcm\nspec:\n  schedule: \"0 */6 * * *\"\n  storageLocation: aws-s3\n</code></pre>"},{"location":"admin-backup/#management-backup-on-demand","title":"Management Backup on Demand","text":"<p>To create a single backup of the existing k0rdent management cluster information, you can create a <code>ManagementBackup</code> object using a YAML document and the <code>kubectl</code> CLI. The object then creates only one instance of a backup. For example you can backup to the location created earlier:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ManagementBackup\nmetadata:\n  name: example-backup\nspec:\n  storageLocation: aws-s3\n</code></pre>"},{"location":"admin-backup/#whats-included-in-the-management-backup","title":"What's Included in the Management Backup","text":"<p>The backup includes all of k0rdent <code>kcm</code> component resources, parts of the <code>cert-manager</code> components required for other components creation, and all the required resources of <code>CAPI</code> and the <code>ClusterDeployment</code>s currently in use in the management cluster.</p> <p>By default, objects satisfying these labels will be included in the backup:</p> <pre><code>cluster.x-k8s.io/cluster-name=\"&lt;cluster-deployment-name&gt;\"\nhelm.toolkit.fluxcd.io/name=\"&lt;cluster-deployment-name&gt;\"\n\ncluster.x-k8s.io/provider=\"bootstrap-&lt;provider&gt;\"\ncluster.x-k8s.io/provider=\"control-plane-&lt;provider&gt;\"\ncluster.x-k8s.io/provider=\"infrastructure-&lt;provider&gt;\"\n\ncluster.x-k8s.io/provider=\"cluster-api\"\n\ncontroller.cert-manager.io/fao=\"true\"\n\nk0rdent.mirantis.com/component=\"kcm\"\n</code></pre> <p>An example sorted set of labels, objects satisfying these labels will be included in the backup:</p> <pre><code>cluster.x-k8s.io/cluster-name=\"some-cluster-deployment-name\"\ncluster.x-k8s.io/provider=\"bootstrap-k0sproject-k0smotron\"\ncluster.x-k8s.io/provider=\"cluster-api\"\ncluster.x-k8s.io/provider=\"control-plane-k0sproject-k0smotron\"\ncluster.x-k8s.io/provider=\"infrastructure-aws\"\ncontroller.cert-manager.io/fao=\"true\"\nhelm.toolkit.fluxcd.io/name=\"some-cluster-deployment-name\"\nk0rdent.mirantis.com/component=\"kcm\"\n</code></pre>"},{"location":"admin-backup/#restoration","title":"Restoration","text":"<p>Note</p> <p> Please refer to the official migration documentation to familiarize yourself with potential limitations of the Velero backup system.</p> <p>In the event of disaster, you can restore from a backup by doing the following:</p> <ol> <li> <p>Create a clean k0rdent installation, including <code>velero</code> and its plugins.    Specifically, you want to avoid creating a <code>Management</code> object and similar objects because they    will be part of your restored cluster. You can remove these objects after installation, but you    can also install k0rdent without them in the first place:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set controller.createManagement=false \\\n --set controller.createAccessManagement=false \\\n --set controller.createRelease=false \\\n --set controller.createTemplates=false \\\n --set velero.initContainers[0].name=velero-plugin-for-&lt;provider-name&gt; \\\n --set velero.initContainers[0].image=velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt; \\\n --set velero.initContainers[0].volumeMounts[0].mountPath=/target \\\n --set velero.initContainers[0].volumeMounts[0].name=plugins\n</code></pre> </li> <li> <p>Create the <code>BackupStorageLocation</code>/<code>Secret</code> objects that were created during the preparation stage    of creating a backup (preferably the same depending on a plugin).</p> </li> <li> <p>Restore the <code>kcm</code> system creating the <code>Restore</code> object.    Note that it is important to set the <code>.spec.existingResourcePolicy</code> field value to <code>update</code>:</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  includedNamespaces:\n  - '*'\n</code></pre> </li> <li> <p>Wait until the <code>Restore</code> status is <code>Completed</code> and all <code>kcm</code> components are up and running.</p> </li> </ol>"},{"location":"admin-backup/#caveats","title":"Caveats","text":"<p>For some CAPI providers it is necessary to make changes to the <code>Restore</code> object due to the large number of different resources and logic in each provider. The resources described below are not excluded from a <code>ManagementBackup</code> by default to avoid logical dependencies on one or another provider, and to create a provider-agnostic system.</p> <p>Note</p> <p> The described caveats apply only to the <code>Restore</code> object creation step and do not affect the other steps.</p>"},{"location":"admin-backup/#azure-capz","title":"Azure (CAPZ)","text":"<p>The following resources should be excluded from the <code>Restore</code> object:</p> <ul> <li><code>natgateways.network.azure.com</code></li> <li><code>resourcegroups.resources.azure.com</code></li> <li><code>virtualnetworks.network.azure.com</code></li> <li><code>virtualnetworkssubnets.network.azure.com</code></li> </ul> <p>Due to the webhook conversion, objects of these resources cannot be restored, and they will be created in the management cluster by the <code>CAPZ</code> provider automatically with the same <code>spec</code> as in the backup.</p> <p>The resulting <code>Restore</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  excludedResources:\n  - natgateways.network.azure.com\n  - resourcegroups.resources.azure.com\n  - virtualnetworks.network.azure.com\n  - virtualnetworkssubnets.network.azure.com\n  includedNamespaces:\n  - '*'\n</code></pre>"},{"location":"admin-backup/#vsphere-capv","title":"vSphere (CAPV)","text":"<p>The following resources should be excluded from the <code>Restore</code> object:</p> <ul> <li><code>mutatingwebhookconfiguration.admissionregistration.k8s.io</code></li> <li><code>validatingwebhookconfiguration.admissionregistration.k8s.io</code></li> </ul> <p>Due to the Velero Restoration Order, some of the <code>CAPV</code> core objects cannot be restored, and they will not be recreated automatically. Because all of the objects have already passed both mutations and validations, there is not much sense in validating them again. The webhook configurations will be restored during installation of the <code>CAPV</code> provider.</p> <p>The resulting <code>Restore</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  excludedResources:\n  - mutatingwebhookconfiguration.admissionregistration.k8s.io\n  - validatingwebhookconfiguration.admissionregistration.k8s.io\n  includedNamespaces:\n  - '*'\n</code></pre>"},{"location":"admin-backup/#upgrades-and-rollbacks","title":"Upgrades and rollbacks","text":"<p>The Disaster Recovery Feature provides a way to create backups on each <code>kcm</code> upgrade automatically.</p>"},{"location":"admin-backup/#automatic-management-backups","title":"Automatic Management Backups","text":"<p>Each <code>ManagementBackup</code> with a non-empty <code>.spec.schedule</code> field can enable the automatic creation of backups before upgrading to a new version.</p> <p>To enable, set the <code>.spec.performOnManagementUpgrade</code> to <code>true</code>.</p> <p>For example, a <code>ManagementBackup</code> object with enabled auto-backup before the <code>kcm</code> version upgrade looks like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ManagementBackup\nmetadata:\n  name: example-backup\nspec:\n  schedule: \"0 */6 * * *\"\n  performOnManagementUpgrade: true\n</code></pre> <p>After the enablement, before each upgrade of <code>kcm</code> to a new version, a new backup will be created.</p> <p>Automatically created backups have the following name template to make it easier to find them: the name of the <code>ManagementBackup</code> object with enabled <code>performOnManagementUpgrade</code> concatenates with the name of the release before the upgrade, for example, <code>example-backup-kcm-0-1-0</code>.</p> <p>Automatically created backups have the label <code>k0rdent.mirantis.com/release-backup</code> with the name of the release before the upgrade as its value to simplify querying if required.</p>"},{"location":"admin-backup/#rollbacks","title":"Rollbacks","text":"<p>If during the <code>kcm</code> upgrade a failure happens, a rollback operation should be performed to restore the <code>kcm</code> to its before-the-upgrade state:</p> <ol> <li> <p>Follow the first 2 steps from the restoration section, creating a clean <code>kcm</code>    installation and <code>BackupStorageLocation</code>/<code>Secret</code>.</p> <p>Warning</p> <p> Please consider the restoration caveats section before proceeding.</p> </li> <li> <p>Create the <code>ConfigMap</code> object with patches to revert the <code>Management</code> <code>.spec.release</code>, substitute the <code>&lt;version-before-upgrade&gt;</code> with    the version of <code>kcm</code> before the upgrade, and create the <code>Restore</code> object,    propagating the <code>ConfigMap</code> to it:</p> <pre><code>---\napiVersion: v1\ndata:\n  patch-mgmt-spec-release: |\n    version: v1\n    resourceModifierRules:\n    - conditions:\n        groupResource: managements.k0rdent.mirantis.com\n      patches:\n      - operation: replace\n        path: \"/spec/release\"\n        value: \"&lt;version-before-upgrade&gt;\"\nkind: ConfigMap\nmetadata:\n  name: patch-mgmt-spec-release\n  namespace: kcm-system\n---\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  includedNamespaces:\n  - '*'\n  resourceModifier: # propagate patches\n    kind: ConfigMap\n    name: patch-mgmt-spec-release\n</code></pre> </li> <li> <p>Wait until the <code>Restore</code> status is <code>Completed</code> and all <code>kcm</code> components are up and running.</p> </li> <li>Optionally delete the created <code>ConfigMap</code>.</li> </ol>"},{"location":"admin-backup/#caveats-limitations","title":"Caveats / Limitations","text":"<p>The credentials stored in backups can and will get stale, so a proper rotation should be considered beforehand.</p> <p>All <code>velero</code> caveats and limitations are transitively implied in <code>k0rdent</code>. In particular, that means no backup encryption is provided until it is implemented by a <code>velero</code> plugin that supports encryption and cloud storage backups.</p>"},{"location":"admin-backup/#velero-backups-restores-deletion","title":"Velero Backups / Restores deletion","text":""},{"location":"admin-backup/#delete-restores","title":"Delete Restores","text":"<p>To delete a <code>velero</code> <code>Restore</code> from the management cluster and from cloud storage, delete <code>restores.velero.io</code> object(s), such as with the following command:</p> <pre><code>kubectl delete restores.velero.io -n kcm-system &lt;restore-name&gt;\n</code></pre> <p>Warning</p> <p> Deletion of a <code>Restore</code> object deletes it from both the management cluster and from cloud storage.</p>"},{"location":"admin-backup/#delete-backups","title":"Delete Backups","text":"<p>To remove a <code>velero</code> <code>Backup</code> from the management cluster, delete <code>backups.velero.io</code> object(s), such as with the following command:</p> <pre><code>kubectl delete backups.velero.io -n kcm-system &lt;velero-backup-name&gt;\n</code></pre> <p>Hint</p> <p> The command above only removes objects from the cluster; the data continues to persist on the cloud storage.</p> <p>The deleted object will be recreated in the cluster if its <code>BackupStorageLocation</code> <code>.spec.backupSyncPeriod</code> is set and does not equal <code>0</code>.</p> <p>To delete a <code>velero</code> <code>Backup</code> from the management cluster and from cloud storage, create the following <code>DeleteBackupRequest</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: DeleteBackupRequest\nmetadata:\n  name: delete-backup-completely\n  namespace: kcm-system\nspec:\n  backupName: &lt;velero-backup&gt;\n</code></pre> <p>Warning</p> <p> Deletion of a <code>Backup</code> object via the <code>DeleteBackupRequest</code> deletes it from both the management cluster and from the cloud storage.</p> <p>Optionally, delete the created <code>DeleteBackupRequest</code> object from the cluster after <code>Backup</code> has been deleted.</p> <p>For reference, follow the official documentation.</p>"},{"location":"admin-backup/#customization","title":"Customization","text":"<p>This section covers different topics of customization regarding backing up and restoring k0rdent.</p>"},{"location":"admin-backup/#velero-installation","title":"Velero installation","text":"<p>The Velero helm chart is supplied with the k0rdent helm chart and is enabled by default. There are 2 ways of customizing the chart values:</p> <ol> <li> <p>Install using <code>helm</code> and add corresponding parameters to the <code>helm install</code> command.</p> <p>Note</p> <p> Only a plugin is required during restoration; the other parameters are optional.</p> <p>For example, this command installs k0rdent via <code>helm install</code> with a configured plugin, <code>BackupStorageLocation</code> and propagated credentials:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set-file velero.credentials.secretContents.cloud=&lt;FULL PATH TO FILE&gt; \\\n --set velero.credentials.useSecret=true \\\n --set velero.backupsEnabled=true \\\n --set velero.configuration.backupStorageLocation[0].name=&lt;backup-storage-location-name&gt; \\\n --set velero.configuration.backupStorageLocation[0].provider=&lt;provider-name&gt; \\\n --set velero.configuration.backupStorageLocation[0].bucket=&lt;bucket-name&gt; \\\n --set velero.configuration.backupStorageLocation[0].config.region=&lt;region&gt; \\\n --set velero.initContainers[0].name=velero-plugin-for-&lt;provider-name&gt; \\\n --set velero.initContainers[0].image=velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt; \\\n --set velero.initContainers[0].volumeMounts[0].mountPath=/target \\\n --set velero.initContainers[0].volumeMounts[0].name=plugins\n</code></pre> </li> <li> <p>Create or modify the existing <code>Management</code> object in the <code>.spec.config.kcm</code>.</p> <p>Note</p> <p> Only a plugin is required during restoration; the other parameters are optional.</p> <p>For example, this is a <code>Management</code> object with a configured plugin and enabled metrics:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  # ...\n  core:\n    kcm:\n      config:\n        velero:\n          initContainers:\n          - name: velero-plugin-for-&lt;provider-name&gt;\n            image: velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt;\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - mountPath: /target\n              name: plugins\n          metrics:\n            enabled: true\n  # ...\n</code></pre> </li> </ol> <p>To fully disable <code>velero</code>, set the <code>velero.enabled</code> parameter to <code>false</code>.</p>"},{"location":"admin-backup/#schedule-expression-format","title":"Schedule Expression Format","text":"<p>The <code>ManagementBackup</code> <code>.spec.schedule</code> field accepts a correct Cron expression, along with the nonstandard predefined scheduling definitions and an extra definition <code>@every</code> with a number and a valid time unit (valid time units are <code>ns</code>, <code>us</code> (or <code>\u00b5s</code>), <code>ms</code>, <code>s</code>, <code>m</code>, <code>h</code>).</p> <p>The following list contains acceptable <code>.spec.schedule</code> example values:</p> <ul> <li><code>0 */1 * * *</code> (standard Cron expression)</li> <li><code>@hourly</code> (nonstandard predefined definition)</li> <li><code>@every 1h</code> (extra definition)</li> </ul>"},{"location":"admin-backup/#putting-extra-objects-in-a-management-backup","title":"Putting Extra Objects in a Management Backup","text":"<p>If you need to back up objects other than those backed up by default, you can add the label <code>k0rdent.mirantis.com/component=\"kcm\"</code> to these objects.</p> <p>All objects containing the label will be automatically added to the management backup.</p>"},{"location":"admin-before/","title":"Before you start","text":"<p>Before you start working with k0rdent, it helps to understand a few basics.</p>"},{"location":"admin-before/#how-k0rdent-works","title":"How k0rdent works","text":"<p>k0rdent has several important subsystems, notably:</p> <ul> <li>KCM - k0rdent Cluster Manager: KCM wraps and manages Kubernetes Cluster API, and lets you treat clusters as Kubernetes objects. Within a k0rdent management cluster, you'll have a <code>ClusterDeployment</code> object that represents a deployed cluster, with <code>Machine</code> objects, and so on. When you create a <code>ClusterDeployment</code>, k0rdent deploys the cluster. When you delete it, k0rdent deletes it, and so on.</li> <li>KSM - k0rdent Service Manager: KSM wraps and manages several interoperating open source projects such as Helm and Sveltos, which let you treat services and applications as Kubernetes objects.</li> </ul> <p>Together, KCM and KSM interoperate to create a complete, template-driven system for defining and managing Internal Development Platforms (IDPs) comprising suites of services, plus a cluster and its components as realized on a particular cloud or infrastructure substrate.</p> <ul> <li> <p>ClusterAPI providers: ClusterAPI uses <code>providers</code> to manage different clouds and infrastructures, including bare metal. k0rdent ships with providers for AWS, Azure, OpenStack and vSphere, and you can add additional providers in order to control other clouds or infrastructures that ClusterAPI supports.</p> </li> <li> <p>Templates: When you create a cluster, that cluster is based on a template, which specifies all of the various information about the cluster, such as where to find images, and so on. These templates get installed into k0rdent, but they don't do anything until you reference them in a <code>ClusterDeployment</code> that represents an actual cluster.</p> <p></p> <p>k0rdent can also manage these clusters, upgrading them, scaling them, or installing software and services.</p> </li> <li> <p>Services: To add (or manage) services, you also use templates. These <code>ServiceTemplate</code> objects are like <code>ClusterTemplate</code> objects, in that you install them into the cluster, but until they're actually referenced, they don't do anything. When you reference a <code>ServiceTemplate</code> as part of a <code>ClusterDeployment</code>, k0rdent knows to install that service into that cluster.</p> <p></p> <p>These services can be actual services, such as Nginx or Kyverno, or they can be user applications.</p> </li> </ul>"},{"location":"admin-before/#how-credentials-work","title":"How Credentials work","text":"<p>Of course you can't do any of this without permissions. As a human, you can log into, say, AWS, and tell it to create a new instance on which you are going to install Kubernetes, but how does k0rdent get that permission? It gets it through the use of <code>Credential</code> objects.</p> <p>When you create a <code>ClusterDeployment</code> or deploy an application, you include a reference to a <code>Credential</code> object that has been installed in the k0rdent management cluster. Depending on whether the target infrastructure is AWS, Azure, or something else, that <code>Credential</code> might reference an access key and secret, or it might reference a service provider, but all of that gets abstracted out by the time you get to the <code>Credential</code>, which is what you'll actually reference.</p> <p></p> <p>By abstracting everything out to create a standard <code>Credential</code> object, users never have to have access to actual credentials (lowercase \"c\"). This enables the administrator to keep those credentials private, and to rotate them as necessary without disturbing users or their applications. The administrator simply updates the <code>Credential</code> object and everything continues to work.</p> <p>You can find more information on creating these <code>Credential</code> objects in the Credentials chapter.</p>"},{"location":"admin-before/#k0rdent-and-gitops","title":"k0rdent and GitOps","text":"<p>At its heart, k0rdent is a Kubernetes-native way to declaratively specify what should be happening in the infrastructure and have that maintained. In other words, if you want to, say, scale up a cluster, you would give that cluster a new definition that includes the additional nodes, and then k0rdent, seeing that reality doesn't match that definition, will make it happen.</p> <p>In some ways that is very similar to GitOps, in which you commit definitions and tools such as Flux or ArgoCD ensure that reality matches the definition. We can say that k0rdent is GitOps-compatible, in the sense that you can (and should) consider storing k0rdent templates and YAML object definitions in Git repos, and can (and may want to) use GitOps tools like ArgoCD to modify and manage them upstream of k0rdent itself.</p> <p>The main difference is that k0rdent's way of representing clusters and services is fully compliant with Kubernetes-native tools like ClusterAPI, Sveltos and Helm. So you can, in fact, port much of what you do with k0rdent templates and objects directly to other solution environments that leverage these standard tools.</p>"},{"location":"admin-before/#the-k0rdent-initialization-process","title":"The k0rdent initialization process","text":""},{"location":"admin-before/#the-process","title":"The process","text":"<p>The k0rdent initialization process involves tools such as Helm and FluxCD.</p> <ol> <li><code>helm install kcm</code> brings up the bootstrap components (yellow in the above diagram).</li> <li><code>kcm-controller-manager</code> sets up webhooks to validate its <code>CustomResource</code> objects, then cert-manager handles the webhooks\u2019 certificates.</li> <li><code>kcm-controller-manager</code> generates a <code>Release</code> object corresponding to the KCM helm chart version.</li> <li><code>kcm-controller-manager</code> (or rather the release-controller inside it) generates template objects (<code>ProviderTemplate</code>/<code>ClusterTemplate</code>/<code>ServiceTemplate</code>) corresponding to a <code>Release</code> to be further processed.</li> <li><code>kcm-controller-manager</code> generates a <code>HelmRelease</code> object for every standard template. Note that this includes the KCM helm chart itself.</li> <li>Flux (source-controller and helm-controller pods) reconciles the HelmRelease objects. In other words, it installs all the helm charts referred to in the templates. After this point, the deployment is completely controlled by Flux.</li> <li><code>kcm-controller-manager</code> creates a <code>Management</code> object that refers to the above <code>Release</code> and the <code>ProviderTemplate</code> objects. The <code>Management</code> object represents the k0rdent management cluster as a whole. The management cluster Day-2 operations (such as upgrade) are  executed by manipulating the <code>Release</code> and <code>Management</code> objects.</li> <li><code>kcm-controller-manager</code> generates an empty <code>AccessManagement</code> object. <code>AccessManagement</code> defines access rules for <code>ClusterTemplate</code>/<code>ServiceTemplate</code> propagation across user namespaces. Further, the <code>AccessManagement</code> might be edited and used along with admin-created <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects.</li> </ol>"},{"location":"admin-create-multiclusterservice/","title":"Deploy beach-head services using MultiClusterService","text":"<p>The <code>MultiClusterService</code> object is used to deploy beach-head services on multiple matching clusters.</p>"},{"location":"admin-create-multiclusterservice/#creation","title":"Creation","text":"<p>You can create the <code>MultiClusterService</code> object with the following YAML:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: MultiClusterService\nmetadata:\n  name: &lt;name&gt;\nspec:\n  clusterSelector:\n    matchLabels:\n      &lt;key1&gt;: &lt;value1&gt;\n      &lt;key2&gt;: &lt;value2&gt;\n      . . .\n serviceSpec:\n    services:\n    - template: &lt;servicetemplate-1-name&gt;\n      name: &lt;release-name&gt;\n      namespace: &lt;release-namespace&gt;\n    priority: 100\n</code></pre>"},{"location":"admin-create-multiclusterservice/#matching-multiple-clusters","title":"Matching Multiple Clusters","text":"<p>Consider the following example where two clusters have been deployed using <code>ClusterDeployment</code> objects:</p> <p>Command:</p> <p><pre><code>kubectl get clusterdeployments.k0rdent.mirantis.com -n kcm-system\n</code></pre> <pre><code>NAME             READY   STATUS\ndev-cluster-1   True    ClusterDeployment is ready\ndev-cluster-2   True    ClusterDeployment is ready\n</code></pre></p> <p>Command: <pre><code> kubectl get cluster -n kcm-system --show-labels\n</code></pre> <pre><code>NAME           CLUSTERCLASS     PHASE         AGE     VERSION   LABELS\ndev-cluster-1                  Provisioned   2h41m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-1,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\ndev-cluster-2                  Provisioned   3h10m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-2,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\n</code></pre></p> <p>The <code>dev-cluster-1</code> <code>ClusterDeployment</code> beach-head services are specified as: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: dev-cluster-1\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 100\n  . . .\n</code></pre></p> <p>The <code>dev-cluster-2</code> <code>ClusterDeployment</code> beach-head services are specified as: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: dev-cluster-2\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 500\n  . . .\n</code></pre></p> <p>Note</p> <p> See Deploy beach-head Services using Cluster Deployment for how to use beach-head services with ClusterDeployment.</p> <p>Now create the following <code>global-ingress</code> <code>MultiClusterService</code> object: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: MultiClusterService\nmetadata:\n  name: global-ingress\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    priority: 300\n</code></pre></p> <p>This MultiClusterService will match any CAPI cluster with the label <code>app.kubernetes.io/managed-by: Helm</code> and deploy chart version 4.11.3 of ingress-nginx service on it.</p>"},{"location":"admin-create-multiclusterservice/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Refer to \"Configuring Custom Values\" in Deploy beach-head Services using Cluster Deployment for more information on using custom values.</p>"},{"location":"admin-create-multiclusterservice/#templating-custom-values","title":"Templating Custom Values","text":"<p>Refer to \"Templating Custom Values\" in Deploy beach-head Services using Cluster Deployment for more information dynamic custom values.</p>"},{"location":"admin-create-multiclusterservice/#services-priority-and-conflict","title":"Services Priority and Conflict","text":"<p>The <code>.spec.serviceSpec.priority</code> field specifies the priority for the services managed by a ClusterDeployment or MultiClusterService object.</p> <p>Considering the example above:</p> <ol> <li>ClusterDeployment <code>dev-cluster-1</code> manages deployment of kyverno (v3.2.6) and ingress-nginx (v4.11.0) with <code>priority=100</code> on its cluster.</li> <li>ClusterDeployment <code>dev-cluster-2</code> manages deployment of ingress-nginx (v4.11.0) with <code>priority=500</code> on its cluster.</li> <li>MultiClusterService <code>global-ingress</code> manages deployment of ingress-nginx (v4.11.3) with <code>priority=300</code> on both clusters. This scenario presents a conflict on both the clusters as the MultiClusterService is attempting to deploy v4.11.3 of ingress-nginx on both whereas the ClusterDeployment for each is attempting to deploy v4.11.0 of ingress-nginx.</li> </ol> <p>This is where <code>.spec.serviceSpec.priority</code> can be used to specify who gets the priority. Higher number means higer priority and vice versa. In this example:</p> <ol> <li>MultiClusterService \"global-ingress\" will take precedence over ClusterDeployment \"dev-cluster-1\" and ingress-nginx (v4.11.3) defined in MultiClusterService object will be deployed on the cluster.</li> <li>ClusterDeployment \"dev-cluster-2\" will take precedence over MultiClusterService \"global-ingress\" and ingress-nginx (v4.11.0) defined in ClusterDeployment object will be deployed on the cluster.</li> </ol> <p>Note</p> <p>If <code>priority</code> values are equal, the first one to reach the cluster wins and deploys its beach-head services.</p>"},{"location":"admin-create-multiclusterservice/#checking-status","title":"Checking Status","text":"<p>The status for the <code>MultiClusterService</code> object shows the deployment status for the beach-head services managed by it on each of the CAPI target clusters that it matches. Consider the same example where 2 ClusterDeployments and 1 MultiClusterService is deployed. The status for the <code>global-ingress</code> <code>MultiClusterService</code> appears as:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: MultiClusterService\nmetadata:\n  . . .\n  name: global-ingress\n  resourceVersion: \"38146\"\n  . . .\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    . . .\n  . . .\nstatus:\n  conditions:\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: \"\"\n    reason: Succeeded\n    status: \"True\"\n    type: SveltosClusterProfileReady\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: MultiClusterService is ready\n    reason: Succeeded\n    status: \"True\"\n    type: Ready\n  observedGeneration: 1\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary p--dev-cluster-2-capi-dev-cluster-2 managing it.\n      reason: Failed\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary p--dev-cluster-2-capi-dev-cluster-2\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>The status under <code>.status.services</code> shows a conflict for <code>dev-cluster-2</code> as expected because the <code>MultiClusterService</code> has a lower priority. On the other hand, it shows provisioned for <code>dev-cluster-1</code> because the <code>MultiClusterService</code> has a higher priority.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . . \n  name: dev-cluster-1\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 100\n    . . .\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary global-ingress-capi-dev-cluster-1 managing it.\n      reason: Provisioning\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T07:44:43Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary global-ingress-capi-dev-cluster-1\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>The status under <code>.status.services</code> for the <code>ClusterDeployment</code> <code>dev-cluster-1</code> shows that it is managing Kyverno but unable to manage ingress-nginx because another object with higher priority is managing it, so it shows a conflict instead.</p> <p>On the otherhand, the <code>dev-cluster-2</code> <code>ClusterDeployment</code> has a higher priority: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  name: dev-cluster-2\n  namespace: kcm-system\n  resourceVersion: \"30889\"\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 500\n    . . .\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>The status under <code>.status.services</code> for <code>ClusterDeployment</code> <code>dev-cluster-2</code> shows that it is managing ingress-nginx, as expected because it has a higher priority.</p>"},{"location":"admin-create-multiclusterservice/#parameter-list","title":"Parameter List","text":"<p>Refer to \"Parameter List\" in Deploy beach-head Services using Cluster Deployment for more information.</p>"},{"location":"admin-creating-clusters/","title":"Creating and lifecycle-managing child clusters","text":"<p>Once you've installed k0rdent, you can use it to create, manage, update and even upgrade clusters.</p>"},{"location":"admin-creating-clusters/#deploying-a-cluster","title":"Deploying a Cluster","text":"<p>k0rdent is designed to simplify the process of deploying and managing Kubernetes clusters across various cloud platforms. It does this through the use of <code>ClusterDeployment</code> objects, which include all of the information k0rdent needs to know in order to create the cluster you're looking for. This <code>ClusterDeployment</code> system relies on predefined templates and credentials. </p> <p>A cluster deployment typically involves:</p> <ol> <li>Setting up credentials for the infrastructure provider (for example, AWS, vSphere).</li> <li>Choosing a template that defines the desired cluster configuration (for example, number of nodes, instance types).</li> <li>Submitting the configuration for deployment and monitoring the process.</li> </ol> <p>Follow these steps to deploy a standalone Kubernetes cluster tailored to your specific needs:</p> <ol> <li> <p>Create the <code>Credential</code> object</p> <p>Credentials are essential for k0rdent to communicate with the infrastructure provider (for example, AWS, Azure, vSphere). These credentials enable k0rdent to provision resources such as virtual machines, networking components, and storage.</p> <p><code>Credential</code> objects are generally created ahead of time and made available to users, so before you look into creating a new one be sure what you're looking for doesn't already exist. You can see all of the existing <code>Credential</code> objects by  querying the management cluster:</p> <pre><code>kubectl get credentials --all-namespaces\n</code></pre> <p>If the <code>Credential</code> you need doesn't yet exist, go ahead and create it.</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure provider. Follow the instructions in the chapter about credential management, as well as the specific instructions for your target infrastructure.</p> <p>Tip</p> <p>Double-check to make sure that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Select a Template</p> <p>Templates in k0rdent are predefined configurations that describe how to set up the cluster. Templates include details such as:</p> <ul> <li>The number and type of control plane and worker nodes</li> <li>Networking settings</li> <li>Regional deployment preferences</li> </ul> <p>Templates act as a blueprint for creating a cluster. To see the list of available templates, use the following command:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-1-0           true\naws-eks-0-1-0                   true\naws-hosted-cp-0-1-0             true\naws-standalone-cp-0-1-0         true\nazure-aks-0-1-0                 true\nazure-hosted-cp-0-1-0           true\nazure-standalone-cp-0-1-0       true\nopenstack-standalone-cp-0-1-0   true\nvsphere-hosted-cp-0-1-0         true\nvsphere-standalone-cp-0-1-0     true\n</code></pre></p> <p>You can then get information on the actual template by describing it, as in:</p> <pre><code>kubectl describe clustertemplate aws-standalone-cp-0-1-0 -n kcm-system\n</code></pre> </li> <li> <p>Create a ClusterDeployment YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is the main configuration file that defines your cluster's specifications. It includes:</p> <ul> <li>The template to use</li> <li>The credentials for the infrastructure provider</li> <li>Optional customizations such as instance types, regions, and networking</li> </ul> <p>Create a <code>ClusterDeployment</code> configuration in a YAML file, following this structure:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>You will of course want to replace the placeholders with actual values. (For more information about <code>dryRun</code> see Understanding the Dry Run.) For example, this is a simple AWS infrastructure provider <code>ClusterDeployment</code>:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-1-0\n  credential: aws-credential\n  dryRun: false\n  config:\n    clusterLabels: {}\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> Note that the <code>.spec.credential</code> value should match the <code>.metadata.name</code> value of a created <code>Credential</code> object.</p> </li> <li> <p>Apply the Configuration</p> <p>Once the <code>ClusterDeployment</code> configuration is ready, apply it to the k0rdent management cluster:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits your deployment request to k0rdent. If you've set <code>dryRun</code> to <code>true</code> you can observe what would happen. Otherwise, k0rdent will go ahead and begin provisioning the necessary infrastructure.</p> </li> <li> <p>Verify Deployment Status</p> <p>After submitting the configuration, verify that the <code>ClusterDeployment</code> object has been created successfully:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output shows the current status and any errors.</p> </li> <li> <p>Monitor Provisioning</p> <p>k0rdent will now start provisioning resources (for example, VMs and networks) and setting up the cluster. To monitor this process, run:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>Tip</p> <p>For a detailed view of the provisioning process, use the <code>clusterctl describe</code> command (note that this requires the <code>clusterctl</code> CLI):</p> <pre><code>clusterctl describe cluster &lt;cluster-name&gt; -n &lt;namespace&gt; --show-conditions all\n</code></pre> </li> <li> <p>Retrieve the Kubernetes Configuration</p> <p>When provisioning is complete, retrieve the kubeconfig file for the new cluster. This file enables you to interact with the cluster using <code>kubectl</code>:</p> <p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> You can then use this file to access the cluster, as in:</p> <pre><code>export KUBECONFIG=kubeconfig\nkubectl get pods -A\n</code></pre> <p>Store the kubeconfig file securely, as it contains authentication details for accessing the cluster.</p> </li> </ol>"},{"location":"admin-creating-clusters/#updating-a-single-standalone-cluster","title":"Updating a Single Standalone Cluster","text":"<p>k0rdent <code>ClusterTemplate</code> objects are immutable, so the only way to change a <code>ClusterDeployment</code> is to change the template that forms its basis. </p> <p>To update the <code>ClusterDeployment</code>, modify the <code>.spec.template</code> field to use the name of the new <code>ClusterTemplate</code>.  This enables you to apply changes to the cluster configuration. These changes will then be applied to the actual  cluster. For example, if the cluster currently uses <code>t2.large</code> instances, that will be specified in its current template.  To change the cluster to use <code>t2.xlarge</code> instances, you would simply apply a template that references that new size;  k0rdent will then realize the cluster is out of sync and will attempt to remedy the situation by updating the cluster.</p> <p>Follow these steps to update the <code>ClusterDeployment</code>:</p> <ol> <li> <p>Patch the <code>ClusterDeployment</code> with the new template</p> <p>Run the following command, replacing the placeholders with the appropriate values:</p> <pre><code>kubectl patch clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; --patch '{\"spec\":{\"template\":\"&lt;new-template-name&gt;\"}}' --type=merge\n</code></pre> </li> <li> <p>Check the status of the <code>ClusterDeployment</code></p> <p>After applying the patch, verify the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt;\n</code></pre> </li> <li> <p>Inspect the detailed status</p> <p>For more details, use the <code>-o=yaml</code> option to check the <code>.status.conditions</code> field:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; -o=yaml\n</code></pre> </li> </ol> <p>Note that not all updates are possible; <code>ClusterTemplateChain</code> objects limit what templates can be applied.  Consider, for example, this <code>ClusterTemplateChain</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws-standalone-cp-0-1-0\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-2\n      availableUpgrades:\n        - name: aws-standalone-cp-0-1-0\n    - name: aws-standalone-cp-0-1-0\n</code></pre> <p>As you can see from the <code>.spec</code>, the <code>aws-standalone-co-0-1-0</code> template can be applied to a cluster that also uses the <code>aws-standalone-co-0-1-0</code> template, or it can be used as an upgrade from a cluster that uses <code>aws-standalone-co-0.0.2</code>. You wouldn't be able to use this template to update a cluster that uses any other <code>ClusterTemplate</code>.</p> <p>Similarly, the <code>AccessManagement</code> object must have properly configured <code>spec.accessRules</code> with a list of allowed  <code>ClusterTemplateChain</code> object names and their namespaces. For more information, see Template Life Cycle Management.</p> <p>Note</p> <p>Support for displaying all available Cluster Templates for updates in the <code>ClusterDeployment</code> status is planned.</p>"},{"location":"admin-creating-clusters/#cleanup","title":"Cleanup","text":"<p>Especially when you are paying for cloud resources, it's crucial to clean up when you're finished. Fortunately, k0rdent makes that straightforward.</p> <p>Because a Kubernetes cluster is represented by a <code>ClusterDeployment</code>, when you delete that <code>ClusterDeployment</code>, k0rdent deletes the cluster. For example:</p> <p><pre><code>kubectl delete clusterdeployment my-cluster-deployment -n kcm-system\n</code></pre> <pre><code>ClusterDeployment my-cluster-deployment deleted.\n</code></pre></p> <p>Note that it takes time to delete these resources.</p>"},{"location":"admin-credentials/","title":"The Credential System","text":"<p>In order for k0rdent to be able to take action on a particular provider, it must have the proper credentials. This chapter explains how that system works.</p>"},{"location":"admin-credentials/#the-process","title":"The process","text":"<p>In order to pass credentials to k0rdent so it can take action, the following has to happen:</p> <ol> <li> <p>The lead platform engineer, or whoever has access to the actual provider credentials, creates a <code>Secret</code> that includes that information. For example, for an AWS cluster, it might look like this:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n    name: aws-cluster-identity-secret\n    namespace: kcm-system\ntype: Opaque\nstringData:\n    AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n    SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Once this secret is created, it can be referenced without the user having access to the content, and thus the actual credentials.</p> </li> <li> <p>A provider-specific <code>ClusterIdentity</code> gets created. The <code>ClusterIdentity</code> references the <code>Secret</code> from step one. For example, for an AWS cluster, this object might look like this:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Notice that it references the <code>aws-cluster-identity-secret</code> we created earlier. It also specifies the namespaces in which this <code>ClusterIdentity</code> can be used. (In this case there are no restrictions.)</p> </li> <li> <p>Now you can create a <code>Credential</code> object that references the <code>ClusterIdentity</code>, thus making the credentials available and specifying the namespaces where it can be used. Continuing our AWS example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-credential\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> Notice that it references the previous <code>ClusterIdentity</code> (in this case an <code>AWSClusterStaticIdentity</code>). Also notice that you can use the <code>.spec.description</code> field to add additional text about the <code>Credential</code> so users can choose if multiple <code>Credential</code> objects are available.</p> </li> <li> <p>Finally, when you create a <code>ClusterDeployment</code>, you reference the <code>Credential</code> object in order to enable k0rdent to pass that information to the infrastructure provider:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n    name: my-aws-clusterdeployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-1-0\n  credential: aws-cluster-credential\n    config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n    instanceType: t3.small\n    worker:\n    instanceType: t3.small\n</code></pre> <p>As you can see, the user doesn't have to pass anything but the name of the <code>Credential</code> in order to deploy the cluster. So all an administrator has to do is add these <code>Credential</code>objects to the system and make them available. Note also that the <code>Credential</code> has to be available in the <code>ClusterDeployment</code>s namespace. (See Cloud provider credentials propagation for more information on how that works. )</p> </li> <li> <p>Optionally, certain credentials MAY be propagated to the <code>ClusterDeployment</code> after it is created.</p> <p>The following diagram illustrates the process:</p> <pre><code>flowchart TD\n  Step1[\"&lt;b&gt;Step 1&lt;/b&gt; (Lead Engineer):&lt;br/&gt;Create ClusterIdentity and Secret objects where ClusterIdentity references Secret\"]\n  Step1 --&gt; Step2[\"&lt;b&gt;Step 2&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create Credential object referencing ClusterIdentity\"]\n  Step2 --&gt; Step3[\"&lt;b&gt;Step 3&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create ClusterDeployment referencing Credential object\"]\n  Step3 --&gt; Step4[\"&lt;b&gt;Step 4&lt;/b&gt; (Any Engineer):&lt;br/&gt;Apply ClusterDeployment, wait for provisioning &amp; reconciliation, then propagate credentials to nodes if necessary\"]</code></pre> <p>By design steps 1 and 2 should be executed by the lead engineer who has access to the credentials. Thus credentials could be used by engineers without a need to have access to actual credentials or underlying resources, like <code>ClusterIdentity</code>.</p> </li> </ol>"},{"location":"admin-credentials/#cloud-provider-credentials-propagation","title":"Cloud provider credentials propagation","text":"<p>Some components in the cluster deployment require cloud provider credentials to be passed for proper functioning. For example, Cloud Controller Manager (CCM) requires provider credentials to create load balancers and provide other functionality.</p> <p>This poses a challenge for credentials delivery. Currently <code>cloud-init</code> is used to pass all necessary credentials, but this approach has several problems:</p> <ul> <li>Credentials are stored unencrypted in the instance metadata.</li> <li>Rotation of the credentials is impossible without complete instance   redeployment.</li> <li>Possible leaks, since credentials are copied to several <code>Secret</code> objects   related to bootstrap data.</li> </ul> <p>To solve these problems in k0rdent we're using the Sveltos controller, which can render the CCM template with all necessary data from the CAPI provider resources (like <code>ClusterIdentity</code>) and can create secrets directly on the cluster deployment.</p> <p>Note</p> <p> CCM template examples can be found in <code>*-credentials.yaml</code> here. Look for the <code>ConfigMap</code> object that has the <code>projectsveltos.io/template: \"true\"</code> annotation and <code>*-resource-template</code> as the object name.</p> <p>This eliminates the need to pass anything credentials-related to <code>cloud-init</code> and makes it possible to rotate credentials automatically without the need for instance redeployment.</p> <p>Also, this automation makes it possible to separate roles and responsibilities so that only the lead engineer has access to credentials, and other engineers can use them without seeing values and even any access to underlying infrastructure platform.</p> <p>The process is fully automated and credentials will be propagated automatically within the <code>ClusterDeployment</code> reconciliation process; user only needs to provide the correct <code>Credential</code> object.</p>"},{"location":"admin-credentials/#provider-specific-notes","title":"Provider-specific notes","text":"<p>Since this feature depends on the provider, it's important to review any provider-specific notes and clarifications.</p> <p>Note</p> <p>More detailed research notes can be found here.</p>"},{"location":"admin-credentials/#aws","title":"AWS","text":"<p>Since AWS uses roles, which are assigned to instances, no additional credentials will be created.</p> <p>The AWS provider supports 3 types of <code>ClusterIdentity</code> and, which one to use depends on your specific use case. More information regarding CAPA <code>ClusterIdentity</code> resources can be found in the CRD Reference.</p>"},{"location":"admin-credentials/#azure","title":"Azure","text":"<p>Currently the Cluster API Azure (CAPZ) provider creates <code>azure.json</code> <code>Secret</code> objects in the same namespace as the <code>Cluster</code> object. By design they should be referenced in the <code>cloud-init</code> YAML later during bootstrap process.</p> <p>In k0rdent these <code>Secret</code> objects aren't used and will not be added to the <code>cloud-init</code>, but engineers can access them without restrictions, which is a security issue.</p>"},{"location":"admin-credentials/#openstack","title":"OpenStack","text":"<p>For OpenStack, CAPO relies on a <code>clouds.yaml</code> file. In k0rdent, you provide this file in a Kubernetes <code>Secret</code> that references OpenStack credentials (ideally application credentials for enhanced security). During reconciliation, KCM automatically generates the cloud-config required by OpenStack\u2019s cloud-controller-manager.</p> <p>For more details, refer to the KCM OpenStack Credential Propagation doc.</p>"},{"location":"admin-credentials/#adopted-clusters","title":"Adopted clusters","text":"<p>Credentials for adopted clusters consist of a secret containing a kubeconfig file to access the existing kubernetes cluster.  The kubeconfig file for the cluster should be contained in the value key of the secret object. The following is an example of  a secret that contains the kubeconfig for an adopted cluster. To create this secret, first create or obtain a kubeconfig file  for the cluster that is being adopted and then run the following command to base64 encode it:</p> <pre><code>cat kubeconfig | base64 -w 0\n</code></pre> <p>Once you have obtained a base64 encoded kubeconfig file create a secret:</p> <pre><code>apiVersion: v1\ndata:\n  value: &lt;base64 encoded kubeconfig file&gt;\nkind: Secret\nmetadata:\n  name: adopted-cluster-kubeconf\n  namespace: &lt;namespace&gt;\ntype: Opaque\n</code></pre>"},{"location":"admin-credentials/#the-credential-distribution-system","title":"The Credential Distribution System","text":"<p>k0rdent provides a mechanism to distribute <code>Credential</code> objects across namespaces using the <code>AccessManagement</code> object. This object defines a set of <code>accessRules</code> that determine how credentials are distributed.</p> <p>Each access rule specifies:</p> <ol> <li>The target namespaces where credentials should be delivered.</li> <li>A list of <code>Credential</code> names to distribute to those namespaces.</li> </ol> <p>The KCM controller copies the specified <code>Credential</code> objects from the <code>system</code> namespace to the target namespaces based on the <code>accessRules</code> in the <code>AccessManagement</code> spec.</p> <p>Info</p> <p> Access rules can also include <code>Cluster</code> and <code>Service</code> Template Chains (<code>ClusterTemplateChain</code> objects and <code>ServiceTemplateChain</code> objects) to distribute templates to target namespaces. For more details, read: Template Life Cycle Management.</p>"},{"location":"admin-credentials/#how-to-configure-credential-distribution","title":"How to Configure Credential Distribution","text":"<p>To configure the distribution of <code>Credential</code> objects:</p> <ol> <li>Edit the <code>AccessManagement</code> object.</li> <li>Populate the <code>.spec.accessRules</code> field with the list of <code>Credential</code> names and the target namespaces.</li> </ol> <p>Here\u2019s an example configuration:</p> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - dev\n        - test\n    credentials:\n      - aws-demo\n      - azure-demo\n</code></pre> <p>In this example, the <code>aws-demo</code> and <code>azure-demo</code> <code>Credential</code> objects will be distributed to the <code>dev</code> and <code>test</code> namespaces.</p>"},{"location":"admin-hosted-control-planes/","title":"Deploying a Hosted Control Plane","text":"<p>A hosted control plane is a Kubernetes setup in which the control plane components (such as the API server,  etcd, and controllers) run inside the management cluster instead of separate controller nodes. This  architecture centralizes control plane management and improves scalability by sharing resources in the management cluster. Hosted control planes are managed by k0smotron.</p> <p>Instructions for setting up a hosted control plane vary slighting depending on the provider.</p>"},{"location":"admin-hosted-control-planes/#aws-hosted-control-plane-deployment","title":"AWS Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on AWS: </p> <ol> <li> <p>Prerequisites</p> <p>Before proceeding, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28 or later) deployed on AWS with k0rdent installed.</li> <li>A default storage class configured on the management cluster to support Persistent Volumes.</li> <li>The VPC ID where the worker nodes will be deployed.</li> <li>The Subnet ID and Availability Zone (AZ) for the worker nodes.</li> <li>The AMI ID for the worker nodes (Amazon Machine Image ID for the desired OS and Kubernetes version).</li> </ul> <p>Important</p> <p>All control plane components for your hosted cluster will reside in the management cluster, and the management cluster  must have sufficient resources to handle these additional workloads.</p> </li> <li> <p>Networking</p> <p>To deploy a hosted control plane, the necessary AWS networking resources must already exist or be created. If you're  using the same VPC and subnets as your management cluster, you can reuse these resources.</p> <p>If your management cluster was deployed using the Cluster API Provider AWS (CAPA), you can gather the required  networking details using the following commands:</p> <p>Retrieve the VPC ID: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre></p> <p>Retrieve Subnet ID: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre></p> <p>Retrieve Availability Zone: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre></p> <p>Retrieve Security Group: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre></p> <p>Retrieve AMI ID: <pre><code>kubectl get awsmachinetemplate &lt;cluster-name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre></p> <p>Tip</p> <p>If you want to use different VPCs or regions for your management and hosted clusters, you\u2019ll need to configure additional networking, such as VPC peering, to allow communication between them.</p> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>Once you've collected all the necessary data, you can create the <code>ClusterDeployment</code> manifest. This file tells k0rdent how to  deploy and manage the hosted control plane. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted-cp\nspec:\n  template: aws-hosted-cp-0-1-0\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    vpcID: vpc-0a000000000000000\n    region: us-west-1\n    publicIP: true\n    subnets:\n      - id: subnet-0aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: true\n        natGatewayID: xxxxxx\n        routeTableId: xxxxxx\n      - id: subnet-1aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: false\n        routeTableId: xxxxxx\n    instanceType: t3.medium\n    securityGroupIDs:\n      - sg-0e000000000000000\n</code></pre> <p>Note</p> <p>The example above uses the <code>us-west-1</code> region, but you should use the region of your VPC.</p> </li> <li> <p>Generate the <code>ClusterDeployment</code> Manifest</p> <p>To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dynamically  inserts the appropriate values: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp-0-1-0\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n    {{- range $subnet := .spec.network.subnets }}\n      - id: \"{{ $subnet.resourceID }}\"\n        availabilityZone: \"{{ $subnet.availabilityZone }}\"\n        isPublic: {{ $subnet.isPublic }}\n        {{- if $subnet.isPublic }}\n        natGatewayId: \"{{ $subnet.natGatewayId }}\"\n        {{- end }}\n        routeTableId: \"{{ $subnet.routeTableId }}\"\n    {{- end }}\n    instanceType: t3.medium\n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre></p> <p>Save this template as <code>clusterdeployment.yaml.tpl</code>, then generate your manifest using the following command:</p> <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre> </li> <li> <p>Apply the <code>ClusterTemplate</code></p> <p>Nothing actually happens until you apply the <code>ClusterDeployment</code> manifest to create a new cluster deployment:</p> <pre><code>kubectl apply -f clusterdeployment.yaml -n kcm-system\n</code></pre> </li> </ol>"},{"location":"admin-hosted-control-planes/#deployment-tips","title":"Deployment Tips","text":"<p>Here are some additional tips to help with deployment:</p> <ol> <li> <p>Controller and Template Availability:</p> <p>Make sure the KCM controller image and templates are available in a public or accessible repository.</p> </li> <li> <p>Install Charts and Templates:</p> <p>If you're using a custom repository, run the following commands with the appropriate <code>kubeconfig</code>:</p> <pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/k0rdent/kcm/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/k0rdent/kcm/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> </li> <li> <p>Mark the Infrastructure as Ready:</p> <p>To scale up the <code>MachineDeployment</code>, manually mark the infrastructure as ready: <pre><code>kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}' -n kcm-system\n</code></pre> For more details on why this is necessary, click here.</p> </li> </ol>"},{"location":"admin-hosted-control-planes/#azure-hosted-control-plane-deployment","title":"Azure Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on Azure:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on Azure with k0rdent installed.</li> <li>A default storage class configured    on the management cluster to support Persistent Volumes.</li> </ul> <p>Note</p> <p>All control plane components for managed clusters will run in the management cluster. Make sure the management cluster    has sufficient CPU, memory, and storage to handle the additional workload.</p> </li> <li> <p>Gather Pre-existing Resources</p> <p>In a hosted control plane setup, some Azure resources must exist before deployment and must be explicitly  provided in the <code>ClusterDeployment</code> configuration. These resources can also be reused by the management cluster.</p> <p>If you deployed your Azure Kubernetes cluster using the Cluster API Provider for Azure (CAPZ), you can retrieve  the required information using the following commands:</p> <p>Location: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.location}}'\n</code></pre></p> <p>Subscription ID: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre></p> <p>Resource Group: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre></p> <p>VNet Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre></p> <p>Subnet Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre></p> <p>Route Table Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre></p> <p>Security Group Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre></p> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>After collecting the required data, create a <code>ClusterDeployment</code> manifest to configure the hosted control plane. It should look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-1-0\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"westus\"\n    subscriptionID: ceb131c7-a917-439f-8e19-cd59fe247e03\n    vmSize: Standard_A4_v2\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n</code></pre> </li> <li> <p>Generate the <code>ClusterDeployment</code> Manifest</p> <p>To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dynamically inserts  the appropriate values:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-1-0\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n</code></pre> Save this YAML as <code>clusterdeployment.yaml.tpl</code> and render the manifest with the following command: <pre><code>kubectl get azurecluster &lt;management-cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre></p> </li> <li> <p>Create the <code>ClusterDeployment</code></p> <p>To actually create the cluster, apply the <code>ClusterDeployment</code> manifest to the management cluster, as in:</p> <pre><code>kubectl apply clusterdeployment.yaml -n kcm-system\n</code></pre> </li> <li> <p>Manually update the <code>AzureCluster</code> object</p> <p>Due to a limitation in k0smotron, (see k0sproject/k0smotron#668),  after applying the <code>ClusterDeployment</code> manifest, you must manually update the status of the <code>AzureCluster</code> object.</p> <p>Use the following command to set the <code>AzureCluster</code> object status to <code>Ready</code>:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}'\n</code></pre> </li> </ol>"},{"location":"admin-hosted-control-planes/#important-notes-on-cluster-deletion","title":"Important Notes on Cluster Deletion","text":"<p>Due to these same k0smotron limitations, you must take some manual steps in order to delete a cluster properly:</p> <ol> <li> <p>Add a Custom Finalizer to the AzureCluster Object:</p> <p>To prevent the <code>AzureCluster</code> object from being deleted too early, add a custom finalizer:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": [\"manual\"]}}'\n</code></pre> </li> <li> <p>Delete the ClusterDeployment:</p> <p>After adding the finalizer, delete the <code>ClusterDeployment</code> object as usual. Confirm that all <code>AzureMachines</code> objects have been deleted successfully.</p> </li> <li> <p>Remove Finalizers from Orphaned AzureMachines:</p> <p>If any <code>AzureMachines</code> are left orphaned, delete their finalizers manually after confirming no VMs remain in Azure. Use this command to remove the finalizer:</p> <pre><code>kubectl patch azuremachine &lt;machine-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": []}}'\n</code></pre> </li> <li> <p>Allowing Updates to Orphaned Objects:</p> <p>If Azure admission controls prevent updates to orphaned objects, you must disable the associated <code>MutatingWebhookConfiguration</code> by deleting it:</p> <pre><code>kubectl delete mutatingwebhookconfiguration &lt;webhook-name&gt;\n</code></pre> </li> </ol>"},{"location":"admin-hosted-control-planes/#vsphere-hosted-control-plane-deployment","title":"vSphere Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on vSphere. </p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on vSphere with k0rdent installed.</li> </ul> <p>All control plane components for managed clusters will reside in the management cluster, so make sure the management  cluster has sufficient resources (CPU, memory, and storage) to handle these workloads.</p> </li> <li> <p>Create the <code>ClusterDeployment</code> Manifest</p> <p>The <code>ClusterDeployment</code> manifest for vSphere-hosted control planes is similar to standalone control plane deployments.  For a detailed list of parameters, refer to our discussion of Template parameters for vSphere.</p> <p>Important</p> <p>The vSphere provider requires you to specify the control plane endpoint IP before deploying the cluster. This IP  address must match the one assigned to the k0smotron load balancer (LB) service. Use an annotation supported by your load balancer provider to assign the control plane endpoint IP to the k0smotron  service. For example, the manifest below includes a <code>kube-vip</code> annotation.</p> <p><code>ClusterDeployment</code> objects for vSphere-based clusters include a <code>.spec.config.vsphere</code> object that contains vSphere-specific parameters. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp-0-1-0\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"172.16.0.10\"\n</code></pre> </li> </ol> <p>For more information on these parameters, see the Template reference for vsphere. </p>"},{"location":"admin-installation/","title":"Installing k0rdent","text":"<p>The process of installing k0rdent is straightforward, and involves the following steps:</p> <ol> <li>Create a Kubernetes cluster to act as the management cluster</li> <li>Install k0rdent into the management cluster</li> <li>Add the necessary credentials and templates to work with the providers in your infrastructure.</li> </ol>"},{"location":"admin-installation/#create-and-prepare-the-management-kubernetes-cluster","title":"Create and prepare the Management Kubernetes cluster","text":"<p>The first step is to create the Kubernetes cluster. You should have the following installed:</p> <ul> <li>Kubernetes</li> <li>Helm</li> <li>kubectl</li> </ul>"},{"location":"admin-installation/#cncf-certified-kubernetes","title":"CNCF-certified Kubernetes","text":"<p>k0rdent is designed to run on any CNCF-certified Kubernetes. This gives you great freedom in setting up k0rdent for convenient, secure, and reliable operations.</p> <ul> <li>Proof-of-concept scale k0rdent implementations can run on a beefy Linux desktop or laptop, using a single-node-capable CNCF Kubernetes distribution like k0s, running natively as bare processes, or inside a container with KinD.</li> <li>More capacious implementations can run on multi-node Kubernetes clusters on bare metal or in clouds.</li> <li>Production users can leverage cloud provider Kubernetes variants like Amazon EKS or Azure AKS to quickly create highly-reliable and scalable k0rdent management clusters.</li> </ul>"},{"location":"admin-installation/#mixed-use-management-clusters","title":"Mixed-use management clusters","text":"<p>k0rdent management clusters can also be mixed-use. For example, a cloud Kubernetes k0rdent management cluster can also be used as a 'mothership' environment for k0smotron hosted control planes managed by k0rdent, integrated with workers bootstrapped (also by k0rdent) on adjacent cloud VMs or on remote VMs, bare metal servers, or edge nodes.</p>"},{"location":"admin-installation/#where-k0rdent-management-clusters-can-live","title":"Where k0rdent management clusters can live","text":"<p>k0rdent management clusters can live anywhere. Unlike prior generations of Kubernetes cluster managers, there's no technical need to co-locate a k0rdent manager with child clusters on a single infrastructure. k0rdent provides a single point of control and visibility across any cloud, any infrastructure, anywhere, on-premise or off. Deciding where to put a management cluster (or multiple clusters) is best done by assessing the requirements of your use case. Considered in the abstract, a k0rdent management cluster should be:</p> <ul> <li>Resilient and available</li> <li>Accessible and secure</li> <li>Easy to network</li> <li>Scalable (particularly for mixed-use implementations)</li> <li>Easy to operate with minimum overhead</li> <li>Monitored and observable</li> <li>Equipped for backup and disaster recovery</li> </ul>"},{"location":"admin-installation/#minimum-requirements-for-single-node-k0rdent-management-clusters-for-testing","title":"Minimum requirements for single-node k0rdent management clusters for testing","text":"<p>There isn't a strict minimum system requirement for k0rdent, but the following are recommended for a single node:</p> <ul> <li>A minimum of 8 GB RAM</li> <li>4 CPU</li> <li>100GB SSD</li> </ul> <p>This configuration is only sufficient for the base case.  If you will run k0rdent Observability and FinOps (KOF) you will need significantly more resources and in particular, much more storage.</p>"},{"location":"admin-installation/#recommended-requirements-for-single-node-k0rdent-management-clusters-for-production","title":"Recommended requirements for single-node k0rdent management clusters for production","text":"<p>We do not recommend running k0rdent in production on a single node.  If you are running in production you will want a scale-out architecture.  These documents do not yet cover this case, but the general Kubernetes administration principles apply.</p> <p>Important</p> <p> Detailed instructions for deploying k0rdent management clusters into a multi-node configuration and scaling them as needed are COMING SOON!</p>"},{"location":"admin-installation/#install-k0rdent","title":"Install k0rdent","text":"<p>This section assumes that you already have a kubernetes cluster installed. If you need to setup a cluster you can follow the Create and prepare a Kubernetes cluster with k0s guide. </p> <p>Note</p> <p> While it is technically possible to run k0rdent on an existing cluster, using a fresh cluster is recommended to avoid conflicts with pre-instalked components.</p> <p>The actual management cluster is a Kubernetes cluster with the k0rdent application installed. The simplest way to install k0rdent is through its Helm chart.  You can find the latest release here, and from there you can deploy the Helm chart, as in:</p> <p><pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.1.0 -n kcm-system --create-namespace\n</code></pre> <pre><code>Pulled: ghcr.io/k0rdent/kcm/charts/kcm:0.1.0\nDigest: sha256:1f75e8e55c44d10381d7b539454c63b751f9a2ec6c663e2ab118d34c5a21087f\nNAME: kcm\nLAST DEPLOYED: Mon Dec  9 00:32:14 2024\nNAMESPACE: kcm-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre></p> <p>Note</p> <p> Make sure to specify the correct release version number.</p> <p>The helm chart deploys the KCM operator and prepares the environment, and KCM then proceeds to deploy the various subcomponents, including CAPI. The entire process takes a few minutes.</p>"},{"location":"admin-installation/#confirming-the-deployment","title":"Confirming the deployment","text":"<p>To understand whether installation is complete, start by making sure all pods are ready in the <code>kcm-system</code> namespace. There should be 17:</p> <p><pre><code>kubectl get pods -n kcm-system\n</code></pre> <pre><code>NAME                                                          READY   STATUS    RESTARTS   AGE\nazureserviceoperator-controller-manager-6b4dd86894-2m2wh      1/1     Running   0          57s\ncapa-controller-manager-64bbcb9f8-kx6wr                       1/1     Running   0          2m3s\ncapi-controller-manager-66f8998ff5-2m5m2                      1/1     Running   0          2m32s\ncapo-controller-manager-588f45c7cf-lmkgz                      1/1     Running   0          50s\ncapv-controller-manager-69f7fc65d8-wbqh8                      1/1     Running   0          46s\ncapz-controller-manager-544845f786-q8rzn                      1/1     Running   0          57s\nhelm-controller-7644c4d5c4-t6sjm                              1/1     Running   0          5m1s\nk0smotron-controller-manager-bootstrap-9fc48d76f-hppzs        2/2     Running   0          2m9s\nk0smotron-controller-manager-control-plane-7df9bc7bf-74vcs    2/2     Running   0          2m4s\nk0smotron-controller-manager-infrastructure-f7f94dd76-ppzq4   2/2     Running   0          116s\nkcm-cert-manager-895954d88-jplf6                              1/1     Running   0          5m1s\nkcm-cert-manager-cainjector-685ffdf549-qlj8m                  1/1     Running   0          5m1s\nkcm-cert-manager-webhook-59ddc6b56-8hfml                      1/1     Running   0          5m1s\nkcm-cluster-api-operator-8487958779-l9lvf                     1/1     Running   0          3m12s\nkcm-controller-manager-7998cdb69-x7kd9                        1/1     Running   0          3m12s\nkcm-velero-b68fd5957-fwg2l                                    1/1     Running   0          5m1s\nsource-controller-6cd7676f7f-7kpjn                            1/1     Running   0          5m1s\n</code></pre></p> <p>State management is handled by Project Sveltos, so you'll want to make sure that all 9 pods are running in the <code>projectsveltos</code> namespace:</p> <pre><code>kubectl get pods -n projectsveltos\n</code></pre> <pre><code>AME                                     READY   STATUS      RESTARTS   AGE\naccess-manager-56696cc7f-5txlb           1/1     Running     0          4m1s\naddon-controller-7c98776c79-dn9jm        1/1     Running     0          4m1s\nclassifier-manager-7b85f96469-666jx      1/1     Running     0          4m1s\nevent-manager-67f6db7f44-hsnnj           1/1     Running     0          4m1s\nhc-manager-6d579d675f-fgvk2              1/1     Running     0          4m1s\nregister-mgmt-cluster-job-rfkdh          0/1     Completed   0          4m1s\nsc-manager-55c99d494b-c8wrl              1/1     Running     0          4m1s\nshard-controller-5ff9cd796d-tlg79        1/1     Running     0          4m1s\nsveltos-agent-manager-7467959f4f-lsnd5   1/1     Running     0          3m34s\n</code></pre> <p>If any of these pods are missing, simply give k0rdent more time. If there's a problem, you'll see pods crashing and restarting, and you can see what's happening by describing the pod, as in:</p> <pre><code>kubectl describe pod classifier-manager-7b85f96469-666jx -n projectsveltos\n</code></pre> <p>As long as you're not seeing pod restarts, you just need to wait a few minutes.</p>"},{"location":"admin-installation/#verify-that-k0rdent-itself-is-ready","title":"Verify that k0rdent itself is ready","text":"<p>The actual measure of whether k0rdent is ready is the state of the <code>Management</code> object. To check, issue this command:</p> <p><pre><code>kubectl get Management -n kcm-system\n</code></pre> <pre><code>NAME   READY   RELEASE     AGE\nkcm    True    kcm-0-1-0   9m\n</code></pre></p>"},{"location":"admin-installation/#verify-the-templates","title":"Verify the templates","text":"<p>Next verify whether the KCM templates have been successfully installed and reconciled.  Start with the <code>ProviderTemplate</code> objects:</p> <pre><code>kubectl get providertemplate -n kcm-system\n</code></pre> <pre><code>NAME                                   VALID\ncluster-api-0-1-0                      true\ncluster-api-provider-aws-0-1-0         true\ncluster-api-provider-azure-0-1-0       true\ncluster-api-provider-openstack-0-1-0   true\ncluster-api-provider-vsphere-0-1-0     true\nk0smotron-0-1-0                        true\nkcm-0-1-0                              true\nprojectsveltos-0-45-0                  true\n</code></pre> <p>Make sure that all templates are not just installed, but valid. Again, this may take a few minutes.</p> <p>You'll also want to make sure the <code>ClusterTemplate</code> objects are installed and valid:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-1-0           true\naws-eks-0-1-0                   true\naws-hosted-cp-0-1-0             true\naws-standalone-cp-0-1-0         true\nazure-aks-0-1-0                 true\nazure-hosted-cp-0-1-0           true\nazure-standalone-cp-0-1-0       true\nopenstack-standalone-cp-0-1-0   true\nvsphere-hosted-cp-0-1-0         true\nvsphere-standalone-cp-0-1-0     true\n</code></pre> <p>Finally, make sure the <code>ServiceTemplate</code> objects are installed and valid:</p> <pre><code>kubectl get servicetemplate -n kcm-system\n</code></pre> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre>"},{"location":"admin-installation/#backing-up-a-k0rdent-management-cluster","title":"Backing up a k0rdent management cluster","text":"<p>In a production environment, you will always want to ensure that your management cluster is backed up. There are a few caveats and things you need to take into account when backing up k0rdent. More info can be found in the guide at use Velero as a backup provider.</p>"},{"location":"admin-installation/#create-and-prepare-a-kubernetes-cluster-with-k0s","title":"Create and prepare a Kubernetes cluster with k0s","text":"<p>Follow these steps to install and prepare a k0s kubernetes management cluster:</p> <ol> <li> <p>Deploy a Kubernetes cluster</p> <p>The first step is to create the actual cluster itself. Again, the actual distribution used for the management cluster isn't important, as long as it's a CNCF-compliant distribution. That means you can use an existing EKS cluster, or whatever is your normal corporate standard. To make things simple this guide uses k0s, a small, convenient, and fully-functional distribution:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --single\nsudo k0s start\n</code></pre> <p>k0s includes its own preconfigured version of <code>kubectl</code> so make sure the cluster is running:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>After 2-3 minutes you should see a single <code>control-plane</code> node with a status of <code>Ready</code>, as in:</p> <pre><code>NAME              STATUS   ROLES            AGE   VERSION\nip-172-31-29-61   Ready    control-plane    46s   v1.31.2+k0s\n</code></pre> </li> <li> <p>Install kubectl</p> <p>Everything you do in k0rdent is done by creating and manipulating Kubernetes objects, so you'll need to have <code>kubectl</code> installed. You can find the full install docs here, or just follow these instructions:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> <li> <p>Get the kubeconfig</p> <p>In order to access the management cluster you will, of course, need the kubeconfig. Again, if you're using another Kubernetes distribution follow those instructions to get the kubeconfig, but for k0s, the process involves simply copying the existing file and adding it to an environment variable so <code>kubectl</code> knows where to find it.</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>Now you should be able to use the non-k0s <code>kubectl</code> to see the status of the cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Again, you should see the single k0s node, but by this time it should have had its role assigned, as in:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre> <p>Now the cluster is ready for installation, which we'll do using Helm.</p> </li> <li> <p>Install Helm</p> <p>The easiest way to install k0rdent is through its Helm chart, so let's get Helm installed. You can find the full instructions here, or use these instructions:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Helm will be installed into <code>/usr/local/bin/helm</code></p> </li> </ol>"},{"location":"admin-kof/","title":"k0rdent Observability and FinOps (kof)","text":""},{"location":"admin-kof/#overview","title":"Overview","text":"<p>k0rdent Observability and FinOps (kof) provides enterprise-grade observability and FinOps capabilities for k0rdent-managed child Kubernetes clusters. It enables centralized metrics, logging, and cost management through a unified OpenTelemetry-based architecture.</p>"},{"location":"admin-kof/#architecture","title":"Architecture","text":""},{"location":"admin-kof/#high-level","title":"High-level","text":"<p>From a high-level perspective, KOF consists of three layers:</p> <ul> <li>the Collection layer, where the statistics and events are gathered,</li> <li>the Regional layer, which includes storage to keep track of those statistics and events,</li> <li>and the Management layer, where you interact through the UI.</li> </ul> <pre><code>           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502   Management   \u2502\n           \u2502   UI, promxy   \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502             \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n        \u2502 Regional \u2502 \u2502 Regional \u2502\n        \u2502 region 1 \u2502 \u2502 region 2 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n             \u2502             \u2502\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510     ...\n      \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Collect   \u2502 \u2502 Collect   \u2502\n\u2502 child 1   \u2502 \u2502 child 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"admin-kof/#mid-level","title":"Mid-level","text":"<p>Getting a little bit more detailed, it's important to undrestand that data flows upwards, from observed objects to centralized Grafana on the Management layer:</p> <pre><code>management cluster_____________________\n\u2502                                     \u2502\n\u2502  kof-mothership chart_____________  \u2502\n\u2502  \u2502                               \u2502  \u2502\n\u2502  \u2502 grafana-operator              \u2502  \u2502\n\u2502  \u2502 victoria-metrics-operator     \u2502  \u2502\n\u2502  \u2502 cluster-api-visualizer        \u2502  \u2502\n\u2502  \u2502 sveltos-dashboard             \u2502  \u2502\n\u2502  \u2502 k0rdent service templates     \u2502  \u2502\n\u2502  \u2502 promxy                        \u2502  \u2502\n\u2502  \u2502_______________________________\u2502  \u2502\n\u2502                                     \u2502\n\u2502  kof-operators chart_____________   \u2502\n\u2502  \u2502                              \u2502   \u2502\n\u2502  \u2502  opentelemetry-operator      \u2502   \u2502\n\u2502  \u2502  prometheus-operator-crds    \u2502   \u2502\n\u2502  \u2502______________________________\u2502   \u2502\n\u2502_____________________________________\u2502\n\n\ncloud 1...\n\u2502\n\u2502  region 1__________________________________________  region 2...\n\u2502  \u2502                                                \u2502  \u2502\n.  \u2502  regional cluster____________________          \u2502  \u2502\n.  \u2502  \u2502                                  \u2502          \u2502  \u2502\n.  \u2502  \u2502  kof-storage chart_____________  \u2502          \u2502  .\n   \u2502  \u2502  \u2502                            \u2502  \u2502          \u2502  .\n   \u2502  \u2502  \u2502 grafana-operator           \u2502  \u2502          \u2502  .\n   \u2502  \u2502  \u2502 victoria-metrics-operator  \u2502  \u2502          \u2502\n   \u2502  \u2502  \u2502 victoria-logs-single       \u2502  \u2502          \u2502\n   \u2502  \u2502  \u2502 external-dns               \u2502  \u2502          \u2502\n   \u2502  \u2502  \u2502____________________________\u2502  \u2502          \u2502\n   \u2502  \u2502                                  \u2502          \u2502\n   \u2502  \u2502  cert-manager (grafana, vmauth)  \u2502          \u2502\n   \u2502  \u2502  ingress-nginx                   \u2502          \u2502\n   \u2502  \u2502__________________________________\u2502          \u2502\n   \u2502                                                \u2502\n   \u2502                                                \u2502\n   \u2502  child deployment 1____________________  2...  \u2502\n   \u2502  \u2502                                    \u2502  \u2502     \u2502\n   \u2502  \u2502  cert-manager (OTel-operator)      \u2502  \u2502     \u2502\n   \u2502  \u2502                                    \u2502  \u2502     \u2502\n   \u2502  \u2502  kof-operators chart_____________  \u2502  .     \u2502\n   \u2502  \u2502  \u2502                              \u2502  \u2502  .     \u2502\n   \u2502  \u2502  \u2502  opentelemetry-operator____  \u2502  \u2502  .     \u2502\n   \u2502  \u2502  \u2502  \u2502                        \u2502  \u2502  \u2502        \u2502\n   \u2502  \u2502  \u2502  \u2502 OpenTelemetryCollector \u2502  \u2502  \u2502        \u2502\n   \u2502  \u2502  \u2502  \u2502________________________\u2502  \u2502  \u2502        \u2502\n   \u2502  \u2502  \u2502                              \u2502  \u2502        \u2502\n   \u2502  \u2502  \u2502  prometheus-operator-crds    \u2502  \u2502        \u2502\n   \u2502  \u2502  \u2502______________________________\u2502  \u2502        \u2502\n   \u2502  \u2502                                    \u2502        \u2502\n   \u2502  \u2502  kof-collectors chart________      \u2502        \u2502\n   \u2502  \u2502  \u2502                          \u2502      \u2502        \u2502\n   \u2502  \u2502  \u2502 opencost                 \u2502      \u2502        \u2502\n   \u2502  \u2502  \u2502 kube-state-metrics       \u2502      \u2502        \u2502\n   \u2502  \u2502  \u2502 prometheus-node-exporter \u2502      \u2502        \u2502\n   \u2502  \u2502  \u2502__________________________\u2502      \u2502        \u2502\n   \u2502  \u2502                                    \u2502        \u2502\n   \u2502  \u2502  observed objects                  \u2502        \u2502\n   \u2502  \u2502____________________________________\u2502        \u2502\n   \u2502________________________________________________\u2502\n</code></pre>"},{"location":"admin-kof/#low-level","title":"Low-level","text":"<p>At a low level, you can see how logs and traces work their way around the system.</p> <p></p>"},{"location":"admin-kof/#helm-charts","title":"Helm Charts","text":"<p>KOF is deployed as a series of Helm charts at various levels.</p>"},{"location":"admin-kof/#kof-mothership","title":"kof-mothership","text":"<ul> <li>Centralized Grafana dashboard, managed by grafana-operator</li> <li>Local VictoriaMetrics storage for alerting rules only, managed by victoria-metrics-operator</li> <li>cluster-api-visualizer for insight into multicluster configuration</li> <li>Sveltos dashboard, automatic secret distribution</li> <li>k0rdent service templates to deploy other charts to regional clusters</li> <li>Promxy for aggregating Prometheus metrics from regional clusters</li> </ul>"},{"location":"admin-kof/#kof-storage","title":"kof-storage","text":"<ul> <li>Regional Grafana dashboard, managed by grafana-operator</li> <li>Regional VictoriaMetrics storage with main data, managed by victoria-metrics-operator<ul> <li>vmauth entrypoint proxy for VictoriaMetrics components</li> <li>vmcluster for high-available fault-tolerant version of VictoriaMetrics database</li> <li>victoria-logs-single for high-performance, cost-effective, scalable logs storage</li> </ul> </li> <li>external-dns to communicate with other clusters</li> </ul>"},{"location":"admin-kof/#kof-operators","title":"kof-operators","text":"<ul> <li>prometheus-operator-crds required to create OpenTelemetry collectors, also required to monitor <code>kof-mothership</code> itself</li> <li>OpenTelemetry collectors below, managed by opentelemetry-operator</li> </ul>"},{"location":"admin-kof/#kof-collectors","title":"kof-collectors","text":"<ul> <li>prometheus-node-exporter for hardware and OS metrics</li> <li>kube-state-metrics for metrics about the state of Kubernetes objects</li> <li>OpenCost \"shines a light into the black box of Kubernetes spend\"</li> </ul>"},{"location":"admin-kof/#installation","title":"Installation","text":""},{"location":"admin-kof/#prerequisites","title":"Prerequisites","text":"<p>Before beginning KOF installation, you should have the following components in place:</p> <ul> <li>A k0rdent management cluster - You can get instructions to create one in the quickstart guide<ul> <li>To test on macOS you can install using:   <code>brew install kind &amp;&amp; kind create cluster -n k0rdent</code></li> </ul> </li> <li>You will also need your infrastructure provider credentials, such as those shown in the guide for AWS<ul> <li>Note that you should skip the \"Create your ClusterDeployment\" and later sections.</li> </ul> </li> <li>Finally, you need access to create DNS records for service endpoints such as <code>kof.example.com</code></li> </ul>"},{"location":"admin-kof/#dns-auto-config","title":"DNS auto-config","text":"<p>To avoid manual configuration of DNS records for service endpoints later, you can automate the process now using external-dns.</p> <p>For example, for AWS you should use the Node IAM Role or IRSA methods in production.</p> <p>For now, however, just for the sake of this demo based on the <code>aws-standalone</code> template, you can use the most straightforward (though less secure) static credentials method:</p> <ol> <li>Create an <code>external-dns</code> IAM user with this policy.</li> <li>Create an access key and <code>external-dns-aws-credentials</code> file, as in:     <pre><code>[default]\naws_access_key_id = &lt;EXAMPLE_ACCESS_KEY_ID&gt;\naws_secret_access_key = &lt;EXAMPLE_SECRET_ACCESS_KEY&gt;\n</code></pre></li> <li>Create the <code>external-dns-aws-credentials</code> secret in the <code>kof</code> namespace:     <pre><code>kubectl create namespace kof\nkubectl create secret generic \\\n  -n kof external-dns-aws-credentials \\\n  --from-file external-dns-aws-credentials\n</code></pre></li> </ol>"},{"location":"admin-kof/#management-cluster","title":"Management Cluster","text":"<p>To install KOF on the management cluster, look through the default values of the kof-mothership and kof-operators charts, and apply this example, or use it as a reference:</p> <ol> <li> <p>Install <code>kof-operators</code> required by <code>kof-mothership</code>:     <pre><code>helm install --wait --create-namespace -n kof kof-operators \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-operators --version 0.1.1\n</code></pre></p> </li> <li> <p>Create the <code>mothership-values.yaml</code> file:     <pre><code>kcm:\n  installTemplates: true\n</code></pre>     This enables installation of <code>ServiceTemplates</code> such as <code>cert-manager</code> and <code>kof-storage</code>,     to make it possible to reference them from the Regional and Child <code>ClusterDeployments</code>.</p> </li> <li> <p>If you want to use a default storage class,     but <code>kubectl get sc</code> shows no <code>(default)</code>, create it.     Otherwise you can use a non-default storage class in the <code>mothership-values.yaml</code> file:     <pre><code>global:\n  storageClass: &lt;EXAMPLE_STORAGE_CLASS&gt;\n</code></pre></p> </li> <li> <p>If you've applied the DNS auto-config section,     add to the <code>kcm:</code> object in the <code>mothership-values.yaml</code> file:     <pre><code>  kof:\n    clusterProfiles:\n      kof-aws-dns-secrets:\n        matchLabels:\n          k0rdent.mirantis.com/kof-aws-dns-secrets: \"true\"\n        secrets:\n          - external-dns-aws-credentials\n</code></pre>     This enables Sveltos to auto-distribute DNS secret to regional clusters.</p> </li> <li> <p>Two secrets are auto-created by default:</p> <ul> <li><code>storage-vmuser-credentials</code> is a secret used by VictoriaMetrics.     You don't need to use it directly.     It is auto-distributed to other clusters by the Sveltos <code>ClusterProfile</code> here.</li> <li><code>grafana-admin-credentials</code> is a secret that we will use in the Grafana section.     It is auto-created here.</li> </ul> </li> <li> <p>Install <code>kof-mothership</code>:     <pre><code>helm install --wait -f mothership-values.yaml -n kof kof-mothership \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-mothership --version 0.1.1\n</code></pre></p> </li> <li> <p>Wait for all pods to show that they're <code>Running</code>:     <pre><code>kubectl get pod -n kof\n</code></pre></p> </li> </ol>"},{"location":"admin-kof/#regional-cluster","title":"Regional Cluster","text":"<p>To install KOF on the regional cluster, look through the default values of the kof-storage chart, and apply this example for AWS, or use it as a reference:</p> <ol> <li> <p>Set your KOF variables using your own values:     <pre><code>REGIONAL_CLUSTER_NAME=cloud1-region1\nREGIONAL_DOMAIN=$REGIONAL_CLUSTER_NAME.kof.example.com\nADMIN_EMAIL=$(git config user.email)\necho \"$REGIONAL_CLUSTER_NAME, $REGIONAL_DOMAIN, $ADMIN_EMAIL\"\n</code></pre></p> </li> <li> <p>Use the up-to-date <code>ClusterTemplate</code>, as in:     <pre><code>kubectl get clustertemplate -n kcm-system | grep aws\nTEMPLATE=aws-standalone-cp-0-1-0\n</code></pre></p> </li> <li> <p>Compose the following objects:</p> <ul> <li><code>ClusterDeployment</code> - regional cluster</li> <li><code>PromxyServerGroup</code> - for metrics</li> <li><code>GrafanaDatasource</code> - for logs</li> </ul> <pre><code>cat &gt;regional-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: $REGIONAL_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    kof: storage\nspec:\n  template: $TEMPLATE\n  credential: aws-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    controlPlane:\n      instanceType: t3.large\n    controlPlaneNumber: 1\n    publicIP: true\n    region: us-east-2\n    worker:\n      instanceType: t3.medium\n    workersNumber: 3\n    clusterLabels:\n      k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n      k0rdent.mirantis.com/kof-aws-dns-secrets: \"true\"\n  serviceSpec:\n    priority: 100\n    services:\n      - name: ingress-nginx\n        namespace: ingress-nginx\n        template: ingress-nginx-4-11-3\n      - name: cert-manager\n        namespace: cert-manager\n        template: cert-manager-1-16-2\n        values: |\n          cert-manager:\n            crds:\n              enabled: true\n      - name: kof-storage\n        namespace: kof\n        template: kof-storage-0-1-1\n        values: |\n          external-dns:\n            enabled: true\n          victoriametrics:\n            vmauth:\n              ingress:\n                host: vmauth.$REGIONAL_DOMAIN\n            security:\n              username_key: username\n              password_key: password\n              credentials_secret_name: storage-vmuser-credentials\n          grafana:\n            ingress:\n              host: grafana.$REGIONAL_DOMAIN\n            security:\n              credentials_secret_name: grafana-admin-credentials\n          cert-manager:\n            email: $ADMIN_EMAIL\n---\napiVersion: kof.k0rdent.mirantis.com/v1alpha1\nkind: PromxyServerGroup\nmetadata:\n  labels:\n    app.kubernetes.io/name: promxy-operator\n    k0rdent.mirantis.com/promxy-secret-name: kof-mothership-promxy-config\n  name: promxyservergroup-sample\n  namespace: kof\nspec:\n  cluster_name: $REGIONAL_CLUSTER_NAME\n  targets:\n    - \"vmauth.$REGIONAL_DOMAIN:443\"\n  path_prefix: /vm/select/0/prometheus/\n  scheme: https\n  http_client:\n    dial_timeout: \"5s\"\n    tls_config:\n      insecure_skip_verify: true\n    basic_auth:\n      credentials_secret_name: storage-vmuser-credentials\n      username_key: username\n      password_key: password\n---\napiVersion: grafana.integreatly.org/v1beta1\nkind: GrafanaDatasource\nmetadata:\n  labels:\n    app.kubernetes.io/managed-by: Helm\n  name: victoria-logs-regional0\n  namespace: kof\nspec:\n  valuesFrom:\n    - targetPath: \"basicAuthUser\"\n      valueFrom:\n        secretKeyRef:\n          key: username\n          name: storage-vmuser-credentials\n    - targetPath: \"secureJsonData.basicAuthPassword\"\n      valueFrom:\n        secretKeyRef:\n          key: password\n          name: storage-vmuser-credentials\n  datasource:\n    name: $REGIONAL_CLUSTER_NAME\n    url: https://vmauth.$REGIONAL_DOMAIN/vls\n    access: proxy\n    isDefault: false\n    type: \"victoriametrics-logs-datasource\"\n    basicAuth: true\n    basicAuthUser: \\${username}\n    secureJsonData:\n      basicAuthPassword: \\${password}\n  instanceSelector:\n    matchLabels:\n      dashboards: grafana\n  resyncPeriod: 5m\nEOF\n</code></pre> </li> <li> <p>The <code>ClusterTemplate</code> above provides the default storage class <code>ebs-csi-default-sc</code>. If you want to use a non-default storage class,     add it to the <code>regional-cluster.yaml</code> file     in the <code>ClusterDeployment.spec.serviceSpec.services[name=kof-storage].values</code>:     <pre><code>global:\n  storageClass: &lt;EXAMPLE_STORAGE_CLASS&gt;\nvictoria-logs-single:\n  server:\n    storage:\n      storageClassName: &lt;EXAMPLE_STORAGE_CLASS&gt;\n</code></pre></p> </li> <li> <p>Verify and apply the Regional <code>ClusterDeployment</code>:     <pre><code>cat regional-cluster.yaml\n\nkubectl apply -f regional-cluster.yaml\n</code></pre></p> </li> <li> <p>Watch how the cluster is deployed to AWS until all values of <code>READY</code> are <code>True</code>:     <pre><code>clusterctl describe cluster -n kcm-system $REGIONAL_CLUSTER_NAME \\\n  --show-conditions all\n</code></pre></p> </li> </ol>"},{"location":"admin-kof/#child-cluster","title":"Child Cluster","text":"<p>To install KOF on the actual cluster to be monitored, look through the default values of the kof-operators and kof-collectors charts, and apply this example for AWS, or use it as a reference:</p> <ol> <li> <p>Set your own value below, verifing the variables:     <pre><code>CHILD_CLUSTER_NAME=$REGIONAL_CLUSTER_NAME-child1\necho \"$CHILD_CLUSTER_NAME, $REGIONAL_DOMAIN\"\n</code></pre></p> </li> <li> <p>Use the up-to-date <code>ClusterTemplate</code>, as in:     <pre><code>kubectl get clustertemplate -n kcm-system | grep aws\nTEMPLATE=aws-standalone-cp-0-1-0\n</code></pre></p> </li> <li> <p>Compose the <code>ClusterDeployment</code>:</p> <pre><code>cat &gt;child-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: $CHILD_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    kof: collector\nspec:\n  template: $TEMPLATE\n  credential: aws-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    controlPlane:\n      instanceType: t3.large\n    controlPlaneNumber: 1\n    publicIP: false\n    region: us-east-2\n    worker:\n      instanceType: t3.small\n    workersNumber: 3\n    clusterLabels:\n      k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n  serviceSpec:\n    priority: 100\n    services:\n      - name: cert-manager\n        namespace: kof\n        template: cert-manager-1-16-2\n        values: |\n          cert-manager:\n            crds:\n              enabled: true\n      - name: kof-operators\n        namespace: kof\n        template: kof-operators-0-1-1\n      - name: kof-collectors\n        namespace: kof\n        template: kof-collectors-0-1-1\n        values: |\n          global:\n            clusterName: $CHILD_CLUSTER_NAME\n          opencost:\n            enabled: true\n            opencost:\n              prometheus:\n                username_key: username\n                password_key: password\n                existingSecretName: storage-vmuser-credentials\n                external:\n                  url: https://vmauth.$REGIONAL_DOMAIN/vm/select/0/prometheus\n              exporter:\n                defaultClusterId: $CHILD_CLUSTER_NAME\n          kof:\n            logs:\n              username_key: username\n              password_key: password\n              credentials_secret_name: storage-vmuser-credentials\n              endpoint: https://vmauth.$REGIONAL_DOMAIN/vls/insert/opentelemetry/v1/logs\n            metrics:\n              username_key: username\n              password_key: password\n              credentials_secret_name: storage-vmuser-credentials\n              endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/insert/0/prometheus/api/v1/write\nEOF\n</code></pre> </li> <li> <p>Verify and apply the <code>ClusterDeployment</code>:     <pre><code>cat child-cluster.yaml\n\nkubectl apply -f child-cluster.yaml\n</code></pre></p> </li> <li> <p>Watch while the cluster is deployed to AWS until all values of <code>READY</code> are <code>True</code>:     <pre><code>clusterctl describe cluster -n kcm-system $CHILD_CLUSTER_NAME \\\n  --show-conditions all\n</code></pre></p> </li> </ol>"},{"location":"admin-kof/#verification","title":"Verification","text":"<p>Finally, verify that KOF installed properly.</p> <p><pre><code>kubectl get clustersummaries -A -o wide\n</code></pre> Wait until the value of <code>HELMCHARTS</code> changes from <code>Provisioning</code> to <code>Provisioned</code>.</p> <p><pre><code>kubectl get secret -n kcm-system $REGIONAL_CLUSTER_NAME-kubeconfig \\\n  -o=jsonpath={.data.value} | base64 -d &gt; regional-kubeconfig\n\nkubectl get secret -n kcm-system $CHILD_CLUSTER_NAME-kubeconfig \\\n  -o=jsonpath={.data.value} | base64 -d &gt; child-kubeconfig\n\nKUBECONFIG=regional-kubeconfig kubectl get pod -A\n  # Namespaces: cert-manager, ingress-nginx, kof, kube-system, projectsveltos\n\nKUBECONFIG=child-kubeconfig kubectl get pod -A\n  # Namespaces: kof, kube-system, projectsveltos\n</code></pre> Wait for all pods to show as <code>Running</code>.</p>"},{"location":"admin-kof/#manual-dns-config","title":"Manual DNS config","text":"<p>If you've opted out of DNS auto-config, you will need to do the following:</p> <ol> <li> <p>Get the <code>EXTERNAL-IP</code> of <code>ingress-nginx</code>:     <pre><code>KUBECONFIG=regional-kubeconfig kubectl get svc \\\n  -n ingress-nginx ingress-nginx-controller\n</code></pre>     It should look like <code>REDACTED.us-east-2.elb.amazonaws.com</code></p> </li> <li> <p>Create these DNS records of type <code>A</code>, both pointing to that <code>EXTERNAL-IP</code>:     <pre><code>echo vmauth.$REGIONAL_DOMAIN\necho grafana.$REGIONAL_DOMAIN\n</code></pre></p> </li> </ol>"},{"location":"admin-kof/#sveltos","title":"Sveltos","text":"<p>Use the Sveltos dashboard to verify secrets have been auto-distributed to the required clusters:</p> <ol> <li> <p>Start by preparing the system:</p> <pre><code>kubectl create sa platform-admin\nkubectl create clusterrolebinding platform-admin-access \\\n  --clusterrole cluster-admin --serviceaccount default:platform-admin\n\nkubectl create token platform-admin --duration=24h\nkubectl port-forward -n kof svc/dashboard 8081:80\n</code></pre> </li> <li> <p>Now open http://127.0.0.1:8081/login and paste the token output in step 1 above.</p> </li> <li>Open the <code>ClusterAPI</code> tab: http://127.0.0.1:8081/sveltos/clusters/ClusterAPI/1</li> <li>Check both regional and child clusters:<ul> <li>Cluster profiles should be <code>Provisioned</code>.</li> <li>Secrets should be distributed.</li> </ul> </li> </ol> <p></p>"},{"location":"admin-kof/#grafana","title":"Grafana","text":""},{"location":"admin-kof/#access-to-grafana","title":"Access to Grafana","text":"<p>To make Grafana available, follow these steps:</p> <ol> <li> <p>Get the Grafana username and password:     <pre><code>kubectl get secret -n kof grafana-admin-credentials -o yaml | yq '{\n  \"user\": .data.GF_SECURITY_ADMIN_USER | @base64d,\n  \"pass\": .data.GF_SECURITY_ADMIN_PASSWORD | @base64d\n}'\n</code></pre></p> </li> <li> <p>Start the Grafana dashboard:     <pre><code>kubectl port-forward -n kof svc/grafana-vm-service 3000:3000\n</code></pre></p> </li> <li> <p>Login to http://127.0.0.1:3000/dashboards with the username/password printed above.</p> </li> <li>Open a dashboard:</li> </ol> <p></p>"},{"location":"admin-kof/#cluster-overview","title":"Cluster Overview","text":"<p>From here you can get an overview of the cluster, including:</p> <ul> <li>Health metrics</li> <li>Resource utilization</li> <li>Performance trends</li> <li>Cost analysis</li> </ul>"},{"location":"admin-kof/#logging-interface","title":"Logging Interface","text":"<p>The logging interface will also be available, including:</p> <ul> <li>Real-time log streaming</li> <li>Full-text search</li> <li>Log aggregation</li> <li>Alert correlation</li> </ul>"},{"location":"admin-kof/#cost-management","title":"Cost Management","text":"<p>Finally there are the cost management features, including:</p> <ul> <li>Resource cost tracking</li> <li>Usage analysis</li> <li>Budget monitoring</li> <li>Optimization recommendations</li> </ul>"},{"location":"admin-kof/#scaling-guidelines","title":"Scaling Guidelines","text":"<p>The method for scaling KOF depends on the type of expansion:</p>"},{"location":"admin-kof/#regional-expansion","title":"Regional Expansion","text":"<ol> <li>Deploy a regional cluster in the new region</li> <li>Configure child clusters in this region to point to this regional cluster</li> </ol>"},{"location":"admin-kof/#adding-a-new-child-cluster","title":"Adding a New Child Cluster","text":"<ol> <li>Apply templates, as in the child cluster section</li> <li>Verify the data flow</li> <li>Configure any custom dashboards</li> </ol>"},{"location":"admin-kof/#maintenance","title":"Maintenance","text":""},{"location":"admin-kof/#backup-requirements","title":"Backup Requirements","text":"<p>Backing up KOF requires backing up the following:</p> <ul> <li>Grafana configurations</li> <li>Alert definitions</li> <li>Custom dashboards</li> <li>Retention policies</li> </ul>"},{"location":"admin-kof/#health-monitoring","title":"Health Monitoring","text":"<p>To implement health monitoring:</p> <ol> <li>Apply the steps in the Verification section</li> <li>Apply the steps in the Sveltos section</li> </ol>"},{"location":"admin-kof/#uninstallation","title":"Uninstallation","text":"<p>To remove the demo clusters created in this section:</p> <p>Warning</p> <p> Make sure these are just your demo clusters and do not contain important data.</p> <pre><code>kubectl delete --wait --cascade=foreground -f child-cluster.yaml\nkubectl delete --wait --cascade=foreground -f regional-cluster.yaml\n</code></pre> <p>To remove KOF from the management cluster:</p> <pre><code>helm uninstall --wait --cascade foreground -n kof kof-mothership\nhelm uninstall --wait --cascade foreground -n kof kof-operators\nkubectl delete namespace kof --wait --cascade=foreground\n</code></pre>"},{"location":"admin-kof/#resource-limits","title":"Resource Limits","text":"<p>See also: System Requirements.</p>"},{"location":"admin-kof/#resources-of-management-cluster","title":"Resources of Management Cluster","text":"<ul> <li> <p>promxy:   <pre><code>resources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    cpu: 100m\n    memory: 128Mi\n</code></pre></p> </li> <li> <p>promxy-deployment:   <pre><code>resources:\n  requests:\n    cpu: 0.02\n    memory: 20Mi\n  limits:\n    cpu: 0.02\n    memory: 20Mi\n</code></pre></p> </li> <li> <p>promxy-operator:   <pre><code>resources:\n  limits:\n    cpu: 500m\n    memory: 128Mi\n  requests:\n    cpu: 10m\n    memory: 64Mi\n</code></pre></p> </li> </ul>"},{"location":"admin-kof/#resources-of-a-child-cluster","title":"Resources of a Child Cluster","text":"<ul> <li>opentelemetry:   <pre><code>resourceRequirements:\n  limits:\n    memory: 128Mi\n  requests:\n    memory: 128Mi\n</code></pre></li> </ul>"},{"location":"admin-kof/#version-compatibility","title":"Version Compatibility","text":"Component Version Notes k0rdent \u2265 0.0.7 Required for template support Kubernetes \u2265 1.32 Earlier versions untested OpenTelemetry \u2265 0.75 Recommended minimum VictoriaMetrics \u2265 0.40 Required for clustering <p>Detailed:</p> <ul> <li>kof-mothership</li> <li>kof-storage</li> <li>kof-operators</li> <li>kof-collectors</li> </ul>"},{"location":"admin-kof/#more","title":"More","text":"<ul> <li>If you've applied this guide you should have kof up and running.</li> <li>Check k0rdent/kof/docs for advanced guides such as configuring alerts.</li> </ul>"},{"location":"admin-prepare/","title":"Prepare k0rdent to create child clusters on multiple providers","text":"<p>Managed clusters can be hosted on a number of different platforms. At the time of this writing, those platforms include:</p> <ul> <li>Amazon Web Services (EC2 and EKS)</li> <li>Microsoft Azure (self-managed and AKS)</li> <li>OpenStack</li> <li>VMware</li> </ul>"},{"location":"admin-prepare/#aws","title":"AWS","text":"<p>k0rdent can deploy managed clusters as both EC2-based Kubernetes clusters and EKS clusters. In both cases, you'll need to create the relevant credentials, and to do that you'll need to configure an IAM user. Follow these steps to make it possible to deploy to AWS:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>Install <code>clusterawsadm</code></p> <p>k0rdent uses the Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. <code>clusterawsadm</code>, a CLI tool created by CAPA project, helps with AWS-specific tasks such as creating IAM roles and policies, as well as credential configuration. To install clusterawsadm on Ubuntu on x86 hardware, execute these commands:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre> </li> <li> <p>Configure AWS IAM</p> <p>Next you'll need to create the IAM policies and service account k0rdent will use to take action within the AWS infrastructure. (Note that you only need to do this once.)</p> <p>The first step is to create the IAM CloudFormation stack based on your admin user. Start by specifying the environment variables <code>clusterawsadm</code> will use as AWS credentials:</p> <pre><code>export AWS_REGION=&lt;EXAMPLE_AWS_REGION&gt;\nexport AWS_ACCESS_KEY_ID=&lt;EXAMPLE_ACCESS_KEY_ID&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;EXAMPLE_SECRET_ACCESS_KEY&gt;\nexport AWS_SESSION_TOKEN=&lt;YOUR_SESSION_TOKEN&gt; # Optional. If you are using Multi-Factor Auth.\n</code></pre> </li> <li> <p>Create the IAM CloudFormation stack</p> <p>Now use <code>clusterawsadm</code> to create the IAM CloudFormation stack:</p> <p><code>shell   clusterawsadm bootstrap iam create-cloudformation-stack</code></p> </li> <li> <p>Install the AWS CLI</p> <p>With the stack in place you can create the AWS IAM user. You can do this in the UI, but it's also possible to do it from the command line using the <code>aws</code> CLI tool.  Start by installing it, if you haven't already:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre> <p>The tool recognizes the environment variables you created earlier, so there's no need to login.</p> </li> <li> <p>Check for available IPs</p> <p>Because k0rdent has 3 availablilty zone NAT gateways, each cluster needs 3 public IPs. Unfortunately, the default <code>EC2-VPC Elastic IPs</code> quota per region is 5, so while you likely won't have issues with a first cluster, if you try to deplay a  second to the same region, you are likely to run into issues.  </p> <p>You can determine how many elastic IPs are available from the command line:</p> <p><pre><code>LIMIT=$(aws ec2 describe-account-attributes --attribute-names vpc-max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)\nUSED=$(aws ec2 describe-addresses --query 'Addresses[*].PublicIp' --output text | wc -w)\nAVAILABLE=$((LIMIT - USED))\necho \"Available Public IPs: $AVAILABLE\"\n</code></pre> <pre><code>Available Public IPs: 5\n</code></pre></p> <p>If you have less than 3 available public IPs, you can request an increase in your quota:</p> <pre><code>aws service-quotas request-service-quota-increase \\\n    --service-code ec2 \\\n    --quota-code L-0263D0A3 \\\n    --desired-value 20\n</code></pre> <p>You can check on the status of your request:</p> <p><pre><code>aws service-quotas list-requested-service-quota-change-history \\\n    --service-code ec2\n</code></pre> <pre><code>{\n    \"RequestedQuotas\": [\n        {\n            \"Id\": \"EXAMPLE_ACCESS_KEY_ID\",\n            \"ServiceCode\": \"ec2\",\n            \"ServiceName\": \"Amazon Elastic Compute Cloud (Amazon EC2)\",\n            \"QuotaCode\": \"L-0263D0A3\",\n            \"QuotaName\": \"EC2-VPC Elastic IPs\",\n            \"DesiredValue\": 20.0,\n            \"Status\": \"PENDING\",\n            \"Created\": \"2025-02-09T02:27:01.573000-05:00\",\n            \"LastUpdated\": \"2025-02-09T02:27:01.956000-05:00\",\n            \"Requester\": \"{\\\"accountId\\\":\\\"EXAMPLE_ACCESS_KEY_ID\\\",\\\"callerArn\\\":\\\"arn:aws:iam::EXAMPLE_ACCESS_KEY_ID:user/nchase\\\"}\",\n            \"QuotaArn\": \"arn:aws:servicequotas:EXAMPLE_AWS_REGION:EXAMPLE_ACCESS_KEY_ID:ec2/L-0263D0A3\",\n            \"GlobalQuota\": false,\n            \"Unit\": \"None\",\n            \"QuotaRequestedAtLevel\": \"ACCOUNT\"\n        }\n    ]\n}\n</code></pre></p> </li> <li> <p>Create the IAM user. </p> <p>The actual <code>user-name</code> parameter is arbitrary; you can specify it as anything you like:</p> <p><pre><code>aws iam create-user --user-name k0rdentUser\n</code></pre> <pre><code>{\n  \"User\": {\n    \"Path\": \"/\",\n    \"UserName\": \"k0rdentUser\",\n    \"UserId\": \"EXAMPLE_USER_ID\",\n    \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentUser\",\n    \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Assign the relevant policies</p> <p>You'll need to assign the following policies to the user you just created:</p> <p><pre><code>control-plane.cluster-api-provider-aws.sigs.k8s.io\ncontrollers.cluster-api-provider-aws.sigs.k8s.io\nnodes.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> To do that, you'll need the ARNs for each policy.  You can get them with the <code>list-policies</code> command, as in:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> <pre><code>{\n  \"Policies\": [\n    {\n      \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 2,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n    },\n    {\n      \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 3,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n    },\n    {\n      \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 3,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n    },\n    {\n      \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 2,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n    }\n  ]\n}\n</code></pre></p> <p>Now you can add the policies using the <code>attach-user-policy</code> command and the ARNs you retrieved in the previous step:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> </li> <li> <p>Create an access key and secret</p> <p>To access AWS as this new user, you'll need to create an access key:</p> <p><pre><code>aws iam create-access-key --user-name k0rdentUser \n</code></pre> <pre><code>{\n  \"AccessKey\": {\n    \"UserName\": \"k0rdentUser\",\n    \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n    \"Status\": \"Active\",\n    \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n    \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Create the IAM Credentials <code>Secret</code> on the k0rdent Management Cluster</p> <p>Create a YAML file called <code>aws-cluster-identity-secret.yaml</code> and add the following text, including the <code>AccessKeyId</code> and <code>SecretAccessKey</code> you created in the previous step:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nstringData:\n  AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n  SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Apply the YAML to your cluster, making sure to add it to the namespace where the CAPA provider is running (currently <code>kcm-system</code>) so the controller can read it:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>AWSClusterStaticIdentity</code></p> <p>Create the <code>AWSClusterStaticIdentity</code> object in a file named <code>aws-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Notice that the <code>secretRef</code> references the <code>Secret</code> you created in the previous step.</p> <p>Apply the YAML to your cluster, again adding it to the <code>kcm-system</code> namespace.</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre> </li> <li> <p>Create the Cluster Identity resource template <code>ConfigMap</code></p> <p>Now we create Cluster Identity resource template <code>ConfigMap</code>. As in prior steps, create a YAML file called <code>aws-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-cluster-identity-resource-template\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\n</code></pre> <p>Note that <code>ConfigMap</code> is empty, this is expected, we don't need to template any object inside child cluster(s), but we can use that object in the future if need arises.</p> <p>Apply the YAML to your cluster, again keeping it in the <code>kcm-system</code> namespace:</p> <pre><code>kubectl apply -f aws-cluster-identity-resource-template.yaml -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>Credential</code></p> <p>Finally, create the KCM <code>Credential</code> object, making sure to reference the <code>AWSClusterStaticIdentity</code> you just created:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> Apply the YAML to your cluster, again keeping it in the <code>kcm-system</code> namespace:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre> </li> <li> <p>Deploy a cluster</p> <p>Make sure everything is configured properly by creating a <code>ClusterDeployment</code>. Start with a YAML file specifying the <code>ClusterDeployment</code>, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n    name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-1-0\n  credential: aws-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> </p> <p>Note</p> <ul> <li>You're giving it an arbitrary name in <code>.metadata.name</code> (<code>my-aws-clusterdeployment1</code>)</li> <li>You're referencing the credential you created in the previous step, <code>aws-cluster-identity-cred</code>. This enables you to set up a system where users can take advantage of having access to the credentials to the AWS account without actually having those credentials in hand.</li> <li>You need to choose a template to use for the cluster, in this case <code>aws-standalone-cp-0-1-0</code>. You can get a list of available templates using:</li> </ul> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-1-0           true\naws-eks-0-1-0                   true\naws-hosted-cp-0-1-0             true\naws-standalone-cp-0-1-0         true\nazure-aks-0-1-0                 true\nazure-hosted-cp-0-1-0           true\nazure-standalone-cp-0-1-0       true\nopenstack-standalone-cp-0-1-0   true\nvsphere-hosted-cp-0-1-0         true\nvsphere-standalone-cp-0-1-0     true\n</code></pre> Apply the YAML to your management cluster: <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> As before, there will be a delay as the cluster finishes provisioning. Follow the provisioning process with: <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> When the cluster is <code>Ready</code>, you can access it via the kubeconfig, as in: <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre></p> </li> <li> <p>Cleanup</p> <p>When you've established that it's working properly, you can delete the managed cluster and its AWS objects:</p> <pre><code>kubectl delete clusterdeployments my-aws-clusterdeployment1 \n</code></pre> </li> </ol>"},{"location":"admin-prepare/#azure","title":"Azure","text":"<p>Standalone clusters can be deployed on Azure instances. Follow these steps to make Azure clusters available to your users:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>The Azure CLI</p> <p>The Azure CLI (<code>az</code>) is required to interact with Azure resources. You can install it on Ubuntu as follows:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre> </li> <li> <p>Log in to Azure</p> <p>Run the <code>az login</code> command to authenticate your session with Azure:</p> <pre><code>az login\n</code></pre> <p>Make sure that the account you're using has at least one active subscription.</p> </li> <li> <p>Register resource providers</p> <p>In order for k0rdent to deploy and manage clusters, it needs to be able to work with Azure resources such as  compute, network, and identity. Make sure the subscription you're using has the following resource providers registered:</p> <pre><code>Microsoft.Compute\nMicrosoft.Network\nMicrosoft.ContainerService\nMicrosoft.ManagedIdentity\nMicrosoft.Authorization\n</code></pre> <p>To register these providers, run the following commands in the Azure CLI:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre> </li> <li> <p>Find Your Subscription ID</p> <p>Creating a child cluster requires a structure of credentials that link to user identities on the provider system without exposing the actual username and password to users. You can find more information on k0rdent  Credentials, but for Azure, this involves creating an <code>AzureClusterIdentity</code> and a  Service Principal (SP) to let CAPZ (Cluster API Azure) communicate with the cloud provider. </p> <p>On Azure, the lowest level of this hierarchy is the subscription, which ties to your billing information for Azure. Your Azure user must have at least one subscription for you to use it with k0rdent, so if you're working with a new account make sure to create a new subscription with billing information before you start.</p> <p>To get the information you need, list all your Azure subscriptions: </p> <p><pre><code>az account list -o table\n</code></pre> <pre><code>Name                     SubscriptionId                        TenantId\n-----------------------  -------------------------------------  --------------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID        TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre></p> <p>Make note of the <code>SubscriptionId</code> for the subscription you want to use.</p> </li> <li> <p>Create a Service Principal (SP)</p> <p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. Create the Service Principal, making sure to replace  with the <code>SubscriptionId</code> from step 1. <p><pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt;\"\n</code></pre> <pre><code>{\n\"appId\": \"SP_APP_ID_SP_APP_ID\",\n\"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n\"password\": \"SP_PASSWORD_SP_PASSWORD\",\n\"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> Note that this information provides access to your Azure account, so make sure to treat these strings  like passwords. Do not share them or check them into a repository.</p> <li> <p>Use the password to create a <code>Secret</code> object</p> <p>The <code>Secret</code> stores the <code>clientSecret</code> (password) from the Service Principal. Save the <code>Secret</code> YAML in a file called <code>azure-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clientSecret: &lt;SP_PASSWORD_SP_PASSWORD&gt; # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>You can then apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>AzureClusterIdentity</code> objects</p> <p>The <code>AzureClusterIdentity</code> object defines the credentials CAPZ uses to manage Azure resources.  It references the <code>Secret</code> you just created, so make sure that <code>.spec.clientSecret.name</code> matches  the name of that <code>Secret</code>.</p> <p>Save the following YAML into a file named <code>azure-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n    k0rdent.mirantis.com/component: \"kcm\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: &lt;SP_APP_ID_SP_APP_ID&gt; # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: &lt;SP_TENANT_SP_TENANT&gt; # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre></p> </li> <li> <p>Create the k0rdent <code>Credential</code> Object</p> <p>Create the YAML for the specification of the <code>Credential</code> and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>You're referencing the <code>AzureClusterIdentity</code> object you just created, so make sure that <code>.spec.name</code> matches  <code>.metadata.name</code> of that object. Also, note that while the overall object's <code>kind</code> is <code>Credential</code>, the  <code>.spec.identityRef.kind</code> must be <code>AzureClusterIdentity</code> to match that object.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre></p> </li> <li> <p>Create the <code>ConfigMap</code> resource-template Object</p> <p>Create a YAML with the specification of our resource-template and save it as <code>azure-cluster-identity-resource-template.yaml</code></p> <p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: azure-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    {{- $subnetName := \"\" -}}\n    {{- $securityGroupName := \"\" -}}\n    {{- $routeTableName := \"\" -}}\n    {{- range $cluster.spec.networkSpec.subnets -}}\n      {{- if eq .role \"node\" -}}\n        {{- $subnetName = .name -}}\n        {{- $securityGroupName = .securityGroup.name -}}\n        {{- $routeTableName = .routeTable.name -}}\n        {{- break -}}\n      {{- end -}}\n    {{- end -}}\n    {{- $cloudConfig := dict\n      \"aadClientId\" $identity.spec.clientID\n      \"aadClientSecret\" (index $secret.data \"clientSecret\" | b64dec)\n      \"cloud\" $cluster.spec.azureEnvironment\n      \"loadBalancerName\" \"\"\n      \"loadBalancerSku\" \"Standard\"\n      \"location\" $cluster.spec.location\n      \"maximumLoadBalancerRuleCount\" 250\n      \"resourceGroup\" $cluster.spec.resourceGroup\n      \"routeTableName\" $routeTableName\n      \"securityGroupName\" $securityGroupName\n      \"securityGroupResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n      \"subnetName\" $subnetName\n      \"subscriptionId\" $cluster.spec.subscriptionID\n      \"tenantId\" $identity.spec.tenantID\n      \"useInstanceMetadata\" true\n      \"useManagedIdentityExtension\" false\n      \"vmType\" \"vmss\"\n      \"vnetName\" $cluster.spec.networkSpec.vnet.name\n      \"vnetResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n    -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: azure-cloud-provider\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-config: {{ $cloudConfig | toJson | b64enc }}\n</code></pre> Object name needs to be exactly <code>azure-cluster-identity-resource-template.yaml</code>, <code>AzureClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-resource-template.yaml\n</code></pre> <pre><code>configmap/azure-cluster-identity-resource-template created\n</code></pre></p> </li> <p>Now you're ready to deploy the cluster.</p> <ol> <li> <p>Create a <code>ClusterDeployment</code></p> <p>To test the configuration, deploy a child cluster by following these steps:</p> <p>First get a list of available locations/regions:</p> <p><pre><code>az account list-locations -o table\n</code></pre> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n. . .\n</code></pre></p> <p>Make note of the location you want to use, such as <code>eastus</code>.</p> <p>To create the actual child cluster, create a <code>ClusterDeployment</code> that references the appropriate template as well as the location, credentials, and <code>subscriptionId</code>.</p> <p>You can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-1-0           true\naws-eks-0-1-0                   true\naws-hosted-cp-0-1-0             true\naws-standalone-cp-0-1-0         true\nazure-aks-0-1-0                 true\nazure-hosted-cp-0-1-0           true\nazure-standalone-cp-0-1-0       true\nopenstack-standalone-cp-0-1-0   true\nvsphere-hosted-cp-0-1-0         true\nvsphere-standalone-cp-0-1-0     true\n</code></pre></p> <p>Create the yaml:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-1-0\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    subscriptionID: &lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt; # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre></p> <p>Note that although the <code>ClusterDeployment</code> object has been created, there will be a delay as actual Azure instances are provisioned and added to the cluster. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre> <p>If the provisioning process continues for a more than a few minutes, check to make sure k0rdent isn't trying to exceed your quotas. If you are near the top of your quotas, requesting an increase can \"unstick\" the provisioning process.</p> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up Azure resources, delete the child cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete clusterdeployments my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin-prepare/#openstack","title":"OpenStack","text":"<p>k0rdent can deploy child clusters on OpenStack virtual machines. Follow these steps to configure and deploy OpenStack clusters for your users:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>OpenStack CLI (optional)</p> <p>If you plan to access OpenStack directly, go ahead and  install the OpenStack CLI.</p> </li> <li> <p>Configure the OpenStack Application Credential</p> <p>The exported list of variables should include:</p> <pre><code>OS_AUTH_URL\nOS_APPLICATION_CREDENTIAL_ID\nOS_APPLICATION_CREDENTIAL_SECRET\nOS_REGION_NAME\nOS_INTERFACE\nOS_IDENTITY_API_VERSION\nOS_AUTH_TYPE\n</code></pre> <p>While it's possible to use a username and password instead of the Application Credential \u2014 adjust your YAML accordingly \u2014 an Application Credential is strongly recommended because it limits scope and improves security over a raw username/password approach.</p> </li> <li> <p>Create the OpenStack Credentials Secret</p> <p>Create a Kubernetes <code>Secret</code> containing the <code>clouds.yaml</code> that defines your OpenStack environment, substituting real values where appropriate. Save this as <code>openstack-cloud-config.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openstack-cloud-config\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clouds.yaml: |\n    clouds:\n      openstack:\n        auth:\n          auth_url: &lt;OS_AUTH_URL&gt;\n          application_credential_id: &lt;OS_APPLICATION_CREDENTIAL_ID&gt;\n          application_credential_secret: &lt;OS_APPLICATION_CREDENTIAL_SECRET&gt;\n        region_name: &lt;OS_REGION_NAME&gt;\n        interface: &lt;OS_INTERFACE&gt;\n        identity_api_version: &lt;OS_IDENTITY_API_VERSION&gt;\n        auth_type: &lt;OS_AUTH_TYPE&gt;\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cloud-config.yaml\n</code></pre> </li> <li> <p>Create the k0rdent Credential Object</p> <p>Next, define a <code>Credential</code> that references the <code>Secret</code> from the previous step. Save this as <code>openstack-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: openstack-cluster-identity-cred\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"  \nspec:\n  description: \"OpenStack credentials\"\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: openstack-cloud-config\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-cred.yaml\n</code></pre> <p>Note that <code>.spec.identityRef.name</code> must match the <code>Secret</code> you created in the previous step, and  <code>.spec.identityRef.namespace</code> must be the same as the one that includes the <code>Secret</code> (<code>kcm-system</code>).</p> </li> <li> <p>Create the ConfigMap resource-template object</p> <p>Create a YAML file with the specification of the resource-template and save it as <code>openstack-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: openstack-cloud-config-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n\n    {{- $clouds := fromYaml (index $identity \"data\" \"clouds.yaml\" | b64dec) -}}\n    {{- if not $clouds }}\n      {{ fail \"failed to decode clouds.yaml\" }}\n    {{ end -}}\n\n    {{- $openstack := index $clouds \"clouds\" \"openstack\" -}}\n\n    {{- if not (hasKey $openstack \"auth\") }}\n      {{ fail \"auth key not found in openstack config\" }}\n    {{- end }}\n    {{- $auth := index $openstack \"auth\" -}}\n\n    {{- $auth_url := index $auth \"auth_url\" -}}\n    {{- $app_cred_id := index $auth \"application_credential_id\" -}}\n    {{- $app_cred_name := index $auth \"application_credential_name\" -}}\n    {{- $app_cred_secret := index $auth \"application_credential_secret\" -}}\n\n    {{- $network_id := $cluster.status.externalNetwork.id -}}\n    {{- $network_name := $cluster.status.externalNetwork.name -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: openstack-cloud-config\n      namespace: kube-system\n    type: Opaque\n    stringData:\n      cloud.conf: |\n        [Global]\n        auth-url=\"{{ $auth_url }}\"\n\n        {{- if $app_cred_id }}\n        application-credential-id=\"{{ $app_cred_id }}\"\n        {{- end }}\n\n        {{- if $app_cred_name }}\n        application-credential-name=\"{{ $app_cred_name }}\"\n        {{- end }}\n\n        {{- if $app_cred_secret }}\n        application-credential-secret=\"{{ $app_cred_secret }}\"\n        {{- end }}\n\n        {{- if and (not $app_cred_id) (not $app_cred_secret) }}\n        username=\"{{ index $openstack \"username\" }}\"\n        password=\"{{ index $openstack \"password\" }}\"\n        {{- end }}\n        region=\"{{ index $openstack \"region_name\" }}\"\n\n        [LoadBalancer]\n        {{- if $network_id }}\n        floating-network-id=\"{{ $network_id }}\"\n        {{- end }}\n\n        [Network]\n        {{- if $network_name }}\n        public-network-name=\"{{ $network_name }}\"\n        {{- end }}\n</code></pre> <p>Object needs to be named <code>openstack-cluster-identity-resource-template.yaml</code>, <code>OpenStackClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-resource-template.yaml\n</code></pre> </li> <li> <p>Create Your First Child Cluster</p> <p>To test the configuration, create a YAML file with the specification of your Managed Cluster and save it as <code>my-openstack-cluster-deployment.yaml</code>.  Note that you can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-1-0           true\naws-eks-0-1-0                   true\naws-hosted-cp-0-1-0             true\naws-standalone-cp-0-1-0         true\nazure-aks-0-1-0                 true\nazure-hosted-cp-0-1-0           true\nazure-standalone-cp-0-1-0       true\nopenstack-standalone-cp-0-1-0   true\nvsphere-hosted-cp-0-1-0         true\nvsphere-standalone-cp-0-1-0     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> should look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-0-1-0\n  credential: openstack-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    externalNetwork:\n      filter:\n        name: \"public\"\n    authURL: ${OS_AUTH_URL}\n    identityRef:\n      name: \"openstack-cloud-config\"\n      cloudName: \"openstack\"\n      region: ${OS_REGION_NAME}\n</code></pre> <p>You can adjust <code>flavor</code>, <code>image name</code>, <code>region name</code>, and <code>authURL</code> to match your OpenStack environment. For more information about the configuration options, see the OpenStack Template Parameters Reference.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-openstack-cluster-deployment.yaml\n</code></pre> <p>This will trigger the provisioning process where k0rdent will create a bunch of OpenStack resources such as OpenStackCluster, OpenStackMachineTemplate, OpenStackMachineDeployment, etc. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-openstack-cluster-deployment --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, just like any other Kubernetes cluster:</p> <pre><code>kubectl -n kcm-system get secret my-openstack-cluster-deployment-kubeconfig -o jsonpath='{.data.value&gt;' | base64 -d &gt; my-openstack-cluster-deployment-kubeconfig.kubeconfig\nKUBECONFIG=\"my-openstack-cluster-deployment-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up OpenStack resources, delete the managed cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-openstack-cluster-deployment   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete clusterdeployments my-openstack-cluster-deployment -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-openstack-cluster-deployment\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin-prepare/#vsphere","title":"vSphere","text":"<p>To enable users to deploy child clusers on vSphere, follow these steps:</p> <ol> <li> <p>Create a k0rdent management cluster</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running, as well as a local install of <code>kubectl</code>.</p> </li> <li> <p>Install a vSphere instance version <code>6.7.0</code> or higher.</p> </li> <li> <p>Create a vSphere account with appropriate privileges</p> <p>To function properly, the user assigned to the vSphere Provider should be able to manipulate vSphere resources. The user should have the following  required privileges:</p> <pre><code>Virtual machine: Full permissions are required\nNetwork: Assign network is sufficient\nDatastore: The user should be able to manipulate virtual machine files and metadata\n</code></pre> <p>In addition to that, specific CSI driver permissions are required. See the official doc for more information on CSI-specific permissions.</p> </li> <li> <p>Image template</p> <p>You can use pre-built image templates from the CAPV project or build your own.</p> <p>When building your own image, make sure that VMware tools and cloud-init are installed and properly configured.</p> <p>You can follow the official open-vm-tools guide on how to correctly install VMware tools.</p> <p>When setting up cloud-init, you can refer to the official docs and specifically the VMware datasource docs for extended information regarding cloud-init on vSphere.</p> </li> <li> <p>vSphere network</p> <p>When creating a network, make sure that it has the DHCP service.</p> <p>Also, ensure that part of your network is out of the DHCP range (for example, the network <code>172.16.0.0/24</code> should have a DHCP range of <code>172.16.0.100-172.16.0.254</code> only) so that LoadBalancer services will not create any IP conflicts in the network.</p> </li> <li> <p>vSphere Credentials</p> <p>To enable k0rdent to access vSphere resources, create the appropriate credentials objects. For a full explanation of how <code>Credential</code> objects work, see the main Credentials chapter, but for now, follow these steps:</p> <p>Create a <code>Secret</code> object with the username and password</p> <p>The <code>Secret</code> stores the username and password for your vSphere instance. Save the <code>Secret</code> YAML in a file named <code>vsphere-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  username: &lt;USERNAME&gt;\n  password: &lt;PASSWORD&gt;\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>VSphereClusterIdentity</code> Object</p> <p>The <code>VSphereClusterIdentity</code> object defines the credentials CAPV will use to manage vSphere resources.</p> <p>Save the <code>VSphereClusterIdentity</code> YAML into a file named <code>vsphere-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: VSphereClusterIdentity\nmetadata:\n  name: vsphere-cluster-identity\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretName: vsphere-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>The <code>VSphereClusterIdentity</code> object references the <code>Secret</code> you created in the previous step, so <code>.spec.secretName</code>  needs to match the <code>.metadata.name</code> for the <code>Secret</code>.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity.yaml\n</code></pre> </li> <li> <p>Create the <code>Credential</code> Object</p> <p>Create a YAML with the specification of our credential and save it as <code>vsphere-cluster-identity-cred.yaml</code></p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: vsphere-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: VSphereClusterIdentity\n    name: vsphere-cluster-identity\n    namespace: kcm-system\n</code></pre> Again, <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>VSphereClusterIdentity</code> object you just created.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-cred.yaml\n</code></pre> </li> <li> <p>Create the <code>ConfigMap</code> resource-template Object</p> <p>Create a YAML with the specification of our resource-template and save it as <code>vsphere-cluster-identity-resource-template.yaml</code></p> <p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vsphere-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: vsphere-cloud-secret\n      namespace: kube-system\n    type: Opaque\n    data:\n      {{ printf \"%s.username\" $cluster.spec.server }}: {{ index $secret.data \"username\" }}\n      {{ printf \"%s.password\" $cluster.spec.server }}: {{ index $secret.data \"password\" }}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: vcenter-config-secret\n      namespace: kube-system\n    type: Opaque\n    stringData:\n      csi-vsphere.conf: |\n        [Global]\n        cluster-id = \"{{ $cluster.metadata.name }}\"\n\n        [VirtualCenter \"{{ $cluster.spec.server }}\"]\n        insecure-flag = \"true\"\n        user = \"{{ index $secret.data \"username\" | b64dec }}\"\n        password = \"{{ index $secret.data \"password\" | b64dec }}\"\n        port = \"443\"\n        datacenters = ${VSPHERE_DATACENTER}\n    ---\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: cloud-config\n      namespace: kube-system\n    data:\n      vsphere.conf: |\n        global:\n          insecureFlag: true\n          port: 443\n          secretName: vsphere-cloud-secret\n          secretNamespace: kube-system\n        labels:\n          region: k8s-region\n          zone: k8s-zone\n        vcenter:\n          {{ $cluster.spec.server }}:\n            datacenters:\n              - ${VSPHERE_DATACENTER}\n            server: {{ $cluster.spec.server }}\n</code></pre> Object name needs to be exactly <code>vsphere-cluster-identity-resource-template</code>, <code>VSphereClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-resource-template.yaml\n</code></pre> </li> <li> <p>Create your first Cluster Deployment</p> <p>Test the configuration by deploying a cluster. Create a YAML document with the specification of your Cluster Deployment and save it as <code>my-vsphere-clusterdeployment1.yaml</code>.</p> <p>You can get a list of available templates:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-1-0           true\naws-eks-0-1-0                   true\naws-hosted-cp-0-1-0             true\naws-standalone-cp-0-1-0         true\nazure-aks-0-1-0                 true\nazure-hosted-cp-0-1-0           true\nazure-standalone-cp-0-1-0       true\nopenstack-standalone-cp-0-1-0   true\nvsphere-hosted-cp-0-1-0         true\nvsphere-standalone-cp-0-1-0     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> YAML file should look something like this. Make sure to replace the placeholders with your specific information:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-vsphere-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: vsphere-standalone-cp-0-1-0\n  credential: vsphere-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    controlPlaneNumber: 1\n    workersNumber: 1\n    vsphere:\n      server: &lt;VSPHERE_SERVER&gt;\n      thumbprint: &lt;VSPHERE_THUMBPRINT&gt;\n      datacenter: &lt;VSPHERE_DATACENTER&gt;\n      datastore: &lt;VSPHERE_DATASTORE&gt;\n      resourcePool: &lt;VSPHERE_RESOURCEPOOL&gt;\n      folder: &lt;VSPHERE_FOLDER&gt;\n      username: ${VSPHERE_USER}\n      password: ${VSPHERE_PASSWORD}\n    controlPlaneEndpointIP: &lt;VSPHERE_CONTROL_PLANE_ENDPOINT&gt;\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      rootVolumeSize: 50\n      cpus: 4\n      memory: 4096\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      rootVolumeSize: 50\n      cpus: 4\n      memory: 4096\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n</code></pre> <p>For more information about the available configuration options, see the vSphere Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-vsphere-clusterdeployment1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-vsphere-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-vsphere-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To delete the provisioned cluster and free consumed vSphere resources run:</p> <pre><code>kubectl -n kcm-system delete cluster my-vsphere-clusterdeployment1\n</code></pre> </li> </ol>"},{"location":"admin-rbac/","title":"Role Based Access Control","text":"<p>k0rdent provides the opportunity to use Role Based Access Control in order to try to use the principle of least privilege and only give users access to the objects and resources they absolutely have to have.</p>"},{"location":"admin-rbac/#what-roles-do","title":"What roles do","text":"<p>k0rdent leverages the Kubernetes RBAC system and provides a set of standard <code>ClusterRole</code> objects with associated permissions. These standard <code>ClusterRole</code> objects are created as part of the k0rdent helm chart. k0rdent roles are based on labels and aggregated permissions, meaning they automatically collect rules from other <code>ClusterRole</code> objects with specific labels.</p> <p>The following table outlines the roles available in k0rdent, along with their respective read/write or read-only permissions:</p> Roles Global Admin Global Viewer Namespace Admin Namespace Editor Namespace Viewer Scope Global Global Namespace Namespace Namespace k0rdent management r/w r/o - - - Namespaces management r/w r/o - - - Provider Templates r/w r/o - - - Global Template Management r/w r/o - - - Multi Cluster Service Management r/w r/o - - - Template Chain Management r/w r/o r/w r/o r/o Cluster and Service Templates r/w r/o r/w r/o r/o Credentials r/w r/o r/w r/o r/o Flux Helm objects r/w r/o r/w r/o r/o Cluster Deployments r/w r/o r/w r/w r/o <p>This section provides an overview of all <code>ClusterRole</code> objects available in k0rdent.</p>"},{"location":"admin-rbac/#roles-summary","title":"Roles summary","text":"<p>Note</p> <p> The names of the <code>ClusterRole</code> objects may have different prefixes depending on the name of the k0rdent Helm chart. The <code>ClusterRole</code> object definitions below use the <code>kcm</code> prefix, which is the default name of the k0rdent Helm chart.</p>"},{"location":"admin-rbac/#global-admin","title":"Global Admin","text":"<p>The <code>Global Admin</code> role provides full administrative access across all the k0rdent system.</p> <p>Name: <code>kcm-global-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to the k0rdent API</li> <li>Full access to Flux Helm repositories and Helm charts</li> <li>Full access to Cluster API identities</li> <li>Full access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Admin</code> role is authorized to perform the following actions:</p> <ol> <li>Manage the k0rdent configuration</li> <li>Manage namespaces in the management cluster</li> <li>Manage <code>ProviderTemplate</code> objects: add new templates or remove unneeded ones</li> <li>Manage <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in any namespace, including adding and removing templates</li> <li>Manage Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in any namespace</li> <li>Manage access rules for <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, including distributing templates across namespaces using    <code>TemplateChain</code> objects</li> <li>Manage upgrade sequences for <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects</li> <li>Manage and deploy Services across multiple clusters in any namespace by modifying <code>MultiClusterService</code> resources</li> <li>Manage <code>ClusterDeployment</code> objects in any namespace</li> <li>Manage <code>Credential</code> and <code>Secret</code> objects in any namespace</li> <li>Upgrade k0rdent</li> <li>Uninstall k0rdent</li> </ol>"},{"location":"admin-rbac/#global-viewer","title":"Global Viewer","text":"<p>The <code>Global Viewer</code> role grants read-only access across the k0rdent system. It does not permit any modifications, including the creation of clusters.</p> <p>Name: <code>kcm-global-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-viewer: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to k0rdent API</li> <li>Read access to Flux Helm repositories and Helm charts</li> <li>Read access to Cluster API identities</li> <li>Read access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Viewer</code> role is authorized to perform the following actions:</p> <ol> <li>View the k0rdent configuration</li> <li>List namespaces available in the management cluster</li> <li>List and get the detailed information about available <code>ProviderTemplate</code> objects</li> <li>List available <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in any namespace</li> <li>List and view detailed information about Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in any namespace</li> <li>View access rules for <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, including <code>TemplateChain</code> objects in any namespace</li> <li>View full details about the created <code>MultiClusterService</code> objects</li> <li>List and view detailed information about <code>ClusterDeployment</code> objects in any namespace</li> <li>List and view detailed information about created <code>Credential</code> and <code>Secret</code> objects in any namespace</li> </ol>"},{"location":"admin-rbac/#namespace-admin","title":"Namespace Admin","text":"<p>The <code>Namespace Admin</code> role provides full administrative access within a namespace.</p> <p>Name: <code>kcm-namespace-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployment</code>, <code>Credential</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in the namespace</li> <li>Full access to <code>TemplateChain</code> objects in the namespace</li> <li>Full access to Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Admin</code> role is authorized to perform the following actions within the namespace:</p> <ol> <li>Create and manage all <code>ClusterDeployment</code> objects in the namespace</li> <li>Create and manage <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in the namespace</li> <li>Manage the distribution and upgrade sequences of Templates within the namespace</li> <li>Create and manage Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> <li>Manage <code>Credential</code> objects created by any user in the namespace</li> </ol>"},{"location":"admin-rbac/#namespace-editor","title":"Namespace Editor","text":"<p>The <code>Namespace Editor</code> role allows users to create and modify <code>ClusterDeployment</code> objects within namespace using predefined <code>Credential</code> and <code>Template</code> objects.</p> <p>Name: <code>kcm-namespace-editor-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployment</code> objects in the allowed namespace</li> <li>Read access to <code>Credential</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code>, and <code>TemplateChain</code> objects in the namespace</li> <li>Read access to Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Editor</code> role has the following permissions in the namespace:</p> <ol> <li>Can create and manage <code>ClusterDeployment</code> objects in the namespace using existing <code>Credential</code> and <code>Template</code> objects</li> <li>Can list and view detailed information about the <code>Credential</code> objects available in the namespace</li> <li>Can list and view detailed information about the available <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects and the <code>Template</code>  upgrade sequences</li> <li>Can list and view detailed information about the Flux <code>HelmRepository</code> and <code>HelmChart</code> objects</li> </ol>"},{"location":"admin-rbac/#namespace-viewer","title":"Namespace Viewer","text":"<p>The <code>Namespace Viewer</code> role grants read-only access to resources within a namespace.</p> <p>Name: <code>kcm-namespace-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to <code>ClusterDeployment</code> objects in the namespace</li> <li>Read access to <code>Credential</code>, <code>ClusterTemplate</code>, <code>ServiceTemplate</code>, and <code>TemplateChain</code> objects in the namespace</li> <li>Read access to Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Viewer</code> role has the following permissions in the namespace:</p> <ol> <li>Can list and view detailed information about all the <code>ClusterDeployment</code> objects in the allowed namespace</li> <li>Can list and view detailed information about <code>Credential</code> objects available in the specific namespace</li> <li>Can list and view detailed information about available <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, and <code>Template</code>    upgrade sequences</li> <li>Can list and view detailed information about Flux <code>HelmRepository</code> and <code>HelmChart</code> objects</li> </ol>"},{"location":"admin-service-templates/","title":"Using and Creating ServiceTemplates: Creating and Deploying Applications","text":"<p>Deploying an application, like deploying a cluster, involves applying a template to the management cluster. Rather than a <code>ClusterTemplate</code>, however, applications are deployed using a <code>ServiceTemplate</code>.</p> <p>You can find more information on Bringing Your Own Templates in the Template Guide, but this section gives you an idea of how to create a <code>ServiceTemplate</code> and use it to deploy an application to a k0rdent child cluster.</p> <p>The basic sequence looks like this:</p> <ol> <li> <p>Define the source of the Helm chart that defines the service. The source object must have the label <code>k0rdent.mirantis.com/managed: \"true\"</code>. For example, this YAML describes a custom <code>Source</code> object of <code>kind</code> <code>HelmRepository</code>:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: custom-templates-repo\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/external-templates-repo/charts\n</code></pre> </li> <li> <p>Create the <code>ServiceTemplate</code></p> <p>A template can either define a Helm chart directly using the template's <code>spec.helm.chartSpec</code> field or reference its location using the <code>spec.helm.chartRef</code> field.</p> <p>For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  helm:\n    chartSpec:\n      chart: ingress-nginx\n      version: 4.11.3\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n</code></pre> <p>In this case, we're creating a <code>ServiceTemplate</code> called <code>ingress-nginx-4.11.3</code> in the <code>my-target-namespace</code> namespace.  It references version 4.11.3 of the <code>ingress-nginx</code> chart located in the <code>k0rdent-catalog</code> Helm repository.</p> <p>For more information on creating templates, see the Template Guide.</p> </li> <li> <p>Create a <code>ServiceTemplateChain</code></p> <p>To let k0rdent know where this <code>ServiceTemplate</code> can and can't be used, create a <code>ServiceTemplateChain</code> object, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplateChain\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  supportedTemplates:\n    - name: project-ingress-nginx-4.11.3\n    - name: project-ingress-nginx-4.10.0\n      availableUpgrades:\n        - name: project-ingress-nginx-4.11.3\n</code></pre> <p>Here you see a template called <code>project-ingress-nginx-4.11.3</code> that is meant to be deployed in the <code>my-target-namespace</code> namespace. The <code>.spec.helm.chartSpec</code> specifies the name of the Helm chart and where to find it, as well as the version and other  important information. The <code>ServiceTempateChain</code> shows that this template is also an upgrade path from version 4.10.0.</p> <p>If you wanted to deploy this as an application, you would first go ahead and add it to the cluster in which you were working, so if you were to save this YAML to a file called <code>project-ingress.yaml</code> you could run this command on the management cluster:</p> <pre><code>kubectl apply -f project-ingress.yaml -n my-target-namespace\n</code></pre> </li> <li> <p>Adding a <code>Service</code> to a <code>ClusterDeployment</code></p> <p>To add the service defined by this template to a cluster, you would simply add it to the <code>ClusterDeployment</code> object when you create it, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-0-1-0\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: project-ingress-nginx-4.11.3\n        name: ingress-nginx\n        namespace: my-target-namespace\n    priority: 100\n</code></pre> As you can see, you're simply referencing the template in the <code>.spec.serviceSpec.services[].template</code> field of the <code>ClusterDeployment</code> to tell k0rdent that you want this service to be part of this cluster.</p> <p>If you wanted to add this service to an existing cluster, you would simply patch the definition of the <code>ClusterDeployment</code>, as in:</p> <p><pre><code>kubectl patch clusterdeployment my-cluster-deployment -n my-target-namespace --type='merge' -p '{\"spec\":{\"services\":[{\"template\":\"project-ingress-nginx-4.11.3\",\"name\":\"ingress-nginx\",\"namespace\":\"my-target-namespace\"}]}}'\n</code></pre> For more information on creating and using <code>ServiceTemplate</code> objects, see the User Guide.</p> </li> </ol>"},{"location":"admin-troubleshooting-aws-vpcs/","title":"Troubleshooting AWS VPCs","text":""},{"location":"admin-troubleshooting-aws-vpcs/#aws-vpc-not-removed-when-deleting-eks-cluster","title":"AWS VPC Not Removed When Deleting EKS Cluster","text":"<p>A bug has been fixed in CAPA (Cluster API Provider AWS) for VPC removal: kubernetes-sigs/cluster-api-provider-aws#5192</p> <p>If you find that a VPC has not been deleted, you can deal with it in three different ways:</p>"},{"location":"admin-troubleshooting-aws-vpcs/#applying-ownership-information-on-vpcs","title":"Applying ownership information on VPCs","text":"<p>When VPCs have owner information, all AWS resources will be removed when the k0rdent EKS cluster is deleted. So after provisioning an EKS cluster, the operator can go and set tags (for example, <code>tag:Owner</code>) and it will be  sufficient for CAPA to manage them.</p>"},{"location":"admin-troubleshooting-aws-vpcs/#guardduty-vpce","title":"GuardDuty VPCE","text":"<p>Another way to prevent an issue with non-deleted VPCs is to disable GuardDuty. GuardDuty creates an extra VPCE (VPC Endpoint) not managed by CAPA and when CAPA  starts EKS cluster removal, this VPCE does not get removed.</p>"},{"location":"admin-troubleshooting-aws-vpcs/#manual-removal-of-vpcs","title":"Manual removal of VPCs","text":"<p>When it is impossible to turn off GuardDuty or applying ownership tags is not permitted, you need to remove VPCs manually. Follow these steps.</p> <ol> <li> <p>Look for the affected VPC. The sign of \u201cstuck\u201d VPC looks like a hidden \u201cDelete\u201d button. </p> </li> <li> <p>Open \u201cNetwork Interfaces\u201d and attempt to detach an interface. You will see a disabled \u201cDetach\u201d button: </p> </li> <li> <p>Go to the VPC endpoints screen and remove the end-point:  </p> </li> <li> <p>OK the Endpoint deletion: </p> </li> <li> <p>Wait until the VPCE is completely removed and all network interfaces disappear. </p> </li> <li> <p>Now you can finally remove the VPC: </p> </li> </ol>"},{"location":"admin-upgrading-k0rdent/","title":"Upgrading k0rdent","text":"<p>Upgrading k0rdent involves making upgrades to the <code>Management</code> object. To do that, you must have the <code>Global Admin</code> role. (For detailed information about k0rdent RBAC roles and permissions, refer to the RBAC documentation.) Follow these steps to upgrade k0rdent:</p> <ol> <li> <p>Create a new <code>Release</code> object</p> <p>Start by creating a <code>Release</code> object in the management cluster that points to the desired version. You can see available versions at https://github.com/k0rdent/kcm/releases.  The actual <code>Release</code> object includes information on the templates and resources that are available, as well as the version of the Kubernetes Cluster API.  For example, the v0.1.0 <code>Release</code> object looks like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Release\nmetadata:\n  name: kcm-0-1-0\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  version: 0.1.0\n  kcm:\n    template: kcm-0-1-0\n  capi:\n    template: cluster-api-0-1-0\n  providers:\n    - name: k0smotron\n      template: k0smotron-0-1-0\n    - name: cluster-api-provider-azure\n      template: cluster-api-provider-azure-0-1-0\n    - name: cluster-api-provider-vsphere\n      template: cluster-api-provider-vsphere-0-1-0\n    - name: cluster-api-provider-aws\n      template: cluster-api-provider-aws-0-1-0\n    - name: cluster-api-provider-openstack\n      template: cluster-api-provider-openstack-0-1-0\n    - name: projectsveltos\n      template: projectsveltos-0-45-0\n</code></pre> <p>Thankfully, you don't have to build these YAML files yourself. Once you've chosen a release, you can go ahead and create the release object by referencing the YAML file online, as in:</p> <pre><code>VERSION=v0.1.0\nkubectl create -f https://github.com/k0rdent/kcm/releases/download/${VERSION}/release.yaml\n</code></pre> </li> <li> <p>List Available <code>Releases</code></p> <p>Once you've created the new <code>Release</code> you need to update the <code>Management</code> object to use it. Start by viewing all available <code>Release</code>s:</p> <pre><code>kubectl get releases\n</code></pre> <pre><code>NAME        AGE\nkcm-0-0-6   71m\nkcm-0-0-7   65m\nkcm-0-1-0   12m\n</code></pre> </li> <li> <p>Patch the <code>Management</code> object with the new <code>Release</code></p> <p>Update the <code>spec.release</code> field in the <code>Management</code> object to point to the new release. Replace <code>&lt;release-name&gt;</code> with the name of your desired release:</p> <pre><code>RELEASE_NAME=kcm-0-1-0\nkubectl patch managements.k0rdent.mirantis.com kcm --patch \"{\\\"spec\\\":{\\\"release\\\":\\\"${RELEASE_NAME}\\\"}}\" --type=merge\n</code></pre> </li> <li> <p>Verify the Upgrade</p> <p>Although the change will be made immediately, it will take some time for k0rdent to update the components it should be using. Monitor the readiness of the <code>Management</code> object to ensure the upgrade was successful. For example:</p> <pre><code>kubectl get managements.k0rdent.mirantis.com kcm\nNAME   READY   RELEASE     AGE\nkcm    True    kcm-0-1-0   4m34s\n</code></pre> </li> </ol>"},{"location":"appendix-airgap/","title":"Air-gapped Installation Guide","text":"<p>Warning</p> <p> Currently only the vSphere infrastructure provider supports full air-gapped installation.</p>"},{"location":"appendix-airgap/#prerequisites","title":"Prerequisites","text":"<p>In order to install k0rdent in an air-gapped environment, you will need the following:</p> <ul> <li>An installed k0s cluster that will be used as the management cluster.  If you   do not yet have a k0s cluster, you can follow the Airgapped Installation   documentation.  While k0rdent supports any certified Kubernetes distribution,    we reccomend k0s for airgapped installations because it   implements an OCI image bundle watcher that enables k0s to easily use a bundle   of management cluster images. </li> <li>The <code>KUBECONFIG</code> of the management cluster that will be the target for the k0rdent   installation.</li> <li> <p>A registry that is accessible from the airgapped hosts to store the k0rdent images.   If you do not have a registry you can deploy a local Docker registry   or use mindthegap</p> <p>Warning</p> <p> If using a local Docker registry, ensure the registry URL is added to the <code>insecure-registries</code> key within the Docker <code>/etc/docker/daemon.json</code> file. <pre><code>{\n  \"insecure-registries\": [\"&lt;registry-url&gt;\"]\n}\n</code></pre></p> </li> <li> <p>A registry and associated chart repository for hosting k0rdent charts.  At this   time all k0rdent charts MUST be hosted in a single OCI chart repository.  See   Use OCI-based registries in the   Helm documentation for more information.</p> </li> <li>jq, Helm and Docker binaries   installed on the machine where the <code>airgap-push.sh</code> script will be run.</li> </ul>"},{"location":"appendix-airgap/#installation","title":"Installation","text":"<p>Follow these steps to perform an airgapped installation:</p> <ol> <li> <p>Download the kcm airgap bundle. This bundle contains the following: </p> <ul> <li><code>images/kcm-images-&lt;version&gt;.tgz</code> - The image bundle tarball for the   management cluster, this bundle will be loaded into the management   cluster.</li> <li><code>images/kcm-extension-images-&lt;version&gt;.tgz</code> - The image bundle tarball for   the managed clusters, this bundle will be pushed to a registry where the   images can be accessed by the managed clusters.</li> <li><code>charts</code> - Contains the kcm Helm chart, dependency charts and k0s   extensions charts within the <code>extensions</code> directory.  All of these charts   will be pushed to a chart repository within a registry.</li> <li><code>scripts/airgap-push.sh</code> - A script that will aid in re-tagging and   pushing the <code>ClusterDeployment</code> required charts and images to a desired   registry.</li> </ul> </li> <li> <p>Extract and use the <code>airgap-push.sh</code> script to push the <code>extensions</code> images    and <code>charts</code> contents to the registry.  Ensure you have logged into the    registry using both <code>docker login</code> and <code>helm registry login</code> before running    the script.</p> <pre><code>tar xvf kcm-airgap-&lt;version&gt;.tgz scripts/airgap-push.sh\n./scripts/airgap-push.sh -r &lt;registry&gt; -c &lt;chart-repo&gt; -a kcm-airgap-&lt;version&gt;.tgz\n</code></pre> </li> <li> <p>Next, extract the <code>management</code> bundle tarball and sync the images to the    k0s cluster that will host the management cluster.  See Sync the Bundle File    for more information.</p> <p>Note</p> <p> Multiple image bundles can be placed in the <code>/var/lib/k0s/images</code> directory for k0s to use, so the existing <code>k0s</code> airgap bundle does not need to be merged into the <code>kcm-images-&lt;version&gt;.tgz</code> bundle.</p> <pre><code>tar -C /var/lib/k0s -xvf kcm-airgap-&lt;version&gt;.tgz \"images/kcm-images-&lt;version&gt;.tgz\"\n</code></pre> </li> <li> <p>Install the kcm Helm chart on the management cluster from the registry where    the kcm charts were pushed.  The kcm controller image is loaded as part of    the airgap <code>management</code> bundle and does not need to be customized within the    Helm chart, but the default chart repository configured via    <code>controller.defaultRegistryURL</code> should be set to reference the repository    where charts have been pushed.</p> <pre><code>helm install kcm oci://&lt;chart-repository&gt;/kcm \\\n  --version &lt;version&gt; \\\n  -n kcm-system \\\n  --create-namespace \\\n  --set controller.defaultRegistryURL=oci://&lt;chart-repository&gt;\n</code></pre> </li> <li> <p>Edit the <code>Management</code> object to add the airgap parameters.</p> <p>Note</p> <p> Use <code>insecureRegistry</code> parameter only if you have a plain HTTP registry.</p> <p>The resulting yaml will look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi:\n      config:\n        airgap: true\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: oci://&lt;registry-url&gt;\n          insecureRegistry: true\n  providers:\n  - config:\n      airgap: true\n    name: k0smotron\n  - config:\n      airgap: true\n    name: cluster-api-provider-vsphere\n  - name: projectsveltos\n  release: &lt;release name&gt;\n</code></pre> </li> <li> <p>Place the k0s binary and airgap bundle on an internal server so they're    available over HTTP. This is required for the airgap provisioning process,    because k0s components must be downloaded at each node upon creation.    Alternatively, you can create the following example deployment using the k0s    image provided in the bundle.</p> <p>Note</p> <p> The k0s image version is the same that is defined in the vSphere template by default.</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k0s-ag-image\n  labels:\n    app: k0s-ag-image\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k0s-ag-image\n  template:\n    metadata:\n      labels:\n        app: k0s-ag-image\n    spec:\n      containers:\n      - name: k0s-ag-image\n        image: k0s-ag-image:v1.31.1-k0s.1\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: k0s-ag-image\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: k0s-ag-image\n  type: NodePort\n</code></pre> </li> </ol>"},{"location":"appendix-airgap/#creation-of-the-clusterdeployment","title":"Creation of the ClusterDeployment","text":"<p>In order to successfully deploy a cluster in an airgapped configuration, several configuration options must be properly defined in the <code>.spec.config</code> of the `ClusterDeployment.</p> <p>You must specify the custom image registry and chart repository to be used (the registry and chart repository where the <code>extensions</code> bundle and charts were pushed).</p> <p>Apart from that, you must provide an endpoint where the k0s binary and airgap bundle can be downloaded (step <code>6</code> of the installation procedure).</p> <pre><code>spec:\n config:\n   airgap: true\n   k0s:\n     downloadURL: \"http://&lt;k0s binary endpoint&gt;/k0s\"\n     bundleURL: \"http://&lt;k0s binary endpoint&gt;/k0s-airgap-bundle\"\n   extensions:\n    imageRepository: ${IMAGE_REPOSITORY}\n    chartRepository: ${CHART_REPOSITORY}\n</code></pre>"},{"location":"appendix-dryrun/","title":"Understanding Dry Run mode","text":"<p>The <code>ClusterDeployment</code> process includes a \"dry run\" mode, which enables you to validate your configuration without actually provisioning resources. By default, <code>.spec.dryRun</code> is set to <code>false</code>, but enabling it can help identify potential issues early.</p> <p>Note that if no configuration (<code>.spec.config</code>) is provided, default values from the selected template will populate the object, and <code>.spec.dryRun</code> will automatically be enabled.</p> <p>Example: Dry Run with default configuration:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  dryRun: true\n</code></pre> <p>After validation (this is, you see <code>TemplateReady</code> as a condition in <code>.status.conditions</code>), remove or disable <code>.spec.dryRun</code> to proceed with deployment.</p> <p>Example: Validated <code>ClusterDeployment</code> object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    publicIP: true\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n  status:\n    conditions:\n    - type: TemplateReady\n      status: \"True\"\n      reason: Succeeded\n      message: Template is valid\n    - type: Ready\n      status: \"True\"\n      reason: Succeeded\n      message: ClusterDeployment is ready\n</code></pre>"},{"location":"appendix-extend-mgmt/","title":"Extended management configuration","text":""},{"location":"appendix-extend-mgmt/#extended-management-configuration","title":"Extended Management Configuration","text":"<p>k0rdent is deployed with the following default configuration, which may vary depending on the release version:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm: {}\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: cluster-api-provider-azure\n  - name: cluster-api-provider-openstack\n  - name: cluster-api-provider-vsphere\n  - name: projectsveltos\nrelease: kcm-0-0-7\n</code></pre> <p>As you can see, the <code>Management</code> object defines the providers that are available from within k0rdent. Some of these are providers directly used by the user, such as aws, azure, and so on, and others are used internally by k0rdent, such as Sveltos.</p> <p>To see what is included in a specific release, look at the <code>release.yaml</code> file in the tagged release. For example, here is the v0.0.7 release.yaml.</p> <p>k0rdent allows you to customize its default configuration by modifying the spec of the <code>Management</code> object. This enables you to manage the list of providers to deploy and adjust the default settings for core components.</p> <p>For detailed examples and use cases, refer to Examples and Use Cases</p>"},{"location":"appendix-extend-mgmt/#configuration-guide","title":"Configuration Guide","text":"<p>There are two options to override the default management configuration of k0rdent:</p> <ol> <li> <p>Update the <code>Management</code> object after the k0rdent installation using <code>kubectl</code>:</p> <p><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; edit management</code></p> </li> <li> <p>Deploy k0rdent skipping the default <code>Management</code> object creation and provide your    own <code>Management</code> configuration:</p> <ul> <li> <p>Create <code>management.yaml</code> file and configure core components and providers.   For example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: \"oci://ghcr.io/my-oci-registry-name/kcm/charts\"\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: projectsveltos\nrelease: kcm-0-0-7\n</code></pre>       In the example above, the <code>Management</code> object is configured with custom registry settings for the KCM controller       and a reduced list of providers.</p> </li> <li> <p>Specify <code>--create-management=false</code> controller argument and install k0rdent:   If installing using <code>helm</code> add the following parameter to the <code>helm   install</code> command:</p> <pre><code>--set=\"controller.createManagement=false\"\n</code></pre> </li> <li> <p>Create <code>kcm</code> <code>Management</code> object after k0rdent installation:</p> <pre><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; create -f management.yaml\n</code></pre> </li> </ul> </li> </ol> <p>You can customize the default configuration options for core components by updating the <code>.spec.core.&lt;core-component-name&gt;.config</code> section in the <code>Management</code> object. For example, to override the default settings for the KCM component, modify the <code>spec.core.kcm.config</code> section. To view the complete list of configuration options available for kcm, refer to: KCM Configuration Options for k0rdent v0.0.7 (Replace v0.0.7 with the relevant release tag for other k0rdent versions).</p> <p>To customize the list of providers to deploy, update the <code>.spec.providers</code> section. You can add or remove providers and configure custom templates for each provider. Each provider in the list must include the <code>name</code> field and may include the <code>template</code> and <code>config</code> fields:</p> <pre><code>- name: &lt;provider-name&gt; \n  template: &lt;provider-template&gt; # optional. If omitted, the default template from the `Release` object will be used\n  config: {} # optional provider configuration containing provider Helm Chart values in YAML format\n</code></pre>"},{"location":"appendix-extend-mgmt/#examples-and-use-cases","title":"Examples and Use Cases","text":""},{"location":"appendix-extend-mgmt/#configuring-a-custom-oci-registry-for-kcm-components","title":"Configuring a Custom OCI Registry for KCM components","text":"<p>You can override the default registry settings in k0rdent by specifying the <code>defaultRegistryURL</code>, <code>insecureRegistry</code>, and <code>registryCredsSecret</code> parameters under <code>spec.core.kcm.config.controller</code>:</p> <ul> <li><code>defaultRegistryURL</code>: Specifies the registry URL for downloading Helm charts representing templates.  Use the <code>oci://</code> prefix for OCI registries. Default: <code>oci://ghcr.io/k0rdent/kcm/charts</code>.</li> <li><code>insecureRegistry</code>: Allows connecting to an HTTP registry. Default: <code>false</code>.</li> <li><code>registryCredsSecret</code>: Specifies the name of a Kubernetes Secret containing authentication credentials for the  registry (optional). This Secret should exist in the system namespace (default: <code>kcm-system</code>).</li> </ul> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: \"oci://ghcr.io/my-private-oci-registry-name/kcm/charts\"\n          insecureRegistry: true\n          registryCredsSecret: my-private-oci-registry-creds\n</code></pre> <p>Example of a Secret with Registry Credentials:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-private-oci-registry-creds\n  namespace: kcm-system\nstringData:\n  username: \"my-user-123\"\n  password: \"my-password-123\"\n</code></pre> <p>The KCM controller will create the default HelmRepository using the provided configuration and fetch KCM components from this repository. For the example above, the following <code>HelmRepository</code> will be created:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\n  name: my-private-oci-registry-name\n  namespace: kcm-system\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/my-private-oci-registry-name/kcm/charts\n  secretRef:\n    name: my-private-oci-registry-creds\n</code></pre>"},{"location":"appendix-extend-mgmt/#configuring-a-custom-image-for-kcm-controllers","title":"Configuring a Custom Image for KCM controllers","text":"<p>You can override the default image for the KCM controllers by specifying the <code>repository</code>, <code>tag</code> and <code>pullPolicy</code> parameters under <code>spec.core.kcm.config.image</code>: </p> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        image:\n          repository: ghcr.io/my-custom-repo/kcm/controller\n          tag: v0.0.7\n          pullPolicy: IfNotPresent\n</code></pre>"},{"location":"appendix-providers/","title":"Cloud Provider Credentials Management in CAPI","text":"<p>Cloud provider credentials in Cluster API (CAPI) environments are managed through objects in the management cluster. <code>Credential</code>, <code>ClusterIdentity</code>, and <code>Secret</code> (related to <code>ClusterIdentity</code>) objects handle credential storage and management, while a dedicated <code>ConfigMap</code> object is used as a template to render configuration into child clusters.</p>"},{"location":"appendix-providers/#configuration-patterns","title":"Configuration Patterns","text":"<p>The configuration follows two patterns:</p> <ol> <li>ClusterIdentity Pattern</li> <li>Uses a <code>ClusterIdentity</code> resource that defines provider identity configuration</li> <li>References a <code>Secret</code> with credentials</li> <li> <p>Used by <code>Azure</code> and <code>vSphere</code> in-tree providers</p> </li> <li> <p>Source Secret Pattern</p> </li> <li>Uses only a <code>Secret</code> without <code>ClusterIdentity</code></li> <li><code>Secret</code> contains all cloud configuration data</li> <li>Used by <code>OpenStack</code> in-tree provider</li> </ol> <p>In both cases <code>ConfigMap</code> with template code is used to render configuration into child clusters.</p>"},{"location":"appendix-providers/#credential-resource","title":"Credential Resource","text":"<p>The <code>Credential</code> resource provides an abstraction layer by either: - Referencing a <code>ClusterIdentity</code> through <code>identityRef</code> - Directly referencing a <code>Secret</code>, depending on the pattern used</p>"},{"location":"appendix-providers/#template-configmap","title":"Template ConfigMap","text":"<ul> <li>Marked with <code>projectsveltos.io/template: \"true\"</code> annotation</li> <li>Contains Go template code for generating child cluster resources via the Sveltos templating system</li> <li>Template processing accesses cluster objects through:</li> <li>Built-in Sveltos variables (<code>Cluster</code>, <code>InfrastructureProvider</code>)</li> <li><code>getResource</code> function for additionally exposed objects (<code>InfrastructureProviderIdentity</code>, <code>InfrastructureProviderIdentitySecret</code>)</li> </ul>"},{"location":"appendix-providers/#templating-system","title":"Templating System","text":"<p>The templating system leverages: - Golang templating - Sprig functions - Sveltos resource manipulation functions</p>"},{"location":"appendix-providers/#examples","title":"Examples","text":"<p>Provider-specific examples are available in <code>*.credentials.yaml</code> files here.</p> <p>Let's take <code>Azure</code> provider as an example azure-credentials.yaml</p> <ul> <li><code>ClusterIdentity</code></li> <li><code>Secret</code> (related to <code>ClusterIdentity</code>)</li> <li><code>Credential</code></li> <li><code>ConfigMap</code></li> </ul>"},{"location":"appendix-providers/#provider-registration","title":"Provider Registration","text":"<p>Providers are registered through YAML configuration files mounted into a predefined path in the manager container at startup using <code>ConfigMap</code>.</p>"},{"location":"appendix-providers/#examples_1","title":"Examples","text":"<p>Provider configuration examples can be found here</p> <p>Let's take <code>Azure</code> provider as an example azure.yml, as seen, the definition is straightforward.</p>"},{"location":"glossary/","title":"k0rdent Glossary","text":"<p>This glossary is a collection of terms related to k0rdent. It clarifies some of the unique terms and concepts we use or explains more common ones that may need a little clarity in the way we use them.</p>"},{"location":"glossary/#beach-head-services","title":"Beach-head Services","text":"<p>We use the term to refer to those Kubernetes services that need to be installed on a Kubernetes cluster to make it actually useful, for example: an ingress controller, CNI, and/or CSI. While from the perspective of how they are deployed they are no different from other Kubernetes services, we define them as distinct from the apps and services deployed as part of the applications.</p>"},{"location":"glossary/#cluster-api-capi","title":"Cluster API (CAPI)","text":"<p>CAPI is a Kubernetes project that provides a declarative way to manage the lifecycle of  Kubernetes clusters. It abstracts the underlying infrastructure, allowing users to  create, scale, upgrade, and delete clusters using a consistent API. CAPI is extensible  via providers that offer infrastructure-specific functionality, such as AWS, Azure, and  vSphere.</p>"},{"location":"glossary/#capi-provider-see-also-infrastructure-provider","title":"CAPI provider (see also Infrastructure provider)","text":"<p>A CAPI provider is a Kubernetes CAPI extension that allows k0rdent to manage and drive  the creation of clusters on a specific infrastructure via API calls.</p>"},{"location":"glossary/#capa","title":"CAPA","text":"<p>CAPA stands for Cluster API Provider for AWS.</p>"},{"location":"glossary/#capg","title":"CAPG","text":"<p>CAPG stands for Cluster API Provider for Google Cloud.</p>"},{"location":"glossary/#capo","title":"CAPO","text":"<p>CAPO stands for Cluster API Provider for OpenStack.</p>"},{"location":"glossary/#capv","title":"CAPV","text":"<p>CAPV stands for Cluster API Provider for vSphere.</p>"},{"location":"glossary/#capz","title":"CAPZ","text":"<p>CAPZ stands for Cluster API Provider for Azure.</p>"},{"location":"glossary/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<p>Cloud Controller Manager (CCM) is a Kubernetes component that embeds logic to manage a  specific infrastructure provider.</p>"},{"location":"glossary/#cluster-deployment","title":"Cluster Deployment","text":"<p>A Kubernetes cluster created and managed by k0rdent.</p>"},{"location":"glossary/#clusteridentity","title":"ClusterIdentity","text":"<p>ClusterIdentity is a Kubernetes object that references a Secret object containing  credentials for a specific infrastructure provider.</p>"},{"location":"glossary/#credential","title":"Credential","text":"<p>A <code>Credential</code> is a custom resource (CR) in kcm that supplies k0rdent with the necessary  credentials to manage a specific infrastructure. The credential object references other  CRs with infrastructure-specific credentials such as access keys, passwords,  certificates, etc. This means that a credential is specific to the CAPI provider that  uses it.</p>"},{"location":"glossary/#k0rdent-cluster-manager-kcm","title":"k0rdent Cluster Manager (kcm)","text":"<p>Deployment and life-cycle management of Kubernetes clusters, including configuration,  updates, and other CRUD operations.</p>"},{"location":"glossary/#k0rdent-observability-and-finops-kof","title":"k0rdent Observability and FinOps (kof)","text":"<p>Cluster and beach-head services monitoring, events and log management.</p>"},{"location":"glossary/#k0rdent-state-manager-ksm","title":"k0rdent State Manager (ksm)","text":"<p>Installation and life-cycle management of beach-head services, policy, Kubernetes API  configurations and more.</p>"},{"location":"glossary/#hosted-control-plane-hcp","title":"Hosted Control Plane (HCP)","text":"<p>An HCP is a Kubernetes control plane that runs outside of the clusters it manages.  Instead of running the control plane components (like the API server, controller  manager, and etcd) within the same cluster as the worker nodes, the control plane is  hosted on a separate, often centralized, infrastructure. This approach can provide  benefits such as easier management, improved security, and better resource utilization,  as the control plane can be scaled independently of the worker nodes.</p>"},{"location":"glossary/#infrastructure-provider-see-also-capi-provider","title":"Infrastructure provider (see also CAPI provider)","text":"<p>An infrastructure provider (aka <code>InfrastructureProvider</code>) is a Kubernetes custom  resource (CR) that defines the infrastructure-specific configuration needed for managing  Kubernetes clusters. It enables Cluster API (CAPI) to provision and manage clusters on  a specific infrastructure platform (e.g., AWS, Azure, VMware, OpenStack, etc.).</p>"},{"location":"glossary/#multi-cluster-service","title":"Multi-Cluster Service","text":"<p>The <code>MultiClusterService</code> is a custom resource used to manage deployment of beach-head  services across multiple clusters.</p>"},{"location":"glossary/#management-cluster","title":"Management Cluster","text":"<p>The Kubernetes cluster where k0rdent is installed and from which all other managed  clusters are managed.</p>"},{"location":"guide-to-quickstarts/","title":"Guide to QuickStarts","text":"<p>The following QuickStart chapters provide a recipe for quickly installing and trying k0rdent. Setting up k0rdent for production is detailed in the Administrator Guide.</p>"},{"location":"guide-to-quickstarts/#what-the-quickstart-covers","title":"What the QuickStart covers","text":"<p>The goal of the QuickStart is:</p> <ul> <li>To get a working environment set up for managing k0rdent.</li> <li>To get a Kubernetes cluster and other tools set up for hosting k0rdent itself.</li> <li>To select a cloud environment (AWS or Azure) and configure k0rdent to lifecycle manage clusters on this substrate.</li> <li>To use k0rdent to deploy a managed cluster.</li> <li>(Optional stretch goal) It's also possible to set up the k0rdent management cluster to simultaneously lifecycle manage clusters on both cloud environments.</li> </ul>"},{"location":"guide-to-quickstarts/#where-the-quickstart-leads","title":"Where the QuickStart leads","text":"<p>The QuickStart shows and briefly explains the hows, whys, and wherefores of manually setting up k0rdent for use. Once built and validated, the QuickStart setup can be leveraged to begin an expanding sequence of demos that let you explore k0rdent's many features. The demos presently use makefiles to speed and simplify setup and operations. We strongly recommend exploring this fast-evolving resource.</p>"},{"location":"guide-to-quickstarts/#quickstart-prerequisites","title":"QuickStart Prerequisites","text":"<p>QuickStart prerequisites are simple \u2014 you'll need:</p> <ul> <li>A desktop or virtual machine running a supported version of linux. You'll use this machine to install a basic Kubernetes working environment, and to host a single-node k0s Kubernetes management cluster to host k0rdent components. For simplest setup, configure this machine as follows:</li> <li>A minimum of 32GB RAM, 8 vCPUs, 100GB SSD (for example, AWS <code>t3.2xlarge</code> or equivalent)</li> <li>Set up for SSH access using keys (standard for cloud VMs)</li> <li>Set up for passwordless sudo (that is, edit <code>/etc/sudoers</code> to configure your user to issue <code>sudo</code> commands without a password challenge)</li> <li>Inbound traffic: SSH (port 22) and ping from your laptop's IP address</li> <li>Outbound traffic: All to any IP address</li> <li>Apply all recent updates and upgrade local applications (<code>sudo apt update</code>/<code>sudo apt upgrade</code>)</li> <li>(Optional) snapshot the machine in its virgin state</li> <li>Administrative-level access to an AWS or Azure cloud account, depending on which cloud environment you prefer. k0rdent will leverage this cloud to provide infrastructure for hosting child clusters.</li> </ul>"},{"location":"guide-to-quickstarts/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Any linux based os that supports deploying k0s will work, though you may need to adjust the suggested commands.</p> OS Package Manager Link Ubuntu Server <code>apt</code> 22.04.5 LTS, Jammy Jellyfish <p>Note</p> <p>Other recent versions of 'enterprise' Linux should work with the following instructions as well, though you will need to adapt for different package managers and perhaps use slightly-different provider-recommended methods for installing required dependencies (for example, Helm). Once you've installed k0rdent in the management cluster and have kubectl, Helm, and other resources connected, you'll mostly be dealing with Kubernetes, and everything should work the same way on any host OS.</p>"},{"location":"guide-to-quickstarts/#limitations","title":"Limitations","text":"<p>This QuickStart guides you in quickly creating a minimal k0rdent working environment. Setting up k0rdent for production is detailed in the Administrator Guide.</p> <p>The current QuickStart focuses on AWS and Azure cloud environments, and guides in creating 'standalone' clusters. In k0rdent parlance, that means 'CNCF-certified Kubernetes clusters with control planes and workers hosted on cloud virtual machines.' The 'CNCF-certified Kubernetes cluster' (in this case) is the k0s Kubernetes distro.</p> <p>k0rdent can do so much more today. Let's take a look at what's possible.</p>"},{"location":"guide-to-quickstarts/#coming-soon","title":"Coming soon","text":"<p>QuickStarts for other Kubernetes distros, clouds, and environments will appear in the near future (short-term roadmap below):</p> <ul> <li>AWS EKS hosted \u2014 Amazon Elastic Kubernetes Service managed clusters </li> <li>Azure AKS hosted \u2014 Azure Kubernetes Service</li> <li>vSphere standalone \u2014 k0s Kubernetes on vSphere virtual machines</li> <li>OpenStack standalone \u2014 k0s Kubernetes on OpenStack virtual machines</li> </ul> <p>Plus (intermediate-term roadmap) tutorials for using k0rdent to create and manage hybrid, edge, and distributed platforms with Kubernetes-hosted control planes and workers on local or remote substrates will be available soon.</p> <p>Demo/Tutorials: We will also be converting the demos gradually into tutorials that explain how to use k0rdent for:</p> <ul> <li>Adding services to individual managed clusters, enabling management of complete platforms/IDPs</li> <li>Adding services to multiple managed clusters, enabling at-scale implementation and lifecycle management of standardized environments</li> <li>(As a Platform Architect) Authorizing cluster and service templates for use by others, and constraining their use within guardrails (enabling self-service)</li> <li>(As an authorized user) Leveraging shared cluster and service templates to lifecycle manage platforms (performing self-service)</li> <li>... and more</li> </ul> <p>Next you'll learn how to use k0rdent.</p>"},{"location":"k0rdent-architecture/","title":"k0rdent architecture","text":"<p>The k0rdent architecture follows a declarative approach to cluster management using Kubernetes principles. The modular extensible architecture provides for a repeatable template-driven solution to interact with sub components such as the Cluster API (CAPI) and other Kubernetes components.</p> <p>The key principles of the architecture include:</p> <ul> <li>Leveraging Kubernetes core principles</li> <li>Its highly aligned but loosely coupled architecture</li> <li>A pluggable and extensible architecture</li> <li>A template-driven approach for repeatability</li> <li>A Standards-driven API</li> <li>Leveraging unmodified upstream components (e.g.CAPI)</li> <li>Supporting integration with custom components downstream</li> </ul> <p>Note</p> <p> This document is a ongoing work in progress, and we would welcome suggestions and questions. </p>"},{"location":"k0rdent-architecture/#summary","title":"Summary","text":"<p>The Management Cluster can orchestrate the provisioning and lifecycle of multiple child clusters on multiple clouds and infrastructures, keeping you from having to directly interact with individual infrastructure providers. By abstracting infrastructure, k0rdent promotes reusability (reducing, for example, the effort required to implement an IDP on a particular cloud), encourages standardization where practical, and lets you use the clouds and technologies you want, while also minimizing the cost of switching components (for example, open source subsystems, cloud substrates, and so on) if you need to.</p> <p>The k0rdent architecture comprises the following highlevel components:</p> <ul> <li>Cluster Management: Tools and controllers for defining, provisioning, and managing clusters.</li> <li>State Management: Controllers and systems for monitoring, updating, and managing the state of child clusters and their workloads.</li> <li>Infrastructure Providers: Services and APIs responsible for provisioning resources such as virtual machines, networking, and storage for clusters.</li> <li>Templates: Templates that can be used to define and create managed child clusters or the workloads that run on them.</li> </ul> <p></p>"},{"location":"k0rdent-architecture/#management-cluster","title":"Management cluster","text":"<p>The management cluster is the core of the k0rdent architecture. It hosts all of the controllers needed to make k0rdent work. This includes:</p> <ul> <li>k0rdent Cluster Manager (KCM) Controller:  KCM provides a wrapper for k0rdent\u2019s CAPI-related capabilities. It orchestrates:<ul> <li>Cluster API (CAPI) Controllers: CAPI controllers are designed to work with specific infrastructure providers. For example, one CAPI controller manages the creation and lifecycle of Kubernetes clusters running on Amazon Web Services, while another manages those on Azure. It\u2019s also possible to create custom CAPI controllers to integrate with internal systems.</li> <li>k0smotron Controller: k0smotron extends CAPI with additional functionality, including control plane and worker node bootstrap providers for k0s Kubernetes, and a control plane provider that supports Hosted Control Plane creation (for example, k0s control planes in pods on a host Kubernetes cluster, which can be the same cluster that hosts k0rdent). The k0smotron project has also provided a so-called \u2018RemoteMachine\u2019 infrastructure provider for CAPI, enabling deployment and cluster operations via SSH on arbitrary remote Linux servers (including small-scale edge devices).</li> </ul> </li> <li>k0rdent Service Manager (KSM) Controller: KSM is responsible for lifecycle managing (deploy, scale, update, upgrade, teardown) services and applications on clusters, and for doing continuous state management of these services and applications. This is currently part of the KCM code base; we may split it out in the future. It orchestrates:<ul> <li>Services Controller: Responsible for coordinating kubernetes services such as combinations of services (and infrastructure provisioning dependencies) that add capabilities to the platform. For example, Nginx, with its dependencies, can be packaged as a service. Artifacts for services are stored locally or in an OCI repository, and are referenced as kubernetes CRD objects.</li> </ul> </li> <li>k0rdent Observability &amp; FinOps (KOF) Controller (not depicted in above diagram): k0rdent Observability and FinOps provides enterprise-grade observability and FinOps capabilities for k0rdent-managed Kubernetes clusters. It enables centralized metrics, logging, and cost management through a unified OpenTelemetry-based architecture.</li> </ul> <p>We\u2019ll take a closer look at these pieces under Roles and Responsibilities.</p>"},{"location":"k0rdent-architecture/#cluster-deployments","title":"Cluster Deployments","text":"<p>A cluster deployment is also known as a child cluster, or a workload cluster. It\u2019s a Kubernetes cluster provisioned and managed by the management cluster, and it\u2019s where developers run their applications and workloads. These are \u201cregular\u201d Kubernetes clusters, and don\u2019t host any management components. Clusters are fully isolated from the management cluster via namespaces, and also from each other, making it possible to create multi-tenant environments. </p> <p>You can tailor a child cluster to specific use cases, with customized addons such as ingress controllers, monitoring tools, and logging solutions. You can also define specific Kubernetes configurations (for example, network policies, storage classes, and security policies) so they work for you and your applications or environments.</p> <p>Simply put, child clusters are where applications and workloads run.</p>"},{"location":"k0rdent-architecture/#templates","title":"Templates","text":"<p>One of the important tenets of the platform engineering philosophy is the use of Infrastructure as Code, but k0rdent takes that one step further through the use of templates. Templates are re-usable text definitions of components that can be used to create and manage clusters. Templates provide a declarative way for users and developers to deploy and manage complex clusters or components while massively reducing the number of parameters they need to configure. Considered generally, k0rdent templates are:</p> <ul> <li>Formatted using YAML: Templates use YAML as an abstraction to represent the target state, so they\u2019re human-readable and editable.</li> <li>Designed to be used in multiple contexts using runtime parameterization: Through the use of placeholders, you can customize templates at runtime without having to directly edit the template.</li> <li>Used for both cluster creation and addon management: Users can define a cluster using YAML, or they can define addons, such as an ingress operator or monitoring tools, to be added to those clusters.</li> <li>Of limited scope: k0rdent lets you set restrictions over what templates can be deployed by whom. For example, as the platform manager (see Roles and Responsibilities), you can specify that non-admin users can only execute templates that deploy a particular set of controllers.</li> </ul> <p>Major template types used in k0rdent include:</p> <ul> <li>Cluster Templates: <code>ClusterTemplate</code>s define clusters in coordination with the clouds and infrastructures they run on. They're designed to be immutable \u2014 they get invoked by k0rdent objects like <code>ClusterDeployment</code>s to create and manage individual clusters and groups of clusters.</li> <li>Service Templates: <code>ServiceTemplate</code>s define services, addons, and workloads that run on clusters. They're also designed to be immutable, and get invoked by 'ClusterDeployment's and other k0rdent objects so that IDPs/platforms can be declared and managed as units. </li> </ul>"},{"location":"k0rdent-architecture/#roles-and-responsibilities","title":"Roles and responsibilities","text":"<p>k0rdent was designed to be used by several groups of people, with hierarchical and complementary roles and responsibilities. You may have your own names for them, but we\u2019ll refer to them as:</p> <ul> <li>Platform Architect: This person or team has global responsibility to the business and technical stakeholders for designing IDPs/platforms for later adaptation to particular clouds and infrastructures, workloads, performance and cost objectives, security and regulatory regimes, and operational requirements. k0rdent enables Platform Architects to create sets of reusable <code>ClusterTemplate</code>s and <code>ServiceTemplate</code>s, closely defining IDPs/platforms in the abstract.</li> <li>Platform Lead: This person or team (sometimes referred to as 'CloudOps') is primarily responsible for actions corresponding to k0rdent Cluster Manager (KCM). They adapt <code>ClusterTemplate</code>s to the correct cloud, and they make sure that everything is working properly. They\u2019re also responsible for limiting the Project Team\u2019s access to <code>Cluster</code> and <code>Service</code> templates necessary to do their jobs. For example, they might limit the templates that can be deployed to an approved set, or provide CAPI operators for only the clouds on which the company wants applications to run, helping to eliminate shadow IT. </li> <li>Platform Engineer: This person or team is responsible for the day-to-day management of the environment. They use <code>ClusterTemplate</code>s and <code>ServiceTemplate</code>s provided by the Platform Lead (as authorized to do so) and may create additional <code>ServiceTemplate</code>s to customize their own Kubernetes cluster so that it\u2019s appropriate for their application.</li> </ul>"},{"location":"k0rdent-architecture/#credentials","title":"Credentials","text":"<p>Creating and managing Kubernetes clusters requires having the proper permissions on the target infrastructure, but you certainly wouldn\u2019t want to give out your AWS account information to every single one of your developers.</p> <p>To solve this problem, k0rdent lets you create a <code>Credential</code> object that provides the access your developers need. It works like this:</p> <ol> <li>The platform lead creates a provider-specific <code>ClusterIdentity</code> and <code>Secret</code> that include all of the information necessary to perform various actions.</li> <li>The platform lead then creates a <code>Credential</code> object that references the <code>ClusterIdentity</code>.</li> <li>Developers reference the <code>Credential</code> object, which gives the cluster the ability to access these credentials (little \u201cc\u201d) without having to expose them to developers directly.</li> </ol>"},{"location":"k0rdent-architecture/#tldr-conclusion","title":"TL;DR - Conclusion","text":"<p>k0rdent provides a comprehensive Kubernetes lifecycle management framework through its three core components:</p> <ul> <li>KCM: Cluster provisioning and management.</li> <li>KSM: Application and runtime state management.</li> <li>KOF: Observability, logging, and cost optimization.</li> </ul> <p>With multi-provider support, templated deployments, and strong security controls, k0rdent is being built to enable scalable, efficient, and consistent Kubernetes operations.</p>"},{"location":"k0rdent-architecture/#terms-and-clarification","title":"Terms and Clarification","text":"<p>Declarative approach: We define the declarative approach to cluster management using the Kubernetes principles as the process where you define the state you want within custom resource objects and the controllers or customer operators ensure that the system moves toward that desired state.</p>"},{"location":"k0rdent-documentation-contributors-guide/","title":"k0rdent documentation contributors' guide","text":"<p>k0rdent documentation is growing rapidly, and community members are very welcome to contribute by filing pull requests against documents in this repository. Thank you for your interest in k0rdent!</p> <p>This page gives technical guidance for contributing to k0rdent documentation. The k0rdent documentation style guide details our markdown style and provides general recommendations for writing style.</p>"},{"location":"k0rdent-documentation-contributors-guide/#how-k0rdent-docs-are-built","title":"How k0rdent docs are built","text":"<p>k0rdent documentation is built from Markdown (.md) documents using the MkDocs static site generator with the Material theme. We use several MkDocs extensions and plugins for rendering glossary items, admonitions (e.g., Note, Tip, Warning, etc.) and other special kinds of content.</p> <p>Below you'll find instructions for setting up MkDocs locally to dynamically build and serve the k0rdent documentation from your local clone of the docs repository. This is handy for reviewing the rendered look and feel of changes and additions before filing a pull request.</p>"},{"location":"k0rdent-documentation-contributors-guide/#how-k0rdent-docs-are-written","title":"How k0rdent docs are written","text":"<p>k0rdent documentation is written mostly in CommonMark as adopted by GitHub, with a few CommonMark-compatible markdown enhancements used by MkDocs, the Material theme, and extensions (see below) to enable more-readable rendering of code, glossary items, images, and admonitions (e.g., NOTE:, TIP:, WARNING:, etc.). The k0rdent documentation style guide details our markdown style, along with those extensions.</p>"},{"location":"k0rdent-documentation-contributors-guide/#how-k0rdent-docs-is-structured","title":"How k0rdent docs is structured","text":"<p>The structure of k0rdent/docs is shown below:</p> <pre><code>docs                                                # repository containing folder /docs\n\u251c\u2500\u2500 docs                                            # subdirectory for /docs, containing further subdirectories for assets, images, theme, etc.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 assets                                      # illustrations for docs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 css                                         # css stylesheets\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 custom_theme                                # Material theme customizations\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 img                                         # site imagery (logos, icons, etc.)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 stylesheets                                 # extra CSS stylesheets\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md                                    # default homepage of the docs site\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 LICENSE                                     # k0rdent docs license (CC-BY-4.0)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (documentation-files.md)                    # actual docs pages (markdown documents)\n\u251c\u2500\u2500 mkdocs.yml                                      # main configuration file for mkdocs, specifies site navigation and table of contents\n\u251c\u2500\u2500 requirements.txt                                # mkdocs requirements (for installation with pip for building docs locally - see below)\n\u251c\u2500\u2500 Dockerfile                                      # Dockerfile for building a container to serve docs interactively\n\u251c\u2500\u2500 .gitignore                                      # Things to ignore when people push\n\u2514\u2500\u2500 .github                                         # Special GitHub features, including allowed-patterns.yml for placeholders replacing secrets\n</code></pre> <p>Note</p> <p> The k0rdent documentation repository is called <code>docs</code> and cloning the repo will create a containing directory called <code>docs</code>. This directory itself contains a subdirectory <code>/docs</code> which contains Markdown (.md) doc files and related subdirectories like /assets, /img, etc. Do not confuse toplevel docs with subdirectory /docs.</p>"},{"location":"k0rdent-documentation-contributors-guide/#raising-issues","title":"Raising issues","text":"<p>If you find issues (errors, omissions, etc.) with k0rdent documentation, we would be very grateful if you would notify us by creating an issue.</p>"},{"location":"k0rdent-documentation-contributors-guide/#contributing-to-k0rdent-docs-simplest-way-use-githubs-webui","title":"Contributing to k0rdent docs - simplest way: use GitHub's webUI","text":"<p>For making small corrections to k0rdent docs, the simplest tool is GitHub's website. You will need a personal (free) or business GitHub account.</p> <ol> <li>Familiarize yourself with the structure of the <code>k0rdent/docs</code> repo.</li> <li>Figure out where the doc you want to update lives, or where a new doc you create might need to live.</li> <li>Visit the k0rdent/docs repo itself. Search for <code>k0rdent/docs</code> or click the following link: (https://github.com/k0rdent/docs)</li> <li>To propose changes to existing docs: On k0rdent/docs, edit the doc you want to change, using the pencil icon (Edit) button in the upper right of GitHub's webUI. This will create a fork of k0rdent/docs in your own account's workspace (if you don't already have your own fork) and create a new feature branch, from which your proposed update will be submitted to k0rdent/docs as a pull request (PR).</li> <li>To create new docs (or propose changes to existing docs): Create a fork of k0rdent/docs in your own account workspace using the Fork button in the upper right of k0rdent/docs. Navigate to the directory in which you want to create your new doc (or go to the directory in which the doc you want to edit lives). Click Add File&gt;Create New File to add a new blank doc, upload an image or other file, or find and edit the doc you want to change.</li> <li>When your edit(s) are complete, click Commit Changes in the upper right.</li> <li>Write a descriptive commit message so project k0rdent docs maintainers understand your proposed addition or change.</li> <li>Click Propose Change.</li> <li>On the next screen, compare your change with content originally there.</li> <li>Click Create Pull Request.</li> </ol> <p>Maintainers will see your pull request and may request discussion or suggest further changes, or they may merge your change. You'll be notified via GitHub either way.</p>"},{"location":"k0rdent-documentation-contributors-guide/#making-regular-contributions-use-git-on-the-desktop","title":"Making regular contributions - use Git on the desktop","text":"<p>For making regular or at-scale contributions to k0rdent documentation, we recommend using a standard open source workflow as follows:</p> <ol> <li>Create a fork of k0rdent/docs in your own GitHub workspace.</li> <li>Clone your fork to your desktop using git.</li> <li>Set origin remotes (fetch, push) to your fork.</li> <li>Set upstream remotes (fetch, push) to k0rdent/docs (main)</li> </ol> <p>Then, when getting ready to work:</p> <ol> <li>Update your local main from upstream and update main on your fork on GitHub:</li> </ol> <pre><code>git checkout main &amp;&amp; git fetch upstream &amp;&amp; git pull upstream main &amp;&amp; git push origin main\n</code></pre> <ol> <li>Create a new feature branch for each intended commit, to isolate your changes:</li> </ol> <pre><code>git checkout -b my-feature-branch main\n</code></pre> <ol> <li>As needed, particularly if you work on a change for a long time, periodically update your feature branch from upstream main to avoid later conflicts:</li> </ol> <pre><code>git checkout my-feature-branch &amp;&amp; git pull --rebase upstream main\n</code></pre> <ol> <li>When finished editing, add and commit changes:</li> </ol> <pre><code>git add . &amp;&amp; git commit -m \"My changes\"\n</code></pre> <ol> <li>Pre-emptively update once more before pushing:</li> </ol> <pre><code>git fetch upstream &amp;&amp; git rebase upstream/main\n</code></pre> <ol> <li>Push your feature branch to your fork on GitHub:</li> </ol> <pre><code>git push origin my-feature-branch\n</code></pre> <p>This will cause Git to display a confirmation message containing a link you can follow to submit a pull request on GitHub, from your feature branch to k0rdent/docs (main).</p> <ol> <li>Follow the link. Compare changes. Write an informative pull request title and extended description. And submit your Pull Request. Thank you in advance!</li> </ol>"},{"location":"k0rdent-documentation-contributors-guide/#install-mkdocs-to-view-k0rdent-docs-locally","title":"Install MkDocs to view k0rdent docs locally","text":"<p>When making docs contributions or edits, it can be very helpful to run MkDocs on your desktop, so you can see your additions and changes as they will be rendered online.</p> <p>MkDocs can be run on almost any contemporary Linux environment, desktop or server, inside a container, or on MacOS, Windows, or WSL. It requires recent (in k0rdent's case, 3.9+) versions of Python and pip. So this should work in whatever desktop environment you use: alongside Git and your Markdown editor or IDE.</p> <p>Note</p> <p> k0rdent's MkDocs environment requires Python 3.9+ and a corresponding version of pip. </p> <p>Assuming you have a machine (desktop, laptop, VM) for editing with recent (Python 3.9+) versions of Python and pip:</p> <ol> <li> <p>Begin by creating a fork of k0rdent/docs on GitHub, and clone your fork to your machine. See Making regular contributions - use Git on the desktop</p> </li> <li> <p>Switch into the cloned project directory on your machine:</p> </li> </ol> <pre><code>cd docs\n</code></pre> <ol> <li>Create a venv into which to install MkDocs and dependencies, saved in the /docs repo in a file called <code>requirements.txt</code>.</li> </ol> <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Your terminal should now show <code>(venv)</code> in the prompt. This will create a local /venv folder in your copy of the repo, which the    .gitignore already in your clone will instruct GitHub to ignore when you make commits and file pull requests. </p> <ol> <li> <p>Install MkDocs and dependencies in the venv, using <code>requirements.txt</code>.</p> <p>Note</p> <p> The file <code>requirements.txt</code> is in the toplevel containing folder (called <code>docs</code>) \u2014 not in the <code>/docs</code> subfolder within the toplevel directory.</p> </li> </ol> <pre><code>pip install -r requirements.txt \n</code></pre> <p>Once you've done this, you can execute MkDocs to dynamically build and serve docs locally on localhost (<code>http://127.0.0.1:8000</code>):</p> <pre><code>python3 mkdocs serve\n</code></pre> <p>Then just visit localhost (<code>http://127.0.0.1:8000</code>) in a browser to view and navigate the docs.</p>"},{"location":"k0rdent-documentation-contributors-guide/#run-mkdocs-in-a-container-to-view-k0rdent-docs-locally","title":"Run MkDocs in a container to view k0rdent docs locally","text":"<p>If you prefer, you can also run MkDocs in a container on your machine to view k0rdent docs locally. This is another way of keeping the MkDocs environment isolated and easily removable.</p> <p>We've put a Dockerfile in the toplevel directory of the repo to help with this. Here's what's in that Dockerfile:</p> <pre><code># Use the official MkDocs Material image\nFROM squidfunk/mkdocs-material:latest\n\n# Set the working directory\nWORKDIR /docs\n\n# Copy and install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy MkDocs files\nCOPY mkdocs.yml .\n\n# Expose the MkDocs server port\nEXPOSE 8000\n</code></pre> <p>As you can see, we're using the official MkDocs Material base image. The Dockerfile creates a working directory called /docs, installs requirements.txt with pip, copies in the mkdocs.yml configuration file, and exposes port 8000 so the container can be accessed as localhost on your machine.</p> <ol> <li>You'll need Docker installed on your machine. Follow instructions here</li> <li>Go to the toplevel directory of the repo, where the Dockerfile and mkdocs.yml reside.</li> <li>Build the container, tagging it with the name <code>mk-local</code>.</li> </ol> <pre><code>docker build -f Dockerfile -t mk-local\n</code></pre> <ol> <li>Then use git to (optionally create and/or) checkout the branch you'll be working in, since this is the branch you want MkDocs to render from.</li> </ol> <pre><code>git checkout -b my-working-branch       # create and checkout new working branch, or ...\ngit checkout my-working-branch          # checkout existing working branch\n</code></pre> <ol> <li>Open a new terminal (so you can stop the container within this terminal window by pressing CTRL-C later), then run the container like this:</li> </ol> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs mk-local\n</code></pre> <p>This ensures that the container is removed when you're finished with it, that it launches as interactive and with a TTY, that the container's port 8000 is mapped to port 8000 on your local machine (enabling you to view the container on localhost - <code>http://127.0.0.1:8000</code>), and it mounts your current directory (the toplevel of the repo) as /docs inside the container. This lets MkDocs see your local copy of the repo to render and serve it to the browser, and refreshes each time you make a change.</p> <p>Just visit the container on <code>http://127.0.0.1:8000</code> to view the docs.</p> <ol> <li>When you want to stop the container, press CTRL+C inside the terminal window used to start it. Alternatively, you can stop the container from any terminal as follows:</li> </ol> <pre><code>docker ps                       # find the container ID, then ...\ndocker stop &lt;container-id&gt;      # stop the container\n</code></pre> <p>Because we started the container with the --rm flag, it will be removed from Docker Engine when it stops. Restart by once again entering:</p> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs mk-local\n</code></pre>"},{"location":"k0rdent-documentation-style-guide/","title":"k0rdent documentation style guide","text":"<p>This page gives style guidance to our Markdown, and makes some general recommendations regarding writing style for docs. It is not a complete style guide. In most cases, Project k0rdent aspires to follow style recommendations adopted by the Kubernetes Project's SIG Docs group. Please see the Kubernetes Documentation Style Guide for details.</p> <p>If you are looking for technical recommendations about how to contribute to docs, these are found in the k0rdent Documentation Contributors Guide.</p>"},{"location":"k0rdent-documentation-style-guide/#our-markdown-style-theme-and-extensions","title":"Our Markdown style, theme, and extensions","text":"<p>k0rdent docs uses:</p> <ul> <li>The MkDocs static site generator.</li> <li>The Material theme for MkDocs.</li> </ul> <p>... plus other extensions and plugins that have less of an effect on how you write documentation. Versions of all components are in the file <code>requirements.txt</code> at toplevel in k0rdent/docs.</p> <p>At present, we are mostly utilizing pure CommonMark with certain Material theme extensions (see below), though we are experimenting with other plugins and extensions.</p>"},{"location":"k0rdent-documentation-style-guide/#commonmark-basics","title":"CommonMark basics","text":"<p>If you don't know Markdown already, CommonMark is a strongly-defined specification of Markdown with unambiguous syntax, plus a suite of tests to validate Markdown implementations against the specification. It's used in MkDocs and (with some iffyness around the edges) GitHub, which means our docs are readable when we use MkDocs to render them into static pages for display on the web, and also readable (in most cases) when you surf GitHub and look at the render there.</p> <p>Comfortingly, the CommonMark spec is very similar to whatever version(s) of Markdown you already know (e.g., GitHub-flavored Markdown) as well as allied structured-text specifications that share DNA with Markdown, like ReStructuredText (.rst), commonly used in open source docs environments.</p> <p>Some resources you may find useful:</p> <ul> <li>CommonMark Markdown Reference</li> <li>CommonMark Markdown Tutorial</li> </ul>"},{"location":"k0rdent-documentation-style-guide/#admonitions","title":"Admonitions","text":"<p>For the moment, we're coming up to speed with a modern solution for admonitions, but meanwhile are using the OG CommonMark form of 'notes are blockquotes with special headers, rendered prettily by MkDocs. Examples:</p> <pre><code>&gt;NOTE:\n&gt;This is a note.\n&gt;It can go over several lines as needed, and can contain `code`, **boldface**, _italics_, and other body-copy Markdown elements.\n\n&gt;INFO:\n&gt;This is an info box.\n&gt;It can go over several lines as needed, and can contain `code`, **boldface**, _italics_, and other body-copy Markdown elements.\n\n&gt;CAUTION:\n&gt;This is a cautionary admonishment.\n&gt;It can go over several lines as needed, and can contain `code`, **boldface**, _italics_, and other body-copy Markdown elements.\n\n&gt;WARNING:\n&gt;This is a warning.\n&gt;It can go over several lines as needed, and can contain `code`, **boldface**, _italics_, and other body-copy Markdown elements.\n</code></pre> <p>Note</p> <p> This is a note. It can go over several lines as needed, and can contain <code>code</code>, boldface, italics, and other body-copy Markdown elements.</p> <p>Info</p> <p> This is an info box. It can go over several lines as needed, and can contain <code>code</code>, boldface, italics, and other body-copy Markdown elements.</p> <p>Caution</p> <p> This is a cautionary admonishment. It can go over several lines as needed, and can contain <code>code</code>, boldface, italics, and other body-copy Markdown elements.</p> <p>Warning</p> <p> This is a warning. It can go over several lines as needed, and can contain <code>code</code>, boldface, italics, and other body-copy Markdown elements.</p>"},{"location":"known-issues-eks/","title":"EKS Machines Are not Created: ControlPlaneIsStable Preflight Check Failed","text":"<p>Related issue: KCM #907</p> <p>The deployment of the EKS cluster is stuck waiting for the machines to be provisioned. The <code>MachineDeployment</code> resource is showing the following conditions:</p> <pre><code>Type: MachineSetReady\nStatus: False\nReason: PreflightCheckFailed\nMessage: ekaz-eks-dev-eks-md: AWSManagedControlPlane kcm-system/ekaz-eks-dev-eks-cp is provisioning (\"ControlPlaneIsStable\" preflight check failed)\nType: Available\nStatus: False\nReason: WaitingForAvailableMachines\nMessage: ekaz-eks-dev-eks-md: Minimum availability requires 1 replicas, current 0 available\nType: Ready\nStatus: False\nReason: WaitingForAvailableMachines\nMessage: ekaz-eks-dev-eks-md: Minimum availability requires 1 replicas, current 0 available\n</code></pre> <p>As a result, the cluster was successfully created in EKS but no nodes are available.</p> <p>Workaround</p> <ol> <li> <p>Edit the <code>MachineDeployment</code> object: <pre><code>kubectl --kubeconfig &lt;management-kubeconfig&gt; edit MachineDeployment -n &lt;cluster-namespace&gt; &lt;cluster-name&gt;-md\n</code></pre></p> </li> <li> <p>Add <code>machineset.cluster.x-k8s.io/skip-preflight-checks: \"ControlPlaneIsStable\"</code> annotation to skip the <code>ControlPlaneIsStable</code> preflight check: <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: aws-eks-dev\n    meta.helm.sh/release-namespace: kcm-system\n    machineset.cluster.x-k8s.io/skip-preflight-checks: \"ControlPlaneIsStable\" # add new annotation\n  name: aws-eks-dev-md\n</code></pre></p> </li> <li> <p>Save and exit</p> </li> </ol>"},{"location":"quickstart-1-mgmt-node-and-cluster/","title":"QuckStart 1 - Set up Management Node and Cluster","text":"<p>Please review the Guide to QuickStarts for preliminaries. This QuickStart unit details setting up a single-VM environment for managing and interacting with k0rdent, and for hosting k0rdent components on a single-node local Kubernetes management cluster. Once k0rdent is installed on the management cluster, you can drive k0rdent by SSHing into the management node (kubectl is there and will be provisioned with the appropriate kubeconfig) or remotely by various means (for example, install the management cluster kubeconfig in Lens or another Kubernetes dashboard on your laptop, tunnel across from your own local kubectl, and so on).</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-a-single-node-k0s-cluster-locally-to-work-as-k0rdents-management-cluster","title":"Install a single-node k0s cluster locally to work as k0rdent's management cluster","text":"<p>k0s Kubernetes is a CNCF-certified minimal single-binary Kubernetes that installs with one command, and brings along its own CLI. We're using it to quickly set up a single-node management cluster on our manager node. However, k0rdent works on any CNCF-certified Kubernetes. If you choose to use something else, Team k0rdent would love to hear how you set things up to work for you.</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --single\nsudo k0s start\n</code></pre> <p>You can check to see if the cluster is working by leveraging kubectl (installed and configured automatically by k0s) via the k0s CLI:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-kubectl","title":"Install kubectl","text":"<p>k0s installs a compatible kubectl and makes it accessible via its own client. But to make your environment easier to configure, we advise installing kubectl the normal way on the manager node and using it to control the local k0s management cluster.</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#get-the-local-k0s-clusters-kubeconfig-for-kubectl","title":"Get the local k0s cluster's kubeconfig for kubectl","text":"<p>On startup, k0s stores the administrator's kubeconfig in a local directory, making it easy to access:</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>At this point, your newly-installed kubectl should be able to interoperate with the k0s management cluster with administrative privileges. Test to see that the cluster is ready (this usually takes about one minute):</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-helm","title":"Install Helm","text":"<p>The Helm Kubernetes package manager is used to install k0rdent services. We'll install Helm as follows:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Issuing these commands should produce something very much like the following output:</p> <pre><code>Downloading https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-k0rdent-into-the-k0s-management-cluster","title":"Install k0rdent into the k0s management cluster","text":"<p>Now we'll install k0rdent itself into the k0s management cluster:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.1.0 -n kcm-system --create-namespace\n</code></pre> <p>You'll see something like the following. Ignore the warnings, since this is an ephemeral, non-production, non-shared environment:</p> <pre><code>WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: ./KUBECONFIG\nWARNING: Kubernetes configuration file is world-readable. This is insecure. Location: ./KUBECONFIG\nPulled: ghcr.io/k0rdent/kcm/charts/kcm:0.1.0\nDigest: sha256:1f75e8e55c44d10381d7b539454c63b751f9a2ec6c663e2ab118d34c5a21087f\nNAME: kcm\nLAST DEPLOYED: Mon Dec  9 00:32:14 2024\nNAMESPACE: kcm-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>k0rdent startup takes several minutes.</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#check-that-k0rdent-cluster-management-pods-are-running","title":"Check that k0rdent cluster management pods are running","text":"<p>One fundamental k0rdent subsystem, k0rdent Cluster Manager (KCM), handles cluster lifecycle management on clouds and infrastructures: for example, it helps you configure and compose clusters and manages infrastructure via Cluster API (CAPI). Before continuing, check that the KCM pods are ready:</p> <pre><code>kubectl get pods -n kcm-system   # check pods in the kcm-system namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre> <p>Pods reported in states other than Running should become ready momentarily.</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#check-that-the-projectsveltos-pods-are-running","title":"Check that the projectsveltos pods are running","text":"<p>The other fundamental k0rdent subsystem, k0rdent State Manager (KSM), handles services configuration and lifecycle management on clusters. This utilizes the projectsveltos Kubernetes Add-On Controller and other open source projects. Before continuing, check that the ksm pods are ready:</p> <pre><code>kubectl get pods -n projectsveltos   # check pods in the projectsveltos namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre> <p>If you have fewer pods than shown above, just wait 5 minutes or so for all the pods to reconcile and start running.</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-k0rdent-itself-is-ready","title":"Verify that k0rdent itself is ready","text":"<p>The actual measure of whether k0rdent is ready is the state of the <code>Management</code> object. To check, issue this command:</p> <p><pre><code>kubectl get Management -n kcm-system\n</code></pre> <pre><code>NAME   READY   RELEASE     AGE\nkcm    True    kcm-0-1-0   9m\n</code></pre></p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-provider-and-related-templates-are-available","title":"Verify that KCM provider and related templates are available","text":"<p>k0rdent KCM leverages CAPI to manage Kubernetes cluster assembly and host infrastructure. CAPI requires infrastructure providers for different clouds and infrastructure types. These are delivered and referenced within k0rdent using templates, instantiated in the management cluster as objects. Before continuing, verify that the default provider template objects are installed and verified. Other templates are also stored as provider templates in this namespace (for example, the templates that determine setup of KCM itself and other parts of the k0rdent system, such as projectsveltos, which is a component of k0rdent Service Manager (KSM, see below)) as well as the k0smotron subsystem, which enables creation and lifecycle management of managed clusters that use Kubernetes-hosted control planes (such as control planes as pods):</p> <pre><code>kubectl get providertemplate -n kcm-system   # list providertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME                                   VALID\ncluster-api-0-1-0                      true\ncluster-api-provider-aws-0-1-0         true\ncluster-api-provider-azure-0-1-0       true\ncluster-api-provider-openstack-0-1-0   true\ncluster-api-provider-vsphere-0-1-0     true\nk0smotron-0-1-0                        true\nkcm-0-1-0                              true\nprojectsveltos-0-45-0                  true\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-clustertemplate-objects-are-available","title":"Verify that KCM ClusterTemplate objects are available","text":"<p>CAPI also requires control plane and bootstrap (worker node) providers to construct and/or manage different Kubernetes cluster distros and variants. Again, these providers are delivered and referenced within k0rdent using templates, instantiated in the management cluster as <code>ClusterTemplate</code> objects. Before continuing, verify that default ClusterTemplate objects are installed and valid:</p> <pre><code>kubectl get clustertemplate -n kcm-system   # list clustertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME                            VALID\nadopted-cluster-0-1-0           true\naws-eks-0-1-0                   true\naws-hosted-cp-0-1-0             true\naws-standalone-cp-0-1-0         true\nazure-aks-0-1-0                 true\nazure-hosted-cp-0-1-0           true\nazure-standalone-cp-0-1-0       true\nopenstack-standalone-cp-0-1-0   true\nvsphere-hosted-cp-0-1-0         true\nvsphere-standalone-cp-0-1-0     true\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-ksm-servicetemplate-objects-are-available","title":"Verify that KSM ServiceTemplate objects are available","text":"<p>k0rdent Service Manager (KSM) uses Service Templates to lifecycle manage services and applications installed on clusters. These, too, are represented as declarative templates, instantiated as <code>ServiceTemplate</code> objects. Check that default <code>ServiceTemplate</code> objects have been created and validated:</p> <pre><code>kubectl get servicetemplate -n kcm-system   # list servicetemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#next-steps","title":"Next steps","text":"<p>Your QuickStart management node is now complete, and k0rdent is installed and operational. Next, it's time to select AWS or Azure as an environment for hosting managed clusters.</p>"},{"location":"quickstart-2-aws/","title":"QuickStart 2 - AWS target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Amazon Web Services (AWS) and deploying our first child cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an AWS account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done our Azure QuickStart (QuickStart 2 - Azure target environment) you can  use the same management cluster, continuing here with steps to add the ability to manage clusters on AWS. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to AWS (for example, it could be on an Azure virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Note</p> <p> CLOUD SECURITY 101: k0rdent requires some but not all permissions to manage AWS \u2014 doing so via the CAPA (ClusterAPI for AWS) provider.</p> <p>Because k0rdent doesn't require all permissions, a best practice for using k0rdent with AWS (this pattern is repeated with other clouds and infrastructures) is to create a new 'k0rdent user' on your account with the particular permissions k0rdent and CAPA require. In this section, we'll create and configure IAM for that user, and perform other steps to make that k0rdent user's credentials accessible to k0rdent in the management node.</p> <p>Note</p> <p> If you're working on a shared AWS account, please ensure that the k0rdent user is not already set up before creating a new one.</p> <p>Creating a k0rdent user with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart-2-aws/#install-the-aws-cli","title":"Install the AWS CLI","text":"<p>We'll use the AWS CLI to create and set IAM permissions for the k0rdent user, so we'll install it on our management node:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre>"},{"location":"quickstart-2-aws/#install-clusterawsadm","title":"Install clusterawsadm","text":"<p>k0rdent uses Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. This QuickStart leverages <code>clusterawsadm</code>, a CLI tool created by the CAPA project that helps with AWS-specific tasks like IAM role, policy, and credential configuration.</p> <p>To install <code>clusterawsadm</code> on Ubuntu on x86 hardware:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre>"},{"location":"quickstart-2-aws/#export-your-administrative-credentials","title":"Export your administrative credentials","text":"<p>You should have these already, preserved somewhere safe. If not, you can visit the AWS webUI (Access Management &gt; Users) and generate new credentials (Access Key ID, Secret Access Key, and Session Token (if using multi-factor authentication)).</p> <p>Export the credentials to the management node environment:</p> <pre><code>export AWS_REGION=EXAMPLE_AWS_REGION\nexport AWS_ACCESS_KEY_ID=EXAMPLE_ACCESS_KEY_ID\nexport AWS_SECRET_ACCESS_KEY=EXAMPLE_SECRET_ACCESS_KEY\nexport AWS_SESSION_TOKEN=EXAMPLE_SESSION_TOKEN # Optional. If you are using Multi-Factor Auth.\n</code></pre> <p>These credentials will be used both by the AWS CLI (to create your k0rdent user) and by <code>clusterawsadm</code> (to create a CloudFormation template used by CAPA within k0rdent).</p>"},{"location":"quickstart-2-aws/#check-for-available-ips","title":"Check for available IPs","text":"<p>Because k0rdent has 3 availablilty zone NAT gateways, each cluster needs 3 public IPs. Unfortunately, the default <code>EC2-VPC Elastic IPs</code> quota per region is 5, so while you likely won't have issues with a first cluster, if you try to deplay a  second to the same region, you are likely to run into issues.  </p> <p>You can determine how many elastic IPs are available from the command line:</p> <p><pre><code>LIMIT=$(aws ec2 describe-account-attributes --attribute-names vpc-max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)\nUSED=$(aws ec2 describe-addresses --query 'Addresses[*].PublicIp' --output text | wc -w)\nAVAILABLE=$((LIMIT - USED))\necho \"Available Public IPs: $AVAILABLE\"\n</code></pre> <pre><code>Available Public IPs: 5\n</code></pre></p> <p>If you have less than 3 available public IPs, you can request an increase in your quota:</p> <pre><code>aws service-quotas request-service-quota-increase \\\n    --service-code ec2 \\\n    --quota-code L-0263D0A3 \\\n    --desired-value 20\n</code></pre> <p>You can check on the status of your request:</p> <p><pre><code>aws service-quotas list-requested-service-quota-change-history \\\n    --service-code ec2\n</code></pre> <pre><code>{\n    \"RequestedQuotas\": [\n        {\n            \"Id\": \"EXAMPLE_ACCESS_KEY_ID\",\n            \"ServiceCode\": \"ec2\",\n            \"ServiceName\": \"Amazon Elastic Compute Cloud (Amazon EC2)\",\n            \"QuotaCode\": \"L-0263D0A3\",\n            \"QuotaName\": \"EC2-VPC Elastic IPs\",\n            \"DesiredValue\": 20.0,\n            \"Status\": \"PENDING\",\n            \"Created\": \"2025-02-09T02:27:01.573000-05:00\",\n            \"LastUpdated\": \"2025-02-09T02:27:01.956000-05:00\",\n            \"Requester\": \"{\\\"accountId\\\":\\\"EXAMPLE_ACCESS_KEY_ID\\\",\\\"callerArn\\\":\\\"arn:aws:iam::EXAMPLE_ACCESS_KEY_ID:user/nchase\\\"}\",\n            \"QuotaArn\": \"arn:aws:servicequotas:EXAMPLE_AWS_REGION:EXAMPLE_ACCESS_KEY_ID:ec2/L-0263D0A3\",\n            \"GlobalQuota\": false,\n            \"Unit\": \"None\",\n            \"QuotaRequestedAtLevel\": \"ACCOUNT\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"quickstart-2-aws/#create-the-k0rdent-aws-user","title":"Create the k0rdent AWS user","text":"<p>Now we can use the AWS CLI to create a new k0rdent user:</p> <p><pre><code> aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <pre><code>{\n    \"User\": {\n        \"Path\": \"/\",\n        \"UserName\": \"k0rdentQuickstart\",\n        \"UserId\": \"EXAMPLE_USER_ID\",\n        \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentQuickstart\",\n        \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n    }\n}\n</code></pre></p>"},{"location":"quickstart-2-aws/#configure-aws-iam-for-k0rdent","title":"Configure AWS IAM for k0rdent","text":"<p>Before k0rdent CAPI can manage resources on AWS, you need to use <code>clusterawsadm</code> to create a bootstrap CloudFormation stack with additional IAM policies and a service account. You do this under the administrative account credentials you earlier exported to the management node environment:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre>"},{"location":"quickstart-2-aws/#attach-iam-policies-to-the-k0rdent-user","title":"Attach IAM policies to the k0rdent user","text":"<p>Next, we'll attach appropriate policies to the k0rdent user. These are:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> <p>We use the AWS CLI to attach them. To do this, you will need to extract the Amazon Resource Name (ARN) for the newly-created user:</p> <pre><code>AWS_ARN_ID=$(aws iam get-user --user-name k0rdentQuickstart --query 'User.Arn' --output text | grep -oP '\\d{12}')\necho $AWS_ARN_ID\n</code></pre> <p>Assemble and execute the following commands to implement the required policies:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> <p>We can check to see that policies were assigned:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> And you'll see output that looks like this (this is non-valid example text):</p> <pre><code>{\n    \"Policies\": [\n        {\n            \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n        },\n        {\n            \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        }\n    ]\n}\n</code></pre>"},{"location":"quickstart-2-aws/#create-aws-credentials-for-the-k0rdent-user","title":"Create AWS credentials for the k0rdent user","text":"<p>In the AWS IAM Console, you can now create the Access Key ID and Secret Access Key for the k0rdent user and download them. You can also do this via the AWS CLI:</p> <pre><code>aws iam create-access-key --user-name k0rdentQuickstart\n</code></pre> <p>You should see something like this. It's important to save these credentials securely somewhere other than the management node, since the management node may end up being ephemeral. Again, this is non-valid example text:</p> <pre><code>{\n    \"AccessKey\": {\n        \"UserName\": \"k0rdentQuickstart\",\n        \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n        \"Status\": \"Active\",\n        \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n        \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstart-2-aws/#create-iam-credentials-secret-on-the-management-cluster","title":"Create IAM credentials secret on the management cluster","text":"<p>Next, we create a <code>Secret</code> containing credentials for the k0rdent user and apply this to the management cluster running k0rdent, in the <code>kcm-system</code> namespace (important: if you use another namespace, k0rdent will be unable to read the credentials). To do this, create the following YAML in a file called <code>aws-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nstringData:\n  AccessKeyID: \"EXAMPLE_ACCESS_KEY_ID\"\n  SecretAccessKey: \"EXAMPLE_SECRET_ACCESS_KEY\"\n</code></pre> <p>Remember: the Access Key ID and Secret Access Key are the ones you generated for the k0rdent user, <code>k0rdentQuickStart</code>.</p> <p>Then apply this YAML to the management cluster as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#create-the-awsclusterstaticidentity-object","title":"Create the AWSClusterStaticIdentity object","text":"<p>Next, we need to create an <code>AWSClusterStaticIdentity</code> object that uses the secret.</p> <p>To do this, create a YAML file named <code>aws-cluster-identity</code> as follows:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Note that the <code>spec.secretRef</code> is the same as the <code>metadata.name</code> of the secret we just created.</p> <p>Create the object as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#create-the-k0rdent-cluster-manager-credential-object","title":"Create the k0rdent Cluster Manager credential object","text":"<p>Now we create the k0rdent Cluster Manager credential object. As in prior steps, create a YAML file called <code>aws-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> <p>Note that <code>.spec.identityRef.kind</code> must be <code>AWSClusterStaticIdentity</code> and <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>AWSClusterStaticIdentity</code> object.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#create-the-k0rdent-cluster-identity-resource-template-configmap","title":"Create the k0rdent Cluster Identity resource template ConfigMap","text":"<p>Now we create the k0rdent Cluster Identity resource template <code>ConfigMap</code>. As in prior steps, create a YAML file called <code>aws-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\n</code></pre> <p>Note that <code>ConfigMap</code> is empty, this is expected, we don't need to template any object inside child cluster(s), but we can use that object in the future if need arises.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-resource-template.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage AWS. To create a cluster, begin by listing the available <code>ClusterTemplate</code> objects provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>aws-standalone-cp-0-1-0</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-1-0           true\nkcm-system   aws-eks-0-1-0                   true\nkcm-system   aws-hosted-cp-0-1-0             true\nkcm-system   aws-standalone-cp-0-1-0         true\nkcm-system   azure-aks-0-1-0                 true\nkcm-system   azure-hosted-cp-0-1-0           true\nkcm-system   azure-standalone-cp-0-1-0       true\nkcm-system   openstack-standalone-cp-0-1-0   true\nkcm-system   vsphere-hosted-cp-0-1-0         true\nkcm-system   vsphere-standalone-cp-0-1-0     true\n</code></pre>"},{"location":"quickstart-2-aws/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-aws-clusterdeployment1.yaml</code>. We'll use this to create a <code>ClusterDeployment</code> object in k0rdent, representing the deployed cluster. The <code>ClusterDeployment</code> identifies for k0rdent the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under (that of your k0rdent user), plus the region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-1-0\n  credential: aws-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre>"},{"location":"quickstart-2-aws/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the <code>ClusterDeployment</code> to deploy the cluster","text":"<p>Finally, we'll apply the <code>ClusterDeployment</code> YAML (<code>my-aws-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can watch the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <p>In a short while, you'll see output such as:</p> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart-2-aws/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's <code>kubeconfig</code>:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1.kubeconfig\n</code></pre> <p>And you can use the <code>kubeconfig</code> to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-aws-clusterdeployment1.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstart-2-aws/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                        READY   STATUS\nkcm-system   my-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart-2-aws/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code>:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-aws-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstart-2-aws/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features.</p>"},{"location":"quickstart-2-azure/","title":"QuickStart 2 - Azure target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Azure, and deploying a child cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an Azure account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done our AWS QuickStart (QuickStart 2 - AWS target environment) you can use the same management cluster, continuing here with steps to add the ability to manage clusters on Azure. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to Azure (for example, it could be on an AWS EC2 virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Note</p> <p> Cloud Security 101: k0rdent requires some but not all permissions to manage Azure resources \u2014 doing so via the CAPZ (ClusterAPI for Azure) provider. </p> <p>A best practice for using k0rdent with Azure (this pattern is repeated with other clouds and infrastructures) is to create a new k0rdent Azure Cluster Identity and Service Principal (SP) on your account with the particular permissions k0rdent and CAPZ require.In this section, we'll create and configure those identity abstractions, and perform other steps to make required credentials accessible to k0rdent in the management node.</p> <p>Note</p> <p> If you're working on a shared Azure account, please ensure that the Azure Cluster Identity and Service Principal are not already set up before creating new abstractions.</p> <p>Creating user identity abstractions with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart-2-azure/#install-the-azure-cli-az","title":"Install the Azure CLI (az)","text":"<p>The Azure CLI (az) is required to interact with Azure resources. Install it according to instructions in How to install the Azure CLI. For Linux/Debian (i.e., Ubuntu Server), it's one command:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre>"},{"location":"quickstart-2-azure/#log-in-with-azure-cli","title":"Log in with Azure CLI","text":"<p>Run the az login command to authenticate your session with Azure.</p> <pre><code>az login\n</code></pre>"},{"location":"quickstart-2-azure/#register-resource-providers","title":"Register resource providers","text":"<p>Azure Resource Manager uses resource providers to manage resources of all different kinds, and required providers must be registered with an Azure account before k0rdent and CAPZ can work with them.</p> <p>You can list resources registered with your account using Azure CLI:</p> <pre><code>az provider list --query \"[?registrationState=='Registered']\" --output table\n</code></pre> <p>And see a listing like this:</p> <pre><code>Namespace                             RegistrationState\n-----------------------------------   -----------------\nMicrosoft.Compute                     Registered\nMicrosoft.Network                     Registered\n</code></pre> <p>You can then select from the commands below (or enter all of them) to register any unregistered resources that k0rdent and CAPZ require:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre>"},{"location":"quickstart-2-azure/#get-your-azure-subscription-id","title":"Get your Azure Subscription ID","text":"<p>Use the following command to list Azure subscriptions and their IDs:</p> <pre><code>az account list -o table\n</code></pre> <p>The output will look like this:</p> <pre><code>Name                     SubscriptionId                    TenantId\n-----------------------  -------------------------------   -----------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID   TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre> <p>The Subcription ID is in the second column.</p>"},{"location":"quickstart-2-azure/#create-a-service-principal-for-k0rdent","title":"Create a Service Principal for k0rdent","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. To create it, run the following command with the Azure CLI, replacing  with the ID you copied earlier. <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <p>You'll see output that resembles what's below:</p> <pre><code>{\n \"appId\": \"SP_APP_ID_SP_APP_ID\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"SP_PASSWORD_SP_PASSWORD\",\n \"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> <p>Capture this output and secure the values it contains. We'll need several of these in a moment.</p>"},{"location":"quickstart-2-azure/#create-a-secret-object-with-the-azure-credentials","title":"Create a Secret object with the Azure credentials","text":"<p>In this quickstart we're assuming a self-managed Azure clusters (non-AKS) so create a <code>Secret</code> object that stores the <code>clientSecret</code> (password) from the Service Principal. Create a YAML file called <code>azure-cluster-identity-secret.yaml</code>, as follows, inserting the password for the Service Principal (represented by the placeholder <code>SP_PASSWORD_SP_PASSWORD</code> above):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clientSecret: SP_PASSWORD_SP_PASSWORD # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Apply the YAML to the k0rdent management cluster using the following command:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre>"},{"location":"quickstart-2-azure/#create-the-azureclusteridentity-object","title":"Create the AzureClusterIdentity Object","text":"<p>This object defines the credentials k0rdent and CAPZ will use to manage Azure resources. It references the <code>Secret</code> you just created above.</p> <p>Create a YAML file called <code>azure-cluster-identity.yaml</code>. Make sure that <code>.spec.clientSecret.name</code> matches the <code>metadata.name</code> in the file you created above.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  name: azure-cluster-identity\n  namespace: kcm-system\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  allowedNamespaces: {}\n  clientID: SP_APP_ID_SP_APP_ID # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: SP_TENANT_SP_TENANT # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre>"},{"location":"quickstart-2-azure/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <p>Note that for non-AKS clusters <code>.spec.kind</code> must be <code>AzureClusterIdentity</code>, and <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>AzureClusterIdentity</code> object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre>"},{"location":"quickstart-2-azure/#create-the-configmap-resource-template-object","title":"Create the <code>ConfigMap</code> resource-template Object","text":"<p>Create a YAML with the specification of our resource-template and save it as <code>azure-cluster-identity-resource-template.yaml</code></p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: azure-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    {{- $subnetName := \"\" -}}\n    {{- $securityGroupName := \"\" -}}\n    {{- $routeTableName := \"\" -}}\n    {{- range $cluster.spec.networkSpec.subnets -}}\n      {{- if eq .role \"node\" -}}\n        {{- $subnetName = .name -}}\n        {{- $securityGroupName = .securityGroup.name -}}\n        {{- $routeTableName = .routeTable.name -}}\n        {{- break -}}\n      {{- end -}}\n    {{- end -}}\n    {{- $cloudConfig := dict\n      \"aadClientId\" $identity.spec.clientID\n      \"aadClientSecret\" (index $secret.data \"clientSecret\" | b64dec)\n      \"cloud\" $cluster.spec.azureEnvironment\n      \"loadBalancerName\" \"\"\n      \"loadBalancerSku\" \"Standard\"\n      \"location\" $cluster.spec.location\n      \"maximumLoadBalancerRuleCount\" 250\n      \"resourceGroup\" $cluster.spec.resourceGroup\n      \"routeTableName\" $routeTableName\n      \"securityGroupName\" $securityGroupName\n      \"securityGroupResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n      \"subnetName\" $subnetName\n      \"subscriptionId\" $cluster.spec.subscriptionID\n      \"tenantId\" $identity.spec.tenantID\n      \"useInstanceMetadata\" true\n      \"useManagedIdentityExtension\" false\n      \"vmType\" \"vmss\"\n      \"vnetName\" $cluster.spec.networkSpec.vnet.name\n      \"vnetResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n    -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: azure-cloud-provider\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-config: {{ $cloudConfig | toJson | b64enc }}\n</code></pre> <p>Object name needs to be exactly <code>azure-cluster-identity-resource-template.yaml</code>, <code>AzureClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-resource-template.yaml\n</code></pre> <pre><code>configmap/azure-cluster-identity-resource-template created\n</code></pre></p>"},{"location":"quickstart-2-azure/#find-your-locationregion","title":"Find your location/region","text":"<p>To determine where to deploy your cluster, you may wish to begin by listing your Azure location/regions:</p> <pre><code>az account list-locations -o table\n</code></pre> <p>You'll see output like this:</p> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n. . .\n</code></pre> <p>What you'll need to insert in your ClusterDeployment is the name (center column) of the region you wish to deploy to.</p>"},{"location":"quickstart-2-azure/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage Azure. To create a cluster, begin by listing the available ClusterTemplates provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the example below, that's <code>azure-standalone-cp-0-1-0</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-1-0           true\nkcm-system   aws-eks-0-1-0                   true\nkcm-system   aws-hosted-cp-0-1-0             true\nkcm-system   aws-standalone-cp-0-1-0         true\nkcm-system   azure-aks-0-1-0                 true\nkcm-system   azure-hosted-cp-0-1-0           true\nkcm-system   azure-standalone-cp-0-1-0       true\nkcm-system   openstack-standalone-cp-0-1-0   true\nkcm-system   vsphere-hosted-cp-0-1-0         true\nkcm-system   vsphere-standalone-cp-0-1-0     true\n</code></pre>"},{"location":"quickstart-2-azure/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-azure-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in k0rdent, representing the deployed cluster. The <code>ClusterDeployment</code> identifies for k0rdent the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under, plus the location/region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-1-0 # name of the clustertemplate\n  credential: azure-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    location: \"AZURE_LOCATION\" # Select your desired Azure Location\n    subscriptionID: SUBSCRIPTION_ID_SUBSCRIPTION_ID # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre>"},{"location":"quickstart-2-azure/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-azure-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre>"},{"location":"quickstart-2-azure/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstart-2-azure/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre></p>"},{"location":"quickstart-2-azure/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre></p>"},{"location":"quickstart-2-azure/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"template-aws/","title":"AWS template parameters","text":""},{"location":"template-aws/#aws-ami","title":"AWS AMI","text":"<p>For AWS, by default k0rdent looks up the AMI ID automatically, using the latest Amazon Linux 2 image.</p> <p>You can override lookup parameters to search your desired image automatically or you can use a specific AMI ID directly. If both the AMI ID and lookup parameters are defined, the AMI ID will have higher precedence.</p>"},{"location":"template-aws/#image-lookup","title":"Image lookup","text":"<p>To configure automatic AMI lookup, k0rdent uses three parameters:</p> <ul> <li> <p><code>.imageLookup.format</code> - Used directly as a value for the <code>name</code> filter (see the describe-images filters). This field supports substitutions for <code>{{.BaseOS}}</code> and <code>{{.K8sVersion}}</code> with the base OS and kubernetes version, respectively.</p> </li> <li> <p><code>.imageLookup.org</code> - The AWS org ID that will be used as value for the <code>owner-id</code> filter.</p> </li> <li> <p><code>.imageLookup.baseOS</code> - The string to be used as a value for the <code>{{.BaseOS}}</code> substitution in the <code>.imageLookup.format</code> string.</p> </li> </ul>"},{"location":"template-aws/#ami-id","title":"AMI ID","text":"<p>The AMI ID can be directly used in the <code>.amiID</code> parameter.</p>"},{"location":"template-aws/#capa-prebuilt-amis","title":"CAPA prebuilt AMIs","text":"<p>Use <code>clusterawsadm</code> to get available AMIs to create a <code>ClusterDeployment</code>:</p> <pre><code>clusterawsadm ami list\n</code></pre> <p>For details, see Pre-built Kubernetes AMIs.</p>"},{"location":"template-aws/#ssh-access-to-cluster-nodes","title":"SSH access to cluster nodes","text":"<p>To access nodes using SSH you'll need to do two things:</p> <ul> <li>Add an SSH key added in the region where you want to deploy the cluster</li> <li>Enable Bastion host is enabled</li> </ul>"},{"location":"template-aws/#ssh-keys","title":"SSH keys","text":"<p>Only one SSH key is supported and it should be added in AWS prior to creating the <code>ClusterDeployment</code> object. The name of the key should then be placed under the <code>.spec.config.sshKeyName</code>.</p> <p>The same SSH key will be used for all machines and a bastion host, or jump box. The bastion host is used as a single entry point that provides access to the rest of the cluster, enabling you to more tightly control access.</p> <p>To enable the bastion, set the <code>.spec.config.bastion.enabled</code> option in the <code>ClusterDeployment</code> object to <code>true</code>.</p> <p>You can get a full list of the bastion configuration options in the CAPA docs.</p> <p>The resulting <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-standalone-cp-0-1-0\n  credential: aws-cred\n  config:\n    clusterLabels: {}\n    sshKeyName: foobar\n    bastion:\n      enabled: true\n...\n</code></pre>"},{"location":"template-aws/#eks-templates","title":"EKS templates","text":"<p>Warning</p> <p> When deploying an EKS cluster please note that additional steps may be needed for proper VPC removal.</p> <p>Warning</p> <p> You may encounter an issue where EKS machines are not created due to the <code>ControlPlaneIsStable</code> preflight check failure during EKS cluster deployment. Please follow this  instruction to apply the workaround.</p> <p>EKS templates use parameters similar to AWS and the resulting EKS <code>ClusterDeployment</code> looks something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-eks-0-1-0\n  credential: aws-cred\n  config:\n    clusterLabels: {}\n    sshKeyName: foobar\n    region: ${AWS_REGION}\n    workersNumber: 1\n...\n</code></pre>"},{"location":"template-aws/#aws-hosted-control-plane-deployment","title":"AWS Hosted control plane deployment","text":"<p>This section covers setting up for a k0smotron hosted control plane on AWS.</p>"},{"location":"template-aws/#prerequisites","title":"Prerequisites","text":"<p>Before starting you must have:</p> <ul> <li>A management Kubernetes cluster (v1.28+) deployed on AWS with kcm installed on it</li> <li>A default default storage class configured on the management cluster</li> <li>A VPC ID for the worker nodes</li> <li>A Subnet ID which will be used along with AZ information</li> <li>An AMI ID which will be used to deploy worker nodes</li> </ul> <p>Keep in mind that all control plane components for all cluster deployments will reside in the management cluster.</p>"},{"location":"template-aws/#networking","title":"Networking","text":"<p>The networking resources needed for a cluster deployment in AWS can be reused with a management cluster.</p> <p>If you deployed your AWS Kubernetes cluster using Cluster API Provider AWS (CAPA) you can get all the necessary data with the commands below, or use the template found below in the kcm ClusterDeployment manifest generation section.</p> <p>VPC ID</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre> <p>Subnet ID</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre> <p>Availability zone</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre> <p>Security group <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre></p> <p>AMI id</p> <pre><code>    kubectl get awsmachinetemplate &lt;cluster-name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre> <p>If you want to use different VPCs/regions for your management or managed clusters you should setup additional connectivity rules like VPC peering.</p> <p>If using the <code>aws-standalone-cp</code> template to deploy a hosted cluster,  a <code>t3.large</code> or larger instance type is recommended, as the <code>kcm-controller</code> and other provider controllers will need a large amount of resources to run.</p>"},{"location":"template-aws/#kcm-clusterdeployment-manifest","title":"kcm ClusterDeployment manifest","text":"<p>With all the collected data your <code>ClusterDeployment</code> manifest will look similar to:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted-cp\nspec:\n  template: aws-hosted-cp-0-1-0\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    vpcID: vpc-0a000000000000000\n    region: us-west-1\n    publicIP: true\n    subnets:\n      - id: subnet-0aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: true\n        natGatewayID: xxxxxx\n        routeTableId: xxxxxx\n      - id: subnet-1aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: false\n        routeTableId: xxxxxx\n    instanceType: t3.medium\n    securityGroupIDs:\n      - sg-0e000000000000000\n</code></pre> <p>Note</p> <p> In this example we're using the <code>us-west-1</code> region, but you should use the region of your VPC.</p>"},{"location":"template-aws/#kcm-clusterdeployment-manifest-generation","title":"kcm ClusterDeployment manifest generation","text":"<p>Grab the following <code>ClusterDeployment</code> manifest template and save it to a file named <code>clusterdeployment.yaml.tpl</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp-0-1-0\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n    {{- range $subnet := .spec.network.subnets }}\n      - id: \"{{ $subnet.resourceID }}\"\n        availabilityZone: \"{{ $subnet.availabilityZone }}\"\n        isPublic: {{ $subnet.isPublic }}\n        {{- if $subnet.isPublic }}\n        natGatewayId: \"{{ $subnet.natGatewayId }}\"\n        {{- end }}\n        routeTableId: \"{{ $subnet.routeTableId }}\"\n        zoneType: \"{{ $subnet.zoneType }}\"\n    {{- end }}\n    instanceType: t3.medium\n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre> <p>Then run the following command to create the <code>clusterdeployment.yaml</code>:</p> <pre><code>kubectl get awscluster cluster -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre>"},{"location":"template-aws/#deployment-tips","title":"Deployment Tips","text":"<ul> <li>Ensure kcm templates and the controller image are somewhere public and   fetchable.</li> <li>For installing the kcm charts and templates from a custom repository, load   the <code>kubeconfig</code> from the cluster and run the commands:</li> </ul> <pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/k0rdent/kcm/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/k0rdent/kcm/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> <ul> <li>The infrastructure will need to manually be marked <code>Ready</code> to get the   <code>MachineDeployment</code> to scale up.  You can patch the <code>AWSCluster</code> kind using   the command:</li> </ul> <pre><code>KUBECONFIG=kubeconfig kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch 'status: {ready: true}' -n kcm-system\n</code></pre> <p>For additional information on why this is required click here.</p>"},{"location":"template-azure/","title":"Azure machine parameters","text":""},{"location":"template-azure/#ssh","title":"SSH","text":"<p>The SSH public key can be passed to <code>.spec.config.sshPublicKey</code>  parameter (in the case of a hosted control plane) or <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (in the case of a standalone control plane) of the <code>ClusterDeployment</code> object.</p> <p>It should be encoded in base64 format.</p>"},{"location":"template-azure/#vm-size","title":"VM size","text":"<p>Azure supports various VM sizes which can be retrieved with the following command:</p> <pre><code>az vm list-sizes --location \"&lt;location&gt;\" -o table\n</code></pre> <p>Then desired VM size could be passed to the:</p> <ul> <li><code>.spec.config.vmSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.vmSize</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.vmSize</code> - for worker nodes in the standalone deployment.</li> </ul> <p>Example: Standard_A4_v2</p>"},{"location":"template-azure/#root-volume-size","title":"Root Volume size","text":"<p>Root volume size of the VM (in GB) can be changed through the following parameters:</p> <ul> <li><code>.spec.config.rootVolumeSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.rootVolumeSize</code> - for control plane nodes in the   standalone deployment.</li> <li><code>.spec.config.worker.rootVolumeSize</code> - for worker nodes in the standalone   deployment.</li> </ul> <p>Default value: 30</p> <p>Please note that this value can't be less than size of the root volume  defined in your image.</p>"},{"location":"template-azure/#vm-image","title":"VM Image","text":"<p>You can define the image which will be used for your machine using the following parameters:</p> <p>*<code>.spec.config.image</code> - for hosted CP deployment. * <code>.spec.config.controlPlane.image</code> - for control plane nodes in the standalone   deployment. * <code>.spec.config.worker.image</code> - for worker nodes in the standalone deployment.</p> <p>There are multiple self-excluding ways to define the image source (for example Azure Compute Gallery, Azure Marketplace, and so on).</p> <p>Detailed information regarding image can be found in CAPZ documentation</p> <p>By default, the latest official CAPZ Ubuntu based image is used.</p>"},{"location":"template-azure/#azure-hosted-control-plane-k0smotron-deployment","title":"Azure Hosted control plane (k0smotron) deployment","text":""},{"location":"template-azure/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on Azure with kcm installed     on it</li> <li>Default storage class configured on the management cluster</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"template-azure/#pre-existing-resources","title":"Pre-existing resources","text":"<p>Certain resources will not be created automatically in a hosted control plane scenario, so you must create them in advance and provide them to  the <code>ClusterDeployment</code> object. You can reuse these resources with management cluster as described below.</p> <p>If you deployed your Azure Kubernetes cluster using Cluster API Provider Azure (CAPZ) you can obtain all the necessary data with the commands below:</p> <p>Location</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.location}}'\n</code></pre> <p>Subscription ID</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre> <p>Resource group</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre> <p>vnet name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre> <p>Subnet name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre> <p>Route table name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre> <p>Security group name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre>"},{"location":"template-azure/#kcm-clusterdeployment-manifest","title":"kcm ClusterDeployment manifest","text":"<p>With all the collected data your <code>ClusterDeployment</code> manifest will look similar to this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-1-0\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"westus\"\n    subscriptionID: SUBSCRIPTION_ID_SUBSCRIPTION_ID\n    vmSize: Standard_A4_v2\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n</code></pre> <p>To simplify creation of the <code>ClusterDeployment</code> object you can use the template below:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-1-0\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n</code></pre> <p>Then you can render it using the command:</p> <pre><code>kubectl get azurecluster &lt;management-cluster-name&gt; -o go-template=\"$(cat template.yaml)\"\n</code></pre>"},{"location":"template-azure/#cluster-creation","title":"Cluster creation","text":"<p>After applying the <code>ClusterDeployment</code> object you must manually set the status of the <code>AzureCluster</code> object due to limitations in k0smotron (see k0sproject/k0smotron#668).</p> <p>Execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --subresource status --patch 'status: {ready: true}'\n</code></pre>"},{"location":"template-azure/#important-notes-on-the-cluster-deletion","title":"Important notes on the cluster deletion","text":"<p>Because of the aforementioned limitation you also need to take manual steps in order to properly delete an Azurecluster.</p> <p>Before removing the cluster, make sure to place a custom finalizer on the <code>AzureCluster</code> object. This is needed to prevent it from being deleted instantly, which will cause cluster deletion to stuck indefinitely.</p> <p>To place a finalizer you can execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --patch 'metadata: {finalizers: [manual]}'\n</code></pre> <p>When the finalizer is placed you can remove the <code>ClusterDeployment</code> as usual. Check that all <code>AzureMachine</code> objects are deleted successfully, then remove the finalizer you've placed to finish cluster deletion.</p> <p>If you have orphaned <code>AzureMachine</code> objects left, you'll have to delete the finalizers on them manually after making sure that no VMs are present in Azure.</p> <p>Note</p> <p> Since Azure admission prohibits orphaned objects mutation, you'll have to disable it by deleting its <code>mutatingwebhookconfiguration</code>.</p>"},{"location":"template-byo/","title":"Bring Your Own Templates","text":"<p>In addition to the templates that ship with k0rdent, it's possible to make your own. These might represent different types of clusters, or they may represent additional services to add to a cluster. Follow these steps:</p>"},{"location":"template-byo/#create-a-source-object","title":"Create a Source Object","text":"<p>Info</p> <p> Skip this step if you're using an existing source.</p> <p>All templates are based on a Helm chart, so the first step is to define the source in which that Helm chart can be found.</p> <p>The source can be one of the following types:</p> <ul> <li>HelmRepository</li> <li>GitRepository</li> <li>Bucket</li> </ul> <p>Note that it's important to pay attention to where the source resides. Cluster-scoped <code>ProviderTemplate</code> objects must reside in the system namespace (<code>kcm-system</code> by default) but other template sources must reside in the same directory as the templates that will come from them.</p> <p>For example, this YAML describes a custom <code>Source</code> object of <code>kind</code> <code>HelmRepository</code>:</p> <p>Note</p> <p> The custom <code>HelmRepository</code> must have the label <code>k0rdent.mirantis.com/managed: \"true\"</code>.</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: k0rdent-catalog\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/k0rdent/kcm/charts\n</code></pre>"},{"location":"template-byo/#create-the-template","title":"Create the Template","text":"<p>Once you have the source, you can create the actual template. This template can be one of three types:</p> <ul> <li><code>ClusterTemplate</code></li> <li><code>ServiceTemplate</code></li> <li><code>ProviderTemplate</code></li> </ul> <p>For <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, configure the namespace where this template should reside (<code>metadata.namespace</code>).</p> <p>For defining the Helm chart, you have two choices. You can either specify the actual helm chart definition in the <code>.spec.helm.chartSpec</code> field of the HelmChartSpec kind, or you can reference and existing <code>HelmChart</code> object in <code>.spec.helm.chartRef</code>.</p> <p>Note</p> <p> <code>spec.helm.chartSpec</code> and <code>spec.helm.chartRef</code> are mutually exclusive.</p> <p>To automatically create the <code>HelmChart</code> for the <code>Template</code>, configure the following custom helm chart parameters under <code>spec.helm.chartSpec</code>:</p> Field Description <code>sourceRef</code>LocalHelmChartSourceReference Reference to the source object (for example, <code>HelmRepository</code>, <code>GitRepository</code>, or <code>Bucket</code>) in the same namespace as the Template. <code>chart</code>string The name of the Helm chart available in the source. <code>version</code>string Version is the chart version semver expression. Defaults to latest when omitted. <code>interval</code>Kubernetes meta/v1.Duration The frequency at which the <code>sourceRef</code> is checked for updates. Defaults to 10 minutes. <p>For the complete list of the <code>HelmChart</code> parameters, see: HelmChartSpec.</p> <p>The controller automatically creates the <code>HelmChart</code> object based on the chartSpec defined in <code>.spec.helm.chartSpec</code>.</p> <p>Note</p> <p> <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects should reside in the same namespace as the <code>ClusterDeployment</code> referencing them. The <code>ClusterDeployment</code> can't reference the Template from another namespace (the creation request will be declined by the admission webhook). All <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects shipped with KCM reside in the system namespace (defaults to <code>kcm-system</code>). To get the instructions on how to distribute Templates along multiple namespaces, read Template Life Cycle Management.</p>"},{"location":"template-byo/#examples","title":"Examples","text":"<p>Example</p> <p>Custom <code>ClusterTemplate</code> with the Chart Definition to Create a new HelmChart <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: custom-template\n  namespace: kcm-system\nspec:\n  providers:\n    - bootstrap-k0sproject-k0smotron\n    - control-plane-k0sproject-k0smotron\n    - infrastructure-openstack\n  helm:\n    chartSpec:\n      chart: os-k0sproject-k0smotron\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n</code></pre></p> <p>Example</p> <p>Custom <code>ClusterTemplate</code> Referencing an Existing HelmChart object</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: custom-template\n  namespace: kcm-system\nspec:\n  helm:\n    chartRef:\n      kind: HelmChart\n      name: custom-chart\n</code></pre>"},{"location":"template-byo/#required-and-exposed-providers-definition","title":"Required and exposed providers definition","text":"<p>The <code>*Template</code> object must specify the list of Cluster API providers that are either required (for <code>ClusterTemplates</code> and <code>ServiceTemplates</code>) or exposed (for <code>ProviderTemplates</code>). These providers include <code>infrastructure</code>, <code>bootstrap</code>, and <code>control-plane</code>. This can be achieved in two ways:</p> <ol> <li>By listing the providers explicitly in the <code>spec.providers</code> field.</li> <li>Alternatively, by including specific annotations in the <code>Chart.yaml</code> of the referenced Helm chart. The annotations should list the providers as a <code>comma-separated</code> value.</li> </ol> <p>For example:</p> <p>In a <code>Template</code> spec:</p> <pre><code>spec:\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n</code></pre> <p>In a <code>Chart.yaml</code>:</p> <pre><code>annotations:\n    cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n</code></pre>"},{"location":"template-byo/#compatibility-attributes","title":"Compatibility attributes","text":"<p>Each of the <code>*Template</code> resources has compatibility versions attributes to constraint the core <code>CAPI</code>, <code>CAPI</code> provider or Kubernetes versions. CAPI-related version constraints must be set in the <code>CAPI</code> contract format. Kubernetes version constraints must be set in the Semantic Version format. Each attribute can be set either via the corresponding <code>.spec</code> fields or via the annotations. Values set via the <code>.spec</code> have precedence over the values set via the annotations.</p> <p>Note</p> <p> All of the compatibility attributes are optional, and validation checks only take place if both of the corresponding type attributes (e.g. provider contract versions in both <code>ProviderTemplate</code> and <code>ClusterTemplate</code>) are set.</p> <ol> <li> <p>The <code>ProviderTemplate</code> resource has dedicated fields to set compatible <code>CAPI</code> contract versions along with CRDs contract versions supported by the provider. Given contract versions will then be set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the core <code>CAPI</code> contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core <code>CAPI</code>. For the core <code>CAPI</code> Template values should be empty.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ProviderTemplate\n# ...\nspec:\n  providers:\n  - infrastructure-aws\n  capiContracts:\n    # commented is the example exclusively for the core CAPI Template\n    # v1alpha3: \"\"\n    # v1alpha4: \"\"\n    # v1beta1: \"\"\n    v1alpha3: v1alpha3\n    v1alpha4: v1alpha4\n    v1beta1: v1beta1_v1beta2\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code> with the same logic as in the <code>.spec</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws\n  cluster.x-k8s.io/v1alpha3: v1alpha3\n  cluster.x-k8s.io/v1alpha4: v1alpha4\n  cluster.x-k8s.io/v1beta1: v1beta1_v1beta2\n</code></pre> </li> <li> <p>The <code>ClusterTemplate</code> resource has dedicated fields to set an exact compatible Kubernetes version in the Semantic Version format and required contract versions per each provider to match against the related <code>ProviderTemplate</code> objects. Given compatibility attributes will be then set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\n# ...\nspec:\n  k8sVersion: 1.30.0 # only exact semantic version is applicable\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n  providerContracts:\n    bootstrap-k0sproject-k0smotron: v1beta1 # only a single contract version is applicable\n    control-plane-k0sproject-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n</code></pre> <p>Example with the <code>.annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n  cluster.x-k8s.io/bootstrap-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/control-plane-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/infrastructure-aws: v1beta2\n  k0rdent.mirantis.com/k8s-version: 1.30.0\n</code></pre> </li> <li> <p>The <code>ServiceTemplate</code> resource has dedicated fields to set an compatibility constrained Kubernetes version to match against the related <code>ClusterTemplate</code> objects. Given compatibility values will be then set accordingly in the <code>.status</code> field.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\n# ...\nspec:\n  k8sConstraint: \"^1.30.0\" # only semantic version constraints are applicable\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>k0rdent.mirantis.com/k8s-version-constraint: ^1.30.0\n</code></pre> </li> </ol>"},{"location":"template-byo/#compatibility-attributes-enforcement","title":"Compatibility attributes enforcement","text":"<p>The aforedescribed attributes are checked for compliance with the following rules:</p> <ul> <li>Both the exact and constraint version of the same type (for example <code>k8sVersion</code> and <code>k8sConstraint</code>) must be set, otherwise no check is performed;</li> <li>If a <code>ClusterTemplate</code> object's provider's contract version does not satisfy contract versions from the related <code>ProviderTemplate</code> object, updates to the <code>ClusterDeployment</code> object will be blocked;</li> <li>If a <code>ProviderTemplate</code> object's <code>CAPI</code> contract version (for example, in a <code>v1beta1: v1beta1_v1beta2</code> key-value pair, the key <code>v1beta1</code> is the core <code>CAPI</code> contract version) is not listed in the core <code>CAPI</code> <code>ProviderTemplate</code> object, the updates to the <code>Management</code> object will be blocked;</li> <li>If a <code>ClusterTemplate</code> object's exact kubernetes version does not satisfy the kubernetes version constraint from the related <code>ServiceTemplate</code> object, the updates to the <code>ClusterDeployment</code> object will be blocked.</li> </ul>"},{"location":"template-intro/","title":"The Templates system","text":"<p>By default, k0rdent delivers a set of default <code>ProviderTemplate</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects:</p> <ul> <li><code>ProviderTemplate</code>    The template containing the configuration of the provider (for example, k0smotron or AWS). These are cluster-scoped.</li> <li><code>ClusterTemplate</code>    The template containing the configuration of the cluster objects. These are namespace-scoped.</li> <li><code>ServiceTemplate</code>    The template containing the configuration of the service to be installed on the cluster deployment. These are namespace-scoped.</li> </ul> <p>All Templates are immutable, so if you want to change something about a cluster that has been deployed, you have to apply a whole new template. You can also build your own templates and use them for deployment along with the templates shipped with k0rdent.</p>"},{"location":"template-intro/#template-naming-convention","title":"Template Naming Convention","text":"<p>The templates can have any name. However, since they are immutable, we have adopted a naming convention that includes semantic versioning in the name, as in <code>template-&lt;major&gt;-&lt;minor&gt;-&lt;patch&gt;</code>. Below are some examples for each of the templates.</p> <p>Example</p> <p>An example of a <code>ProviderTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ProviderTemplate\nmetadata:\n  name: cluster-api-0-1-0\nspec:\n  helm:\n    chartSpec:\n      chart: cluster-api\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n      version: 0.1.0\nstatus:\n  capiContracts:\n    v1alpha3: \"\"\n    v1alpha4: \"\"\n    v1beta1: \"\"\n  chartRef:\n    kind: HelmChart\n    name: cluster-api-0-0-4\n    namespace: kcm-system\n  config:\n    airgap: false\n    config: {}\n    configSecret:\n      create: false\n      name: \"\"\n      namespace: \"\"\n  description: A Helm chart for Cluster API core components\n  observedGeneration: 1\n  valid: true\n</code></pre></p> <p>Example</p> <p>An example of a <code>ClusterTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: aws-standalone-cp-0-1-0\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: aws-standalone-cp\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n      version: 0.1.0\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: aws-standalone-cp-0-1-0\n    namespace: kcm-system\n  config:\n    bastion:\n      allowedCIDRBlocks: []\n      ami: \"\"\n      disableIngressRules: false\n      enabled: false\n      instanceType: t2.micro\n    clusterIdentity:\n      kind: AWSClusterStaticIdentity\n      name: \"\"\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n        - 10.244.0.0/16\n      services:\n        cidrBlocks:\n        - 10.96.0.0/12\n    controlPlane:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    controlPlaneNumber: 3\n    extensions:\n      chartRepository: \"\"\n      imageRepository: \"\"\n    k0s:\n      version: v1.31.1+k0s.1\n    publicIP: false\n    region: \"\"\n    sshKeyName: \"\"\n    worker:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    workersNumber: 2\n  description: 'An kcm template to deploy a k0s cluster on AWS with bootstrapped control\n    plane nodes. '\n  observedGeneration: 1\n  providerContracts:\n    bootstrap-k0smotron: v1beta1\n    control-plane-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n  providers:\n  - bootstrap-k0smotron\n  - control-plane-k0smotron\n  - infrastructure-aws\n  valid: true\n</code></pre></p> <p>Example</p> <p>An example of a <code>ServiceTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n      version: 3.2.6\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: kyverno-3-2-6\n    namespace: kcm-system\n  description: A Helm chart to refer the official kyverno helm chart\n  observedGeneration: 1\n  valid: true\n</code></pre></p>"},{"location":"template-intro/#template-life-cycle-management","title":"Template Life Cycle Management","text":"<p>Cluster and Service Templates can be delivered to target namespaces using the <code>AccessManagement</code>, <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects. The <code>AccessManagement</code> object contains the list of access rules to apply. Each access rule contains the namespaces' definition for delivering templates into and the template chains to deliver. Each <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> contains the supported templates and the upgrade sequences for them.</p> <p>The example of <code>ClusterTemplate</code> Management:</p> <ol> <li> <p>Create a <code>ClusterTemplateChain</code> object in the system namespace (defaults to <code>kcm-system</code>). Properly configure     the list of <code>.spec.supportedTemplates[].availableUpgrades</code> for the specified <code>ClusterTemplate</code> if you want to     allow upgrading. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-2\n      availableUpgrades:\n        - name: aws-standalone-cp-0-1-0\n    - name: aws-standalone-cp-0-1-0\n</code></pre> </li> <li> <p>Edit the <code>AccessManagement</code> object and configure the <code>.spec.accessRules</code>.     For example, to apply all templates and upgrade sequences defined in the <code>aws</code> <code>ClusterTemplateChain</code> to the     <code>default</code> namespace, add the following <code>accessRule</code>:</p> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - default\n    clusterTemplateChains:\n      - aws\n</code></pre> </li> </ol> <p>The kcm controllers will deliver all the <code>ClusterTemplate</code> objects across the target namespaces. As a result, the following new objects should be created:</p> <ul> <li><code>ClusterTemplateChain</code> <code>default/aws</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-2</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-1-0</code> (available for the upgrade from <code>aws-standalone-cp-0-0-2</code>)</li> </ul> <p>Note</p> <ol> <li>The target <code>ClusterTemplate</code> defined as being available for the upgrade should reference the same helm chart name as the source <code>ClusterTemplate</code>. Otherwise, after the upgrade is triggered, the cluster will be removed and then recreated from scratch, even if the objects in the helm chart are the same.</li> <li>The target template should not affect immutable fields or any other incompatible internal objects upgrades, otherwise the upgrade will fail.</li> </ol>"},{"location":"template-openstack/","title":"OpenStack Machine parameters","text":""},{"location":"template-openstack/#clusterdeployment-parameters","title":"ClusterDeployment Parameters","text":"<p>To deploy an OpenStack cluster, the following are the primary parameters in the <code>ClusterDeployment</code> resource:</p> Parameter Example Description .spec.credential <code>openstack-cluster-identity-cred</code> Reference to the Credential object. .spec.template <code>openstack-standalone-cp-0-1-0</code> Reference to the ClusterTemplate. .spec.config.authURL <code>https://keystone.yourorg.net/</code> Keystone authentication endpoint for OpenStack. .spec.config.controlPlaneNumber <code>3</code> Number of control plane nodes. .spec.config.workersNumber <code>2</code> Number of worker nodes. .spec.config.clusterLabels(optional) <code>k0rdent: demo</code> Labels to apply to the cluster. Used by MultiClusterService."},{"location":"template-openstack/#ssh-configuration","title":"SSH Configuration","text":"<p><code>sshPublicKey</code> is the reference name for an existing SSH key configured in OpenStack.</p> <ul> <li>ClusterDeployment: Specify the SSH public key using the <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (for the standlone control plane).</li> </ul>"},{"location":"template-openstack/#machine-configuration","title":"Machine Configuration","text":"<p>Configurations for control plane and worker nodes are specified separately under <code>.spec.config.controlPlane</code> and <code>.spec.config.worker</code>:</p> Parameter Example Description <code>flavor</code> <code>m1.medium</code> OpenStack flavor for the instance. <code>image.filter.name</code> <code>ubuntu-22.04-x86_64</code> Name of the image. <code>sshPublicKey</code> <code>ramesses-pk</code> Reference name for an existing SSH key. <code>securityGroups.filter.name</code> <code>default</code> Security group for the instance. <p>Note</p> <p> Make sure <code>.spec.credential</code> references the <code>Credential</code> object. The recommended minimum vCPU value for the control plane flavor is 2, while for the worker node flavor, it is 1. For detailed information, refer to the machine-flavor CAPI docs.</p>"},{"location":"template-openstack/#example-clusterdeployment","title":"Example ClusterDeployment","text":"<pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-0-1-0\n  credential: openstack-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    clusterLabels:\n      k0rdent: demo\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      sshPublicKey: my-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      sshPublicKey: my-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    authURL: https://my-keystone-openstack-url.com\n    identityRef:\n      name: openstack-cloud-config\n      cloudName: openstack\n      region: RegionOne\n</code></pre>"},{"location":"template-predefined/","title":"Removing predefined templates","text":""},{"location":"template-predefined/#remove-templates-shipped-with-k0rdent","title":"Remove Templates shipped with k0rdent","text":"<p>If you need to limit the templates that exist in your k0rdent installation, follow the instructions below:</p> <ol> <li> <p>Get the list of <code>ProviderTemplate</code>, <code>ClusterTemplate</code> or <code>ServiceTemplate</code> objects shipped with k0rdent. For example, for <code>ClusterTemplate</code> objects, run:</p> <p><pre><code>kubectl get clustertemplates -n kcm-system -l helm.toolkit.fluxcd.io/name=kcm-templates\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-1-0           true\nkcm-system   aws-eks-0-1-0                   true\nkcm-system   aws-hosted-cp-0-1-0             true\nkcm-system   aws-standalone-cp-0-1-0         true\nkcm-system   azure-aks-0-1-0                 true\nkcm-system   azure-hosted-cp-0-1-0           true\nkcm-system   azure-standalone-cp-0-1-0       true\nkcm-system   openstack-standalone-cp-0-1-0   true\nkcm-system   vsphere-hosted-cp-0-1-0         true\nkcm-system   vsphere-standalone-cp-0-1-0     true\n</code></pre></p> </li> <li> <p>Remove the template from the list using <code>kubectl delete</code>, as in:</p> <pre><code>kubectl delete clustertemplate -n kcm-system &lt;template-name&gt;\n</code></pre> </li> </ol>"},{"location":"template-vsphere/","title":"vSphere cluster template parameters","text":""},{"location":"template-vsphere/#clusterdeployment-parameters","title":"ClusterDeployment parameters","text":"<p>To create a cluster deployment a number of parameters should be passed to the <code>ClusterDeployment</code> object.</p>"},{"location":"template-vsphere/#parameter-list","title":"Parameter list","text":"<p>The following is the list of vSphere specific parameters that are required for successful cluster creation.</p> Parameter Example Description <code>.spec.config.vsphere.server</code> <code>vcenter.example.com</code> Address of the vSphere instance <code>.spec.config.vsphere.thumbprint</code> <code>\"00:00:00:...\"</code> Certificate thumbprint <code>.spec.config.vsphere.datacenter</code> <code>DC</code> Datacenter name <code>.spec.config.vsphere.datastore</code> <code>/DC/datastore/DS</code> Datastore path <code>.spec.config.vsphere.resourcePool</code> <code>/DC/host/vCluster/Resources/ResPool</code> Resource pool path <code>.spec.config.vsphere.folder</code> <code>/DC/vm/example</code> Folder path <code>.spec.config.controlPlane.network</code> <code>/DC/network/vm_net</code> Network path for <code>controlPlane</code> <code>.spec.config.worker.network</code> <code>/DC/network/vm_net</code> Network path for <code>worker</code> <code>.spec.config.*.ssh.publicKey</code> <code>\"ssh-ed25519 AAAA...\"</code> SSH public key in <code>authorized_keys</code> format <code>.spec.config.*.vmTemplate</code> <code>/DC/vm/templates/ubuntu</code> VM template image path <code>.spec.config.controlPlaneEndpointIP</code> <code>172.16.0.10</code> <code>kube-vip</code> vIP which will be created for control plane endpoint <p>To get the vSphere certificate thumbprint you can use the following command:</p> <pre><code>curl -sw %{certs} https://vcenter.example.com | openssl x509 -sha256 -fingerprint -noout | awk -F '=' '{print $2}'\n</code></pre> <p><code>govc</code>, a vSphere CLI, can also help to discover proper values for some of the parameters:</p> <pre><code># vsphere.datacenter\ngovc ls\n\n# vsphere.datastore\ngovc ls /*/datastore/*\n\n# vsphere.resourcePool\ngovc ls /*/host/*/Resources/*\n\n# vsphere.folder\ngovc ls -l /*/vm/**\n\n# controlPlane.network, worker.network\ngovc ls /*/network/*\n\n# *.vmTemplate\ngovc vm.info -t '*'\n</code></pre> <p>Note</p> <p> Follow official <code>govc</code> installation instructions from here. The <code>govc</code> usage guide is here.</p> <p>Minimal <code>govc</code> configuration requires setting: <code>GOVC_URL</code>, <code>GOVC_USERNAME</code>, <code>GOVC_PASSWORD</code> environment variables.</p>"},{"location":"template-vsphere/#example-of-a-clusterdeployment-cr","title":"Example of a ClusterDeployment CR","text":"<p>With all above parameters provided your <code>ClusterDeployment</code> can look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-standalone-cp-0-1-0\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"&lt;VSPHERE_SERVER&gt;\"\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n</code></pre> <p>Don't forget to replace placeholder values such as <code>VSPHERE_SERVER</code> with actual values for your environment.</p>"},{"location":"template-vsphere/#ssh","title":"SSH","text":"<p>Currently SSH configuration on vSphere expects that the user is already created before template creation. Because of that you must pass the username along with the SSH public key to configure SSH access.</p> <p>The SSH public key can be passed to <code>.spec.config.ssh.publicKey</code> (in the case of a hosted control plane) or <code>.spec.config.controlPlane.ssh.publicKey</code> and <code>.spec.config.worker.ssh.publicKey</code> (in the case of a standalone control) of the <code>ClusterDeployment</code> object.</p> <p>The SSH public key must be passed literally as a string.</p> <p>You can pass the username to <code>.spec.config.controlPlane.ssh.user</code>, <code>.spec.config.worker.ssh.user</code> or <code>.spec.config.ssh.user</code>, depending on you deployment model.</p>"},{"location":"template-vsphere/#vm-resources","title":"VM resources","text":"<p>The following parameters are used to define VM resources:</p> Parameter Example Description <code>.rootVolumeSize</code> <code>50</code> Root volume size in GB (can't be less than the one defined in the image) <code>.cpus</code> <code>2</code> Number of CPUs <code>.memory</code> <code>4096</code> Memory size in MB <p>The resource parameters are the same for hosted and standalone CP deployments, but they are positioned differently in the spec, which means that they're going to:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"template-vsphere/#vm-image-and-network","title":"VM Image and network","text":"<p>To provide image template path and network path the following parameters must be used:</p> Parameter Example Description <code>.vmTemplate</code> <code>/DC/vm/template</code> Image template path <code>.network</code> <code>/DC/network/Net</code> Network path <p>As with resource parameters the position of these parameters in the <code>ClusterDeployment</code> depends on deployment type and these parameters are used in:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"template-vsphere/#hosted-control-plane-k0smotron-deployment","title":"Hosted control plane (k0smotron) deployment","text":""},{"location":"template-vsphere/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on vSphere with k0rdent installed on it</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster, so make sure the server is robust enough to handle it.</p>"},{"location":"template-vsphere/#clusterdeployment-manifest","title":"ClusterDeployment manifest","text":"<p>The hosted CP template has mostly identical parameters to the standalone CP, and you can check them in the template parameters section.</p> <p>Note</p> <p> The vSphere provider requires the control plane endpoint IP to be specified before deploying the cluster. Ensure that this IP matches the IP assigned to the k0smotron load balancer (LB) service. Provide the control plane endpoint IP to the k0smotron service via an annotation accepted by your LB provider (such as the <code>kube-vip</code> annotation in the example below).</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp-0-1-0\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"&lt;VSPHERE_SERVER&gt;\"\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"&lt;VSPHERE_LOADBALANCER_IP&gt;\"\n</code></pre> <p>Don't forget to substitute placeholders such as <code>&lt;VSPHERE_SERVER&gt;</code> with actual values.</p>"},{"location":"user-create-cluster/","title":"Deploying a Cluster","text":"<p>k0rdent simplifies the process of deploying and managing Kubernetes clusters across various cloud platforms through the use of <code>ClusterDeployment</code> objects, which include all of the information k0rdent needs to know in order to create the cluster you want. This <code>ClusterDeployment</code> system relies on predefined templates and credentials. </p> <p>A cluster deployment typically involves:</p> <ol> <li>Credentials for the infrastructure provider (such as AWS, vSphere, and so on).</li> <li>A template that defines the desired cluster configuration (for example, number of nodes or instance types).</li> <li>Submitting the configuration for deployment and monitoring the process.</li> </ol> <p>Follow these steps to deploy a standalone Kubernetes cluster:</p> <ol> <li> <p>Obtain the <code>Credential</code> object</p> <p>k0rdent needs credentials to communicate with the infrastructure provider (for example, AWS, Azure, or vSphere). These credentials enable k0rdent to provision resources such as virtual machines, networking components, and storage.</p> <p><code>Credential</code> objects are generally created ahead of time and made available to users. You can see all of the existing <code>Credential</code> objects by querying the management cluster:</p> <p><pre><code>kubectl get credentials -n accounting\n</code></pre> When you find a <code>Credential</code> that looks appropriate, you can get more information by <code>describe</code>-ing it, as in:</p> <pre><code>kubectl describe credential accounting-cluster-credential -n accounting\n</code></pre> <p>You'll see the YAML for the <code>Credential</code> object, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: accounting-cluster-credential\n  namespace: accounting\nspec:\n  description: \"Credentials for Accounting AWS account\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: accountingd-cluster-identity\n</code></pre> <p>As you can see, the <code>.spec.description</code> field gives more information about the <code>Credential</code>.</p> <p>If the <code>Credential</code> you need doesn't yet exist, you can ask your cloud administrator to create it, or you can follow the instructions in the Credential System, as well as the specific instructions for your target infrastructure, to create it yourself.</p> <p>Tip</p> <p>Double-check to make sure that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Select a Template</p> <p>Templates in k0rdent are predefined configurations that describe how to set up the cluster. Templates include details such as:</p> <ul> <li>The number and type of control plane and worker nodes.</li> <li>Networking settings.</li> <li>Regional deployment preferences.</li> </ul> <p>Templates act as a blueprint for creating a cluster. To see the list of available templates, use the following command:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-1-0           true\nkcm-system   aws-eks-0-1-0                   true\nkcm-system   aws-hosted-cp-0-1-0             true\nkcm-system   aws-standalone-cp-0-1-0         true\nkcm-system   azure-aks-0-1-0                 true\nkcm-system   azure-hosted-cp-0-1-0           true\nkcm-system   azure-standalone-cp-0-1-0       true\nkcm-system   openstack-standalone-cp-0-1-0   true\nkcm-system   vsphere-hosted-cp-0-1-0         true\nkcm-system   vsphere-standalone-cp-0-1-0     true\n</code></pre></p> <p>You can then get information on the actual template by describing it, as in:</p> <pre><code>kubectl describe clustertemplate aws-standalone-cp-0-1-0 -n kcm-system\n</code></pre> </li> <li> <p>Create a ClusterDeployment YAML Configuration</p> <p>Once you have the <code>Credential</code> and the <code>ClusterTemplate</code> you can create the <code>ClusterDeployment</code> object configuration.  It includes:</p> <ul> <li>The template to use.</li> <li>The credentials for the infrastructure provider.</li> <li>Optional customizations such as instance types, regions, and networking.</li> </ul> <p>Create a <code>ClusterDeployment</code> configuration in a YAML file, following this structure:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>You will of course want to replace the placeholders with actual values. (For more information about <code>dryRun</code> see Understanding the Dry Run) For example, this is a simple AWS infrastructure provider <code>ClusterDeployment</code>:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-1-0\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> Note that the <code>.spec.credential</code> value should match the <code>.metadata.name</code> value of a created <code>Credential</code> object.</p> </li> <li> <p>Apply the Configuration</p> <p>Once the <code>ClusterDeployment</code> configuration is ready, apply it to the k0rdent management cluster:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits your deployment request to k0rdent. </p> </li> <li> <p>Verify Deployment Status</p> <p>After submitting the configuration, verify that the <code>ClusterDeployment</code> object has been created successfully:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output shows the current status and any errors.</p> </li> <li> <p>Monitor Provisioning</p> <p>k0rdent will now start provisioning resources (for example, VMs or networks) and setting up the cluster. To monitor this process, run:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> </li> <li> <p>Retrieve the Kubernetes Configuration</p> <p>When provisioning is complete, you can retrieve the kubeconfig file for the new cluster so you can interact with the cluster using <code>kubectl</code>:</p> <p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> You can then use this file to access the cluster, as in:</p> <pre><code>export KUBECONFIG=kubeconfig\nkubectl get pods -A\n</code></pre> <p>Store the kubeconfig file securely, as it contains authentication details for accessing the cluster.</p> </li> </ol>"},{"location":"user-create-cluster/#cleanup","title":"Cleanup","text":"<p>When you're finished you'll want to remove the cluster. Because the cluster is represented by the <code>ClusterDeployment</code> object, deleting the cluster is a simple matter of deleting that object.  For example:</p> <pre><code>kubectl delete clusterdeployment &lt;cluster-name&gt; -n kcm-system\n</code></pre> <p>Note that even though the Kubernetes object is deleted immediately, it will take a few minutes for the actual resources to be removed.</p>"},{"location":"user-create-service/","title":"Deploy Services to a Managed Cluster","text":"<p>At its heart, everything in k0rdent is based on templates that help define Kubernetes objects. For clusters, these are <code>ClusterTemplate</code> objects. For applications and services, these are <code>ServiceTemplate</code> objects.</p>"},{"location":"user-create-service/#understanding-servicetemplates","title":"Understanding ServiceTemplates","text":"<p><code>ServiceTemplate</code> objects are meant to let k0rdent know where to find a Helm chart with instructions for installing an application. In many cases, these charts will be in a private repository.  For example, consider this template for installing Nginx Ingress:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.0\n  namespace: tenant42\nspec:\n  helm:\n    chartSpec:\n      chart: demo-ingress-nginx\n      version: 4.11.0\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-demos\n---\napiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplateChain\nmetadata:\n  name: project-ingress-nginx-4.11.0\n  namespace: tenant42\nspec:\n  supportedTemplates:\n    - name: project-ingress-nginx-4.11.0\n    - name: project-ingress-nginx-4.10.0\n      availableUpgrades:\n        - name: project-ingress-nginx-4.11.0\n</code></pre> <p>Here you see a template called <code>project-ingress-nginx-4.11.0</code> that is meant to be deployed in the <code>tenant42</code> namespace. The <code>.spec.helm.chartSpec</code> specifies the name of the Helm chart and where to find it, as well as the version and other  important information. The <code>ServiceTemplateChain</code> shows that this template is also an upgrade path from version 4.10.0.</p> <p>If you wanted to deploy this as an application, you would first go ahead and add the template to the cluster in which you were working, so if you were to save this YAML to a file called <code>project-ingress.yaml</code> you could run this command on the management cluster:</p> <pre><code>kubectl apply -f project-ingress.yaml -n tenant42\n</code></pre>"},{"location":"user-create-service/#adding-a-service-to-a-clusterdeployment","title":"Adding a <code>Service</code> to a <code>ClusterDeployment</code>","text":"<p>To add the service defined by this template to a cluster, you simply add it to the <code>ClusterDeployment</code> object when you create it, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-0-1-0\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: project-ingress-nginx-4.11.0\n        name: ingress-nginx\n        namespace: tenant42\n    priority: 100\n</code></pre> As you can see, you're simply referencing the template in the <code>.spec.serviceSpec.services[].template</code> field of the <code>ClusterDeployment</code> to tell k0rdent that you want this service to be part of this cluster.</p> <p>If you wanted to add this service to an existing cluster, you would simply patch the definition of the <code>ClusterDeployment</code>, as in:</p> <pre><code>kubectl patch clusterdeployment my-cluster-deployment -n tenant42 --type='merge' -p '\nspec:\n  serviceSpec:\n    services:\n      - template: project-ingress-nginx-4.11.0\n        name: ingress-nginx\n        namespace: tenant42\n</code></pre> <p>Let's look at a more complex case, involving deploying beach-head services on a single cluster.</p>"},{"location":"user-create-service/#deployment-of-beach-head-services","title":"Deployment of beach-head services","text":"<p>Beach-head services can be installed on a cluster deployment (that is, a target cluster) using the <code>ClusterDeployment</code> object, just as with a single service. Consider the following example of a <code>ClusterDeployment</code> object for AWS Infrastructure Provider with beach-head services.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  config:\n    clusterLabels: {}\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    controlPlane:\n      amiID: ami-0eb9fdcf0d07bd5ef\n      instanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: t3.small\n      controlPlaneNumber: 1\n      publicIP: true\n      region: ca-central-1\n    worker:\n      amiID: ami-0eb9fdcf0d07bd5ef\n      instanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: t3.small\n    workersNumber: 1\n  credential: aws-cluster-identity-cred\n  serviceSpec:\n    services:\n      - template: kyverno-3-2-6\n        name: kyverno\n        namespace: kyverno\n      - template: ingress-nginx-4-11-3\n        name: ingress-nginx\n        namespace: ingress-nginx\n    priority: 100\n  template: aws-standalone-cp-0-1-0\n</code></pre> <p>In the example above, the fields under <code>serviceSpec</code> are relevant to the deployment of beach-head services.</p> <p>Note</p> <p> Refer to the Template Guide for more detail about these fields.</p> <p>This example <code>ClusterDeployment</code> object deploys kyverno and ingress-nginx, as referred to by their service templates respectively, on the target cluster.  As before, the <code>ServiceTemplate</code> includes information on the service. For example, here is the <code>ServiceTemplate</code> for kyverno:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      version: 3.2.6\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n</code></pre> <p>The <code>k0rdent-catalog</code> helm repository hosts the actual kyverno chart version 3.2.6. For more details see the Bring your own Templates guide.</p>"},{"location":"user-create-service/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Helm values can be passed to each beach-head service with the <code>.spec.serviceSpec.services[].values</code> field in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object. For example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - template: ingress-nginx-4-11-3\n      name: ingress-nginx\n      namespace: ingress-nginx\n      values: |\n        ingress-nginx:\n          controller:\n            replicaCount: 3\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n      values: |\n        kyverno:\n          admissionController:\n            replicas: 3\n    - name: motel-regional\n      namespace: motel\n      template: motel-regional-0-1-1\n      values: |\n        victoriametrics:\n          vmauth:\n            ingress:\n              host: vmauth.kcm0.example.net\n            credentials:\n              username: motel\n              password: motel\n        grafana:\n          ingress:\n            host: grafana.kcm0.example.net\n   . . .\n</code></pre> </p> <p>Note</p> <p>The values for ingress-nginx and kyverno start with the \"ingress-nginx:\" and \"kyverno:\" keys respectively because the helm charts used by the ingress-nginx-4-11-3 and kyverno-3-2-6 <code>ServiceTemplate</code> objects use the official upstream helm charts for ingress-nginx and kyverno as dependencies.</p>"},{"location":"user-create-service/#templating-custom-values","title":"Templating Custom Values","text":"<p>Using the Sveltos templating feature, we can also write templates that can be useful for automatically fetching pre-existing information within the cluster. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - template: motel-0-1-0\n      name: motel\n      namespace: motel\n    - template: myappz-0-3-0\n      name: myappz\n      namespace: myappz\n      values: |\n        controlPlaneEndpointHost: {{ .Cluster.spec.controlPlaneEndpoint.host }}\n        controlPlaneEndpointPort: \"{{ .Cluster.spec.controlPlaneEndpoint.port }}\"\n    priority: 100\n    . . .        \n</code></pre> <p>In this case, the host and port information will be fetched from the spec of the CAPI cluster that hosts this <code>ClusterDeployment</code>.</p>"},{"location":"user-create-service/#checking-status","title":"Checking status","text":"<p>The <code>.status.services</code> field of the <code>ClusterDeployment</code> object shows the status for each of the beach-head services. For example, if you were to <code>describe</code> the <code>ClusterDeployment</code> with these services, you would see conditions that show status information, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 1\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    . . .\nstatus:\n  . . .\n  observedGeneration: 1\n  services:\n  - clusterName: my-cluster-deployment\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>Based on the information above both kyverno and ingress-nginx are installed in their respective namespaces on the target cluster. You can check to see for yourself:</p> <p><pre><code>kubectl get pod -n kyverno\n</code></pre> <pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\nkyverno-admission-controller-96c5d48b4-sg5ts     1/1     Running   0          2m39s\nkyverno-background-controller-65f9fd5859-tm2wm   1/1     Running   0          2m39s\nkyverno-cleanup-controller-848b4c579d-ljrj5      1/1     Running   0          2m39s\nkyverno-reports-controller-6f59fb8cd6-s8jc8      1/1     Running   0          2m39s\n</code></pre> <pre><code>kubectl get pod -n ingress-nginx \n</code></pre> <pre><code>NAME                                       READY   STATUS    RESTARTS   AGE\ningress-nginx-controller-cbcf8bf58-zhvph   1/1     Running   0          24m\n</code></pre></p> <p>Youc an get more information on how to access the child cluster in the create a cluster deployment chapter, and more on <code>ServiceTemplate</code> objects in the Template Guide.</p>"},{"location":"user-create-service/#removing-beach-head-services","title":"Removing beach-head services","text":"<p>To remove a beach-head service simply remove its entry from <code>.spec.serviceSpec.services</code>. The example below removes <code>kyverno-3-2-6</code>, so its status also removed from <code>.status.services</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 2\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    priority: 100\n    . . .\nstatus:\n  . . .\n  observedGeneration: 2\n  services:\n  - clusterName: wali-aws-dev\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre>"},{"location":"user-create-service/#parameter-list","title":"Parameter List","text":"<p>Here is an idea of the parameters involved.</p> Parameter Example Description <code>.spec.serviceSpec.syncMode</code> <code>Continuous</code> Specifies how beach-head services are synced i the target cluster (default:<code>Continuous</code>) <code>.spec.serviceSpec.DriftIgnore</code> specifies resources to ignore for drift detection <code>.spec.serviceSpec.DriftExclusions</code> specifies specific configurations of resources to ignore for drift detection <code>.spec.serviceSpec.priority</code> <code>100</code> Sets the priority for the beach-head services defined in this spec (default: <code>100</code>) <code>.spec.serviceSpec.stopOnConflict</code> <code>false</code> Stops deployment of beach-head services upon first encounter of a conflict (default: <code>false</code>) <code>.spec.serviceSpec.services[].template</code> <code>kyverno-3-2-6</code> Name of the <code>ServiceTemplate</code> object located in the same namespace <code>.spec.serviceSpec.services[].name</code> <code>my-kyverno-release</code> Release name for the beach-head service <code>.spec.serviceSpec.services[].namespace</code> <code>my-kyverno-namespace</code> Release namespace for the beach-head service (default: <code>.spec.services[].name</code>) <code>.spec.serviceSpec.services[].values</code> <code>replicas: 3</code> Helm values to be used with the template while deployed the beach-head services <code>.spec.serviceSpec.services[].valuesFrom</code> `` Can reference a ConfigMap or Secret containing helm values <code>.spec.serviceSpec.services[].disable</code> <code>false</code> Disable handling of this beach-head service (default: <code>false</code>)"},{"location":"user-enable-drift-detection/","title":"Detecting and Correcting Drift","text":""},{"location":"user-enable-drift-detection/#how-drift-detection-and-correction-works","title":"How Drift Detection and Correction Works","text":"<p>The drift-detection-manager watches for the deployed helm chart resources (that is, the resources deployed via a <code>ServiceTemplate</code>) and if it detects any changes  in the spec of the resources based on hash value, it updates the status of the <code>ResourceSummary</code> object. This change triggers the addon-controller in the <code>projectsveltos</code> namespace in the management cluster to update the status of the associated <code>ClusterSummary</code> object, which then triggers a reconcile to  re-deploy the spec to the target cluster.</p> <p>Note</p> <p>The <code>ResourceSummary</code> and <code>ClusterSummary</code> are CRDs provided by Sveltos.</p>"},{"location":"user-enable-drift-detection/#enabling-drift-detection","title":"Enabling Drift Detection","text":"<p>Set <code>.spec.serviceSpec.syncMode=Continuous</code> in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object to enable drift detection and correction. Sveltos will then automatically deploy the drift-detection-manager on the targeted clusters:</p> <p><pre><code>kubectl -n projectsveltos get deployments.apps \n</code></pre> <pre><code>NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ndrift-detection-manager   1/1     1            1           152m\nsveltos-agent-manager     1/1     1            1           152m\n</code></pre></p>"},{"location":"user-enable-drift-detection/#using-drift-ignore","title":"Using Drift Ignore","text":"<p>Certain resources can be completely opted out of drift correction by using this feature. In the following example, the \"ingress-nginx/ingress-nginx-controller\" deployment is ignored for drift correction on the target cluster.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n      . . .\n    priority: 100\n    syncMode: ContinuousWithDriftDetection\n    driftIgnore:\n      - target:\n        group: apps\n        version: v1\n        kind: Deployment\n        name: ingress-nginx-controller\n        namespace: ingress-nginx\n  . . .\n</code></pre> <p>If we manually remove the <code>app.kubernetes.io/managed-by=Helm</code> label, we can observe that the drift is not corrected as can be seen in the following watch output.</p> <p><pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller --show-labels -w\n</code></pre> <pre><code>NAME                       READY   UP-TO-DATE   AVAILABLE   AGE     LABELS\ningress-nginx-controller   3/3     3            3           3h58m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx,app.kubernetes.io/version=1.11.0,helm.sh/chart=ingress-nginx-4.11.0\ningress-nginx-controller   3/3     3            3           3h59m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx,app.kubernetes.io/version=1.11.0,helm.sh/chart=ingress-nginx-4.11.0\n</code></pre></p> <p>This can also be verified by observing that <code>ignoreForConfigurationDrift: true</code> is set for the targeted resource in the <code>ResourceSummary</code> spec on the target cluster.</p> <pre><code>kind: ResourceSummary\nmetadata:\n  . . .\nspec:\n  chartResources:\n  - chartName: ingress-nginx\n    group:\n    . . .\n    - group: apps\n      ignoreForConfigurationDrift: true\n      kind: Deployment\n      name: ingress-nginx-controller\n      namespace: ingress-nginx\n      version: v1\n    releaseName: ingress-nginx\n    releaseNamespace: ingress-nginx\nstatus:\n  helmResourceHashes:\n  . . .\n</code></pre> <p>Yet another way to check if a resource is being ignored for drift is by verifying that the <code>projectsveltos.io/driftDetectionIgnore: ok</code> annotation has been applied to it, as in:</p> <pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller -o=jsonpath='{.metadata.annotations}'\n{\"deployment.kubernetes.io/revision\":\"1\",\"meta.helm.sh/release-name\":\"ingress-nginx\",\"meta.helm.sh/release-namespace\":\"ingress-nginx\",\"projectsveltos.io/driftDetectionIgnore\":\"ok\"}%\n</code></pre>"},{"location":"user-enable-drift-detection/#removing-drift-ignore","title":"Removing Drift Ignore","text":"<p>The drift ignore setting can be removed by removing the <code>.spec.serviceSpec.driftIgnore</code> field.</p>"},{"location":"user-enable-drift-detection/#using-drift-exclusions","title":"Using Drift Exclusions","text":"<p>Certain fields of a resource can be excluded from drift detection using this feature. In the following example, the <code>.spec.replicas</code> field of the <code>ingress-nginx/ingress-nginx-controller</code> deployment on the target cluster is excluded from drift detection.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n      . . .\n    priority: 100\n    syncMode: ContinuousWithDriftDetection\n    driftExclusions:\n      - paths:\n        - \"/spec/replicas\"\n        target:\n          kind: Deployment\n          name: ingress-nginx-controller\n          namespace: ingress-nginx\n  . . .\n</code></pre> <p>If we manually edit the replicas to be 1, the number of replicas is not corrected back to 3 as is indicated by the following watch output.</p> <pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller -o=jsonpath='{.spec.replicas}' -w\n3111\n</code></pre> <p>We can also verify that this is the case by observing that the <code>ResourceSummary</code> object has the following patch in its spec now.</p> <pre><code>kind: ResourceSummary\nmetadata:\n  . . .\nspec:\n  chartResources:\n  - chartName: ingress-nginx\n    group:\n    . . .\n    releaseName: ingress-nginx\n    releaseNamespace: ingress-nginx\n  patches:\n  - patch: |-\n      - op: remove\n        path: /spec/replicas\n    target:\n      kind: Deployment\n      name: ingress-nginx-controller\n      namespace: ingress-nginx\nstatus:\n  helmResourceHashes:\n  . . .\n</code></pre>"},{"location":"user-enable-drift-detection/#removing-drift-exclusion","title":"Removing Drift Exclusion","text":"<p>The drift exclusion can be removed by removing the <code>.spec.serviceSpec.driftExclusion</code> field and re-triggering the drift correction by editing any field in the \"ingress-nginx/ingress-nginx-controller\" deployment. This will force a drift correction and since the drift exclusion has been removed, it will restore the deployment to it's original spec.</p>"},{"location":"why-k0rdent/","title":"Why k0rdent?","text":"<p>k0rdent was developed to provide for the needs of platform engineers and the developers that they serve, as well as the application workloads that they support. </p>"},{"location":"why-k0rdent/#applications-and-workloads","title":"Applications and Workloads","text":"<p>Workloads evolve and grow, often faster then the infrastructure needed to support them. Infrastructure is almost always the lagging factor in getting new or existing applications into the hands of users and scaling them to meet user expectations. This challenge has grown exponentially with the rise of AI and ML workloads. Specifically:</p> <ul> <li>Workload complexity is increasing</li> <li>Modern workloads depend on specialized infrastructure</li> <li>Developers have high expectations of time to value</li> </ul>"},{"location":"why-k0rdent/#platform-engineering","title":"Platform Engineering","text":"<p>Modern infrastructure systems are increasingly complex, and administrators need to manage that complexity while still responding quickly to developers' needs efficiently as possible. This has led to development of internal developer platforms (IDP) and platform engineering. These environments provide the frameworks and tools for increasing developer productivity when developing, deploying, and managing applications and services, enabling developers to focus on their specific tasks or goals and not the underlying complexities. Overall:</p> <ul> <li>Developer platforms increase developer productivity</li> <li>Platform engineers need to implement and grow the platforms</li> <li>Infrastructure needs to support the required complexity</li> </ul>"},{"location":"why-k0rdent/#modern-infrastructure-systems","title":"Modern Infrastructure Systems","text":"<p>The increasingly distributed nature of modern infrastructure systems and the demands of modern workloads is leading to increasing complexity. Solutions need to solve a diverse set of challenges and provide consistency, repeatability, and prevention of lock-in, all without increasing the burden on operators. Modern platform engineers and operators are increasingly time constrained with the vast number of challenges they need to overcome, including security and compliance, cost management, resilience, and scale to name but a few. Keep in mind that:</p> <ul> <li>Distributed deployments are the new normal</li> <li>Infrastructure management is not just a technical problem</li> <li>Operators need to focus on building value chains</li> </ul>"},{"location":"why-k0rdent/#open-source","title":"Open Source","text":"<p>The open source ecosystem, and especially Kubernetes, is mature and offers an increasing number of tools that solve real problems. The open source ecosystem, if leveraged correctly, also supports building unique architectures to support a business' needs while helping to avoid lock-in and architectural dead ends. All of these tools need to be selected, deployed, and lifecycle-managed in a way that is repeatable and traceable. Open source:</p> <ul> <li>Prevents lock-in and supports architectural self determination</li> <li>Solutions allow for solving of problems in unique ways</li> <li>Can help solve the problem of managing the complexity of modern infrastructure</li> </ul>"}]}